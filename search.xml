<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[使用setuptools对Python进行打包分发]]></title>
    <url>%2F2019%2F01%2F19%2Fpython-setuptools%2F</url>
    <content type="text"><![CDATA[1. 分发工具 setuptools由于 distutils 无法定义包之间的依赖关系，Python 研究人员开发了增强版的打包工具 setuptools 以便更好的创建和分发 python 包。setuptools 通过添加基本的依赖系统和相关功能，提供了自动包查询程序，用来自动获取包之间的依赖关系，并完成这些包的安装，大大降低了安装各种包的难度。 通常安装 python 时会自带 setuptools，如果没有可以使用 pip 安装。setuptools 简单易用，只需写一个简短的 setup.py 安装文件，就可以将开发完成的 Python 应用打包。 $ pip install setuptools 2. 使用安装包创建 wheel 文件. +-- hello | +-- hello_world.py | +-- __init__.py +-- setup.py ├── build // webpack配置文件 ├── config // 项目打包路径 ├── elm // 上线项目文件，放在服务器即可正常访问 ├── screenshots // 项目截图 ├── src // 源码目录 │ ├── components // 组件 │ │ ├── common // 公共组件 │ │ │ ├── alertTip.vue // 弹出框组件 │ │ │ ├── buyCart.vue // 购物车组件 │ │ │ ├── computeTime.vue // 倒计时组件 │ │ │ ├── loading.vue // 页面初始化加载数据的动画组件 │ │ │ ├── mixin.js // 组件混合(包括：指令-下拉加载更多，处理图片地址) │ │ │ ├── ratingStar.vue // 评论的五颗星组件 │ │ │ └── shoplist.vue // msite和shop页面的餐馆列表公共组件 │ │ ├── footer │ │ │ └── footGuide.vue // 底部公共组件 │ │ └── header │ │ └── head.vue // 头部公共组件 │ ├── config // 基本配置 │ │ ├── env.js // 环境切换配置 │ │ ├── fetch.js // 获取数据 │ │ ├── mUtils.js // 常用的js方法 │ │ └── rem.js // px转换rem │ ├── images // 公共图片 │ ├── page │ │ ├── balance │ │ │ ├── balance.vue // 余额页 │ │ │ └── children │ │ │ └── detail.vue // 余额说明 │ │ ├── benefit │ │ │ ├── benefit.vue // 红包页 │ │ │ └── children │ │ │ ├── commend.vue // 推荐有奖 │ │ │ ├── coupon.vue // 代金券说明 │ │ │ ├── exchange.vue // 兑换红包 │ │ │ ├── hbDescription.vue // 红包说明 │ │ │ └── hbHistory.vue // 历史红包 │ │ ├── city │ │ │ └── city.vue // 当前城市页 │ │ ├── confirmOrder │ │ │ ├── children │ │ │ │ ├── children │ │ │ │ │ ├── addAddress.vue // 添加地址页 │ │ │ │ │ └── children │ │ │ │ │ └── searchAddress.vue // 搜索地址页 │ │ │ │ ├── chooseAddress.vue // 选择地址页 │ │ │ │ ├── invoice.vue // 选择发票页 │ │ │ │ ├── payment.vue // 付款页 │ │ │ │ ├── remark.vue // 订单备注页 │ │ │ │ └── userValidation.vue // 用户验证页 │ │ │ └── confirmOrder.vue // 确认订单页 │ │ ├── download │ │ │ └── download.vue // 下载App │ │ ├── find │ │ │ └── find.vue // 发现页 │ │ ├── food │ │ │ └── food.vue // 食品筛选排序页 │ │ ├── forget │ │ │ └── forget.vue // 忘记密码，修改密码页 │ │ ├── home │ │ │ └── home.vue // 首页 │ │ ├── login │ │ │ └── login.vue // 登录注册页 │ │ ├── msite │ │ │ └── msite.vue // 商铺列表页 │ │ ├── order │ │ │ ├── children │ │ │ │ └── orderDetail.vue // 订单详情页 │ │ │ └── order.vue // 订单列表页 │ │ ├── points │ │ │ ├── children │ │ │ │ └── detail.vue // 积分说明 │ │ │ └── points.vue // 积分页 │ │ ├── profile │ │ │ ├── children │ │ │ │ ├── children │ │ │ │ │ ├── address.vue // 地址 │ │ │ │ │ └── children │ │ │ │ │ ├── add.vue // 新增地址 │ │ │ │ │ └── children │ │ │ │ │ └── addDetail.vue // 搜索地址 │ │ │ │ ├── info.vue // 帐户信息 │ │ │ │ └── setusername.vue // 重置用户名 │ │ │ └── profile.vue // 个人中心 │ │ ├── search │ │ │ └── search.vue // 搜索页 │ │ ├── service │ │ │ ├── children │ │ │ │ └── questionDetail.vue // 问题详情 │ │ │ └── service.vue // 服务中心 │ │ ├── shop │ │ │ ├── children │ │ │ │ ├── children │ │ │ │ │ └── shopSafe.vue // 商铺认证信息页 │ │ │ │ ├── foodDetail.vue // 商铺信息页 │ │ │ │ └── shopDetail.vue // 单个商铺信息页 │ │ │ └── shop.vue // 商铺筛选页 │ │ └── vipcard │ │ ├── children │ │ │ ├── invoiceRecord.vue // 购买记录 │ │ │ ├── useCart.vue // 使用卡号购买 │ │ │ └── vipDescription.vue // 会员说明 │ │ └── vipcard.vue // 会员卡办理页 │ ├── plugins // 引用的插件 │ ├── router │ │ └── router.js // 路由配置 │ ├── service // 数据交互统一调配 │ │ ├── getData.js // 获取数据的统一调配文件，对接口进行统一管理 │ │ └── tempdata // 开发阶段的临时数据 │ ├── store // vuex的状态管理 │ │ ├── action.js // 配置actions │ │ ├── getters.js // 配置getters │ │ ├── index.js // 引用vuex，创建store │ │ ├── modules // store模块 │ │ ├── mutation-types.js // 定义常量muations名 │ │ └── mutations.js // 配置mutations │ └── style │ ├── common.scss // 公共样式文件 │ ├── mixin.scss // 样式配置文件 │ └── swiper.min.css │ ├── App.vue // 页面入口文件 │ ├── main.js // 程序入口文件，加载各种公共组件 ├── favicon.ico // 图标 ├── index.html // 入口html文件]]></content>
      <categories>
        <category>基础知识</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[创建python虚拟环境]]></title>
    <url>%2F2019%2F01%2F16%2Fpython-virtual-env%2F</url>
    <content type="text"><![CDATA[使用 Anaconda 创建 python 虚拟环境 基于 Python-3.6 版本创建自己的深度学习环境，命名为 MY_DL_ENV，离线环境下需要使用 —offine参数，同时 pip 还是主环境的 pip # 默认安装路径 ../Ananconda3/envs $ conda create --name MY_DL_ENV python=3.6 --offline # 查看所创建的环境并激活，命令行前会出现 &#39;(虚拟环境名)&#39; $ conda env list $ conda activate MY_DL_ENV # 安装所需要的包 $ pip install tensorflow-gpu==1.10.0 # 退出环境 $ conda deactivate MY_DL_ENV # 删除虚拟环境中的包 $ conda remove --name MY_DL_ENV numpypip # 删除环境 $ conda remove --name MY_DL_ENV --all 使用 Virtualenv 创建 python 虚拟环境 Virtualenv 是创建隔绝的Python环境的工具，通过创建一个包含所有必要的可执行文件的文件夹来使用Python工程所需的包。 # 安装 virtualenv $ pip install virtualenv # 创建虚拟环境目录名 $ mkdir deep_env $ virtualenv -p /usr/bin/python3 deep_env # 创建完全隔离的Python环境， 设置 --no-site-packages 参数取消对系统 Python 库的引用 # 只包含 setuptools、pip、wheel 和 easy_install.py $ virtualenv --no-site-packages deep_env # 激活虚拟环境, 命令行前会出现 &#39;(虚拟环境名)&#39; $ source deep_env/bin/activate # 安装所需要的包 $ pip install tensorflow-gpu==1.10.0 # 删除安装包 $ pip uninstall tensorflow # 退出虚拟环境 $ deactivate # 删除虚拟环境 $ rm -rf deep_env]]></content>
      <categories>
        <category>基础教程</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[提高Python程序性能的建议]]></title>
    <url>%2F2019%2F01%2F02%2Fcoding-effective-python%2F</url>
    <content type="text"><![CDATA[1. Python为什么慢？编程语言的效率：（1） 开发效率（程序员完成编码的时间）；（2） 运行效率（计算机完成计算任务的时间）。 1.1 Python是动态语言动态语言是指程序运行时可以根据某些条件改变自身结构，例如新的函数、对象、甚至代码可以被引进，已有的函数可以被删除或是其他结构上的变化。 运行时结构不可变的语言就是静态语言。 在程序执行时，解释器并不知道变量的类型，只知道该变量是某种Python对象。因此解释器必须检查每个变量的PyObject_HEAD才能知道变量类型，然后执行对应的数据操作，最后要创建一个新的Python对象来保存返回值。 (1) 计算 $a+b$ 的 C++ 命令 int a = 1 int b = 2 int c = a + b 编译器始终知道a和b是整型，在执行相加运算时，流程如下: (a) 首先把1赋值给a，把2赋值给b； (b) 然后调用 binary_add(a,b)； (c) 最后把结果赋值给c。 (2) 实现同样功能的Python命令如下: a = 1 b = 2 c = a + b 编译器始终不知道 a 和 b 的数据类型，在执行相加运算时，流程如下: (a) 首先把1赋值给a。 - 设置a-&gt;PyObject_HEAD-&gt;typecode为整型； - 设置a-&gt;val = 1。 (b) 接着把2赋值给b。 (c) 然后调用binary_add(a, b)。 - a-&gt;PyObject_HEAD获取类型编码，a为整型；值为a-&gt;val。 - 同理b。 - 调用binary_add(a-&gt;val,b-&gt;val)，结果为整型并存在result中。 (c) 最后创建对象c。 - 设c-&gt;PyObject_HEAD-&gt;typecode为整型。 - 设置c-&gt;val为result。 动态类型意味着任何操作都会涉及更多的步骤，每一个简单的操作都需要大量的指令才能完成。这也是Python等动态语言对数值操作比C语言慢的主要原因。 1.2 Python中一切都是对象Python的对象模型会导致内存效率较低。 最简单的NumPy数组是根据C语言的数据结构创建的Python对象，它有一个指向连续数据缓存区的指针。而Python的list虽然具有指向连续的指针缓冲区的指针，但是每一个指针都指向一个整数类型的Python对象。如上图所示，如果正在执行按顺序逐步完成数据的操作，numpy的内存布局比Python的内存布局更为高效，因为存储成本和访问的时间成本都更低。 1.3 Python全局解释器锁全局解释器锁（Global Interpreter Lock, GIL）并不是Python的特性，它是在实现Python解析器(CPython)时所引入的概念，Python完全可以不依赖于GIL。每个线程在执行的过程都需要先获取GIL，保证同一时刻只有一个线程对共享资源进行存取，使得CPython中的多线程并不能真正的并发。 首先了解一下并发和并行的概念：什么是并发什么是并行，他们的区别是什么? 你吃饭吃到一半，电话来了，你一直到吃完了以后才去接，这就说明你不支持并发也不支持并行。你吃饭吃到一半，电话来了，你停了下来接了电话，接完后电话以后继续吃饭，这说明你支持并发。你吃饭吃到一半，电话来了，你一边打电话一边吃饭，这说明你支持并行。 并发：交替处理多个任务的能力。并发的关键是你有处理多个任务的能力，不一定要同时。 并行：同时处理多个任务的能力。并行的关键是你有同时处理多个任务的能力，强调的是同时。 所以它们最大的区别就是：是否是同时处理任务。对于一个多核cpu来说并行显然要比并发快的多，使用多线程和多进程写程序的目的是为了让多核cup发挥最大的功效实现并行处理。 为了更有效的利用多核处理器的性能，多线程编码方式应运而生。解决多线程之间数据完整性和状态同步困难问题的最简单方法自然就是加锁。全局解释器锁GIL来控制线程的执行，每一个时刻只允许一个线程执行。Python中并没有实现线程调度，其多线程调度完全依赖于操作系统。所以python多线程编程中没有线程优先级等概念。 尽量使用线程进行并发I/O操作，在进程中进行并行计算。 （1）在处理像科学计算等需要持续使用cpu的任务时，单线程会比多线程快； （2）在处理像IO操作等可能引起阻塞的任务时，多线程会比单线程快； import time from threading import Thread from multiprocessing import Process from concurrent import futures # CPU密集型程序 def func(number): while(number&gt;0): number -= 1 print(number) def multi_thread(number_thread, function, params): thread_set = {} for i in range(number_thread): t = Thread(target=function, args=(params,)) t.start() thread_set[i] = t for j in range(number_thread): thread_set[j].join() def multi_process(number_process, function, params): process_set = {} for i in range(number_process): p = Process(target=function, args=(params,)) p.start() process_set[i] = p for j in range(number_process): process_set[j].join() def thread_pool(number_works, function, params): with futures.ThreadPoolExecutor(number_works) as executor: executor.map(function, params) # with futures.ProcessPoolExecutor(number_works) as executor: # executor.map(function, params) if __name__ == &quot;__main__&quot;: number = 10000000 number_thread = 2 number_process = 2 number_work = 2 time_start_1 = time.time() func(number) time_1 = time.time() - time_start_1 multi_thread(number_thread, func, number) time_2 = time.time() - time_start_1 multi_process(number_process, func, number) time_3 = time.time() - time_start_1 - time_start_2 multi_process(number_process, func, number) time_4 = time.time() - time_start_1 - time_start_2 - time_start_3 print(&#39;Time of func_1 is {0:.4f} seconds&#39;.format(time_1)) print(&#39;Time of func_2 is {0:.4f} seconds&#39;.format(time_2)) print(&#39;Time of func_3 is {0:.4f} seconds&#39;.format(time_3)) print(&#39;Time of func_4 is {0:.4f} seconds&#39;.format(time_4)) 2. Python性能分析方法虽然运行速度慢是 Python 与生俱来的特点，大多数时候我们用 Python 就意味着放弃对性能的追求。很多时候，我们将自己的代码运行缓慢地原因归结于python本来就很慢，从而心安理得地放弃深入探究。但是，事实真的是这样吗？面对python代码，你有分析下面这些问题吗： 程序运行的速度如何？ 程序运行时间的瓶颈在哪里？ 能否稍加改进以提高运行速度呢？ 为了更好了解python程序，我们需要一套工具和方法，方便彻底了解代码，能够记录代码的运行时间，生成性能分析报告，从而对代码进行针对性的优化。 2.1 什么是性能分析性能分析就是分析代码和它正在使用的资源之间有着怎样的关系。例如,性能分析可以告诉你一个指令占用了多少CPU时间,或者整个程序消耗了多少内存。性能分析是通过使用一种被称为性能分析器(profiler)的工具，对程序或者二进制可执行文件的源代码进行调整来完成的。性能分析软件有两类方法论：基于事件的性能分析(event-based profiling)和统计式性能分析(statistical profiling)。 基于事件的性能分析器(也称为轨迹性能分析器，tracing profiler)是通过收集程序执行过程中的具体事件进行工作的。性能分析器会产生大量的数据，导致其不太实用，在开始对程序进行性能分析时也不是首选。但是,当其他性能分析方法不够用或者不够精确时,它们可以作为最后的选择。 统计式性能分析器以固定的时间间隔对程序计数器(program counter)进行抽样统计，这样做可以让开发者掌握目标程序在每个函数上消耗的时间。由于它对程序计数器进行抽样,所以数据结果是对真实值的统计近似，不仅能够分析程序的性能细节，查出性能的瓶颈所在，而且分析的数据更少，对性能造成的影响更小。 性能分析并不是每个程序都要做的事情，因为其需要花费时间，而且只有在程序中发现了错误的时候才有用。但是，在执行程序之前进行性能分析，可以捕获潜在的bug，为后续的程序调试节省时间。 2.2 性能分析的内容程序的西能分析可以归纳为四个基本问题： （1）它运行的有多块？ （2）哪里是速度的瓶颈？ （3）它使用了多少内存？ （4）哪里发生了内存泄漏？ 2.2.1 运行时间2.2.1.1 使用 profile 进行时间分析对代码优化的前提是需要了解性能瓶颈在什么地方，程序运行的主要时间是消耗在哪里，对于比较复杂的代码可以借助一些工具来定位，python内置了丰富的性能分析工具，如profile，cProfile与hotshot等。其中Profiler是python自带的一组程序，能够描述程序运行时候的性能，并提供各种统计帮助用户定位程序的性能瓶颈。使用非常简单，只需要在使用之前进行 import 即可。 def profile_test(): value_list = [] total = 1 for i in range(10): total = total * (i + 1) value_list.append(total) return value_list if __name__ == &quot;__main__&quot;: import cProfile cProfile.run(&#39;profile_test()&#39;, &#39;profile_test.txt&#39;) import pstats p = pstats.Stats(&#39;profile_test.txt&#39;) p.sort_stats(&#39;time&#39;).print_stats() 其中输出每列的具体解释如下: ncalls：表示函数调用的次数； tottime：表示指定函数的总的运行时间，除掉函数中调用子函数的运行时间； percall：（第一个 percall）等于tottime/ncalls； cumtime：表示该函数及其所有子函数的调用运行的时间，即函数开始调用到返回的时间； percall：（第二个 percall）即函数运行一次的平均时间，等于 cumtime/ncalls； filename:lineno(function)：每个函数调用的具体信息； 如果需要将输出以日志的形式保存，只需要在调用的时候加入另外一个参数。如 profile.run(“profileTest()”,”testprof”)，调用 pstats 模块即可读取日志。 2.2.1.2 使用 line_profile 进行时间分析开源工具line_profiler可以统计脚本中每行代码的运行时间和执行次数。通过pip安装该python包： $ pip install line_profiler 安装完成之后得到名为 line_profiler 的新模组和 kernprof.py 可执行脚本。使用时不需要导入任何模组，只需要在源代码中被测量的函数上装饰@profile装饰器。kernprof.py脚本将会在执行的时候将模组自动注入到运行脚本中。 @profile def primes(n): if n == 2: return [2] elif n &lt; 2: return [] s = list(range(3, n + 1, 2)) m_root = n ** 0.5 half = (n + 1) / 2 - 1 i = 0 m = 3 while m &lt;= m_root: if s[i]: j = int((m * m - 3) / 2) s[j] = 0 while j &lt; half: s[j] = 0 j += m i = i + 1 m = 2 * i + 3 return [2] + [x for x in s if x] $ kernprof -l -v primes.py-l 选项通知 kernprof 注入 @profile 装饰器到执行脚本，-v 选项通知kernprof在脚本执行完毕的时候显示统计信息。输出每列的含义如下： Line: 行号 Hits: 当前行执行的次数 Time: 当前行执行耗费的时间 Per Hit: 平均执行一次耗费的时间 %Time: 当前行执行时间占总时间的比例 Line Contents: 当前行的代码 具有高Hits值或高Time值的行就是可以通过优化带来最大性能改善的地方。 2.2.2 内存资源除了运行时间之外，程序所消耗的内存资源也是性能分析需要考虑的问题。内存消耗不仅仅是关注程序使用了多少内存,还应该考虑控制程序使用内存的数量。跟踪程序内存的消耗情况比较简单。最基本的方法就是使用操作系统的任务管理器。它会显示很多信息,包括程序占用的内存数量或者占用总内存的百分比。任务管理器也是检查CPU时间使用情况的好工具。 2.2.2.1 使用 memory_profile 进行内存分析开源工具memory_profiler可以统计脚本所占用的内存以及每行代码所增加的占用内存。通过pip安装该python包: $ pip install memory_profiler 安装完成之后得到名为 memory_profiler 的新模组和 memory_profiler.py可执行脚本。安装psutil包：pip install psutil ，因为它可以大大改善 memory_profiler 的性能使用方法和 line_profiler 类似，只需要在感兴趣的函数上面添加@profile装饰器： @profile def primes(n): pass 2.2.2.2 使用 objgraph 分析内存泄漏cPython解释器使用引用计数做为记录内存使用的主要方法。这意味着每个对象都包含一个计数器，当某处对该对象的引用被存储时计数器增加，当引用被删除时计数器递减。当计数器到达零时，cPython解释器认为该对象不再被使用，就会删除对象，释放所占用的内存。如果程序中不再被使用的对象的引用一直被占有，那么就可能会发生内存泄漏。 开源工具objgraph可以有效查找“内存泄漏”，它允许查看内存中对象的数量，定位含有该对象的引用的所有代码的位置。 a. 显示占据python程序内存的头N个对象 b. 显示一段时间以后哪些对象被删除，哪些对象增加了 c. 显示脚本中某个给定对象的所有引用 $ pip install objgraph import objgraph if __name__ == &quot;__main__&quot;: x = [&#39;a&#39;, &#39;1&#39;, [2, 3]] objgraph.show_refs([x], filename=&#39;test.png&#39;) objgraph.show_most_common_types() 3. 实用优化技巧3.1 编码规范3.1.1 了解代码优化的基本原则 优先保证代码是可以工作的 过早优化是编程中一切“罪恶”的根源，过早优化可能会忽视对总体性能指标的把握，忽略可移植性、可读性等 权衡优化的代价（质量、时间和成本） 优化是有代价的，想解决所有性能问题几乎是不可能的 定义性能指标，集中力量解决首要问题 在进行优化之前，针对问题进行主次排列，集中力量解决主要问题 不要忽略可读性 实际应用中，经常运行的代码可能只占很少部分，但是几乎所有代码都需要维护，因此优化时需要考虑可读性和可维护性。 3.1.2 编写函数的四个原则 函数设计要尽量短小，嵌套层次不宜过深 函数申明应该做到合理、简单、易于使用 函数参数设计应该考虑向下兼容 一个函数制作一件事，尽量保证函数语句粒度的一致性 3.1.3 在代码中适当添加注释 使用块注释或者行注释的时候只注释复杂的操作和算法 注释和代码隔开一定的距离 给外部可访问的函数和方法添加文档注释 在文件开头包含版权申明、模块描述和变更记录等信息 3.2 语法技巧3.2.1 数据交换值不推荐使用中间变量Python表达式赋值的时候右边操作数先于左边的进行计算，首先创建元组(y, x)，x和y初始化已在内存中，然后通过解压缩将元组依次分配给左边的标识符。 from timeit import Timer time_1 = Timer(&#39;temp=x; x=y; y=temp&#39;, &#39;x=2; y=3&#39;).timeit() time_2 = Timer(&#39;x, y = y, x&#39;, &#39;x=2; y=3&#39;).timeit() print((time_1-time_2)/time_1) 3.2.2 充分利用 Lazy Evaluation 的特性延迟计算仅仅在真正需要执行的时候才计算表达式的值(a) 避免不必要的计算，带来性能上的提升 import time def method(word_list, word_set): time_start = time.time() for i in range(1000000): for word in word_list: if word in word_set: pass time_1 = time.time() - time_start for i in range(1000000): for word in word_list: if word[-1] == &#39;.&#39; and word in word_set: pass time_2 = time.time() - time_start - time_1 return (time_1 - time_2) / time_1 if __name__ == &quot;__main__&quot;: word_set = [&#39;aa.&#39;, &#39;bb.&#39;, &#39;cc.&#39;, &#39;dd.&#39;, &#39;ee.&#39;, &#39;ff.&#39;, &#39;gg.&#39;, &#39;hh.&#39;, &#39;ii.&#39;, &#39;jj.&#39;] word_list = [&#39;aa&#39;, &#39;bb.&#39;, &#39;cc&#39;, &#39;dd.&#39;, &#39;ee&#39;, &#39;ff.&#39;, &#39;gg&#39;] print(method(word_list, word_set)) (b) 节省空间，使得无限循环成为可能 from itertools import islice # 斐波那契数列 def fibonacci(): a, b = 0, 1 while True: yield a a, b = b, a+b if __name__ == &quot;__main__&quot;: print(list(islice(fibonacci(), 5))) 3.2.3 有节制地使用 from…import 语句 尽量优先使用 import a 形式，使用 a.B 访问模块中的对象 有节制地使用 from a import B 形式，直接访问 B 避免使用 from a import *，会污染命名空间，无法清晰地显示导入对象 3.2.4 使用 with 自动关闭资源对文件的操作完成后应该立即关闭文件 f = open(&#39;test.txt&#39;, &#39;w&#39;) f.write(&#39;test&#39;) with open(&#39;test.txt&#39;, &#39;w&#39;) as f: f.write(&#39;test&#39;) 3.2.5 连接字符串应该优先使用 join 而不是 +python字符串为不可变对象，使用 + 连接字符串时会复制原有的字符串，从而直接导致链接效率降低。 def func_string(string, string_list): new_string = string time_start = time.time() for i in range(10000): for sub_string in string_list: new_string += sub_string time_1 = time.time() - time_start for i in range(10000): new_string += &#39;&#39;.join(string_list) time_2 = time.time() - time_start - time_1 return ((time_1-time_2)/time_1) if __name__ == &quot;__main__&quot;: string = &quot;&quot; string_list = [&#39;aa.&#39;, &#39;bb.&#39;, &#39;cc.&#39;, &#39;dd.&#39;, &#39;ee.&#39;, &#39;ff.&#39;, &#39;gg.&#39;, &#39;hh.&#39;, &#39;ii.&#39;, &#39;jj.&#39;] print(func_string(string, string_list)) 操作符'+'连接字符串示意图 3.2.6 格式化字符串时尽量使用 .format 而不是 %格式化字符串四指根据所规定的转换说明符返回格式化后的字符串 format 方式参数的顺序与格式化的顺序不必完全相同 format 方式可以方便地作为参数传递 % 最终会被 .format 方式所取代# 在 Pycharm 控制台中执行 string_1 = &#39;xxx&#39; string_2 = &#39;yyy&#39; %timeit -n 10000 (&#39;abc%s%s&#39; % (string_1, string_2)) %timeit -n 10000 (&#39;abc{0}{1}&#39;.format(string_1, string_2)) 3.3 库3.3.1 使用 copy 模块拷贝对象浅拷贝：构造一个新的符合对象并将从原对象中发现的引用插入该对象中。深拷贝：构造一个新的符合对象，但是遇到引用会继续递归拷贝其所指向的具体内容。 # 在 Pycharm 控制台中执行 import copy a = range(100000) %timeit -n 100 copy.copy(a) %timeit -n 100 copy.deepcopy(a) 对象copy示意图 3.3.2 使用 Counter 进行计数统计Counter 类属于字典类的子类，是一个容器对象，主要用来统计散列对象，支持集合操作 + - &amp; | import time from collections import Counter def count_frequency(data_list): time_start = time.time() for i in range(10000): count_dict = dict() for item in data_list: if item in count_dict: count_dict[item] += 1 else: count_dict[item] = 1 time_dict = time.time() - time_start for i in range(10000): count_set = set(data_list) count_list = [] for item in count_set: count_list.append((item, data_list.count(item))) time_set = time.time() - time_start - time_dict for i in range(10000): Counter(data_list) time_counter = time.time() - time_start - time_dict - time_set print(&#39;time_dict: {dict:.6}\ntime_set: {set:.6}\ntime_counter: {counter:.6}&#39;.format( dict=time_dict, set=time_set, counter=time_counter)) if __name__ == &#39;__main__&#39;: data_list = [&#39;a&#39;, &#39;2&#39;, 2, 4, 5, &#39;2&#39;, &#39;b&#39;, 4, 7, &#39;a&#39;, 5, &#39;d&#39;, &#39;a&#39;, &#39;z&#39;] count_frequency(data_list) 3.3.3 使用 argparse 处理命令行参数相比于参数配置文件，命令行参数更加灵活，用户的学习成本更低。Python标准库中有 getopt、optparse 和 argparse 三个模块实现命令行参数。 import argparse parser = argparse.ArgumentParser() parser.add_argument(&#39;-o&#39;, &#39;--output&#39;) parser.add_argument(&#39;-v&#39;, dest=&#39;verbose&#39;, action=&#39;store_true&#39;) args = parser.parse_args() 3.3.4 使用 pandas 处理大型 CSV 文件csv模块无法处理大型 CSV 文件，而 pandas 提供了丰富的数据模型，支持多种文件格式处理，包括 CSV、HDF5 和 HTML 等，能够提供高效的大数据处理能力 import csv f = open(&#39;large.csv&#39;, &#39;w&#39;) f.seek(2**30 - 1) f.write(&quot;\0&quot;) f.close() with open(&#39;large.csv&#39;, &#39;r&#39;) as csv_file: my_csv = csv.reader(csv_file) for row in my_csv: pass 上述代码会报错，因为csv无力处理大数据，使用pandas处理则不会报错。 import pandas as pd df = pd.read_csv(&#39;large.csv&#39;) 3.3.5 使用 ElementTree 解析 XML 文件xml.dom.minidom 和 xml.sax 作为解析 XML 文件的两种实现，DOM 需要将整个XML文件加载到内存中解析，占用内存多，性能不占优势； SAX 不需要全部载入 XML 文件，但处理过程较为复杂。 import xml.etree.ElementTree as ET tree = ET.ElementTree(file=&#39;test.xml&#39;) root = tree.getroot() print(root.tag) 3.3.6 使用 pickle 和 JSON 模块进行序列化序列化是指把内存中的数据结构在不丢失其身份和类型信息的情况下转换成对象文本或二进制表示的过程。Python中的序列化模块包括 pickle、json、marshal 和 shelve 等。 pickle 是最通用的序列化模块 a. 接口简单，通过 dump() 和 load() 即可轻易实现序列化和反序列化 b. pickle 存储格式可以跨平台通用 c. 支持的数据类型广泛 相比于 pickle 模块，JSON 具有如下优势： a. 文档构成简单，仅存在键值对集合和值的有序列表两种数据结构 b. 存储格式可读性更为友好，容易修改 c. 支持跨平台，同时也可被其他语言解析 d. 用户可以对默认不支持的序列化类型进行扩展 Python 中标准模块 JSON 的性能弱于 pickle 模块 # 在 Pycharm 控制台中执行 import json import pickle s_pickle = pickle.dump(range(10000)) s_json = json.dump(list(range(10000))) %timeit -n 100 x=pickle.load(s_pickle) %timeit -n 100 x=json.loads(s_json) 3.4 循环和数据结构3.4.1 掌握循环优化的基本技巧 尽量减少循环过程中的计算量，多重循环时尽量将内层计算提到上一层```pythonimport mathimport time def func_1(iter, number): sum = 0 for i in range(iter): d = math.sqrt(number) sum = i + d return sum def func_2(iter, number): sum = 0 d = math.sqrt(number) for i in range(iter): sum = i + d return sum if name == “main“: iter = 100000 number = 100 time_start = time.time() func_1(iter, number) time_1 = time.time() - time_start() func_2(iter, number) time_2 = time.time() - time_start - time_1 print((time_1 - time_2) / time_1) 2. 将显示循环改为隐式循环，需要添加恰当的注释保持代码的可读性 ```python import math import time def func_1(number): sum = 0 for i in range(number+1): sum += i return sum def func_2(number): sum = number * (number+1) / 2 return sum if __name__ == &quot;__main__&quot;: iter = 1000000 time_start = time.time() func_1(number) time_1 = time.time() - time_start() func_2(number) time_2 = time.time() - time_start - time_1 print((time_1 - time_2) / time_1) 在循环中尽量引用局部变量，命名空间中搜索局部变量比全局变量更快```pythonimport mathimport time def func_1(number, list): for i in range(number+1): [math.sin(k) for k in list] def func_2(number, list): local_sin = math.sin for i in range(number+1): [local.sin(k) for k in list] if name == “main“: iter = 10000 data_list = range(100) time_start = time.time() func_1(number, data_list) time_1 = time.time() - time_start() func_2(number, data_list) time_2 = time.time() - time_start - time_1 print((time_1 - time_2) / time_1) ### 3.4.2 选择合适的数据结构 &lt;div style=&quot;text-align:center&quot;&gt; &lt;img src=&quot;coding-effective-python/1_10.png&quot; alt=&quot;数据结构时间复杂度&quot; width=&quot;600&quot; height=&quot;300&quot;&gt; &lt;p style=&quot;color:green&quot;&gt;Python数据结构常见操作的时间复杂度&lt;/p&gt; &lt;/div&gt; 1. 将列表的交集、并集或者差集等问题转换为集合再运算 ```python import time def func_1(list_1, list_2): intersection = [] for i in range(100000): for a in list_1: for b in list_2: if a == b: intersection.append(a) return intersection def func_2(list_1, list_2): intersection = [] for i in range(100000): intersection = list(set(list_1) &amp; set(list_2)) return intersectiion if __name__ == &quot;__main__&quot;: list_1 = [1, 2, 3, 4, 5, 6, 7, 8, 9, 13, 34, 53, 42, 44] list_2 = [2, 4, 6, 9, 23] time_start = time.time() func_1(list_1, list_2) time_1 = time.time() - time_start() func_2(list_1, list_2) time_2 = time.time() - time_start - time_1 print((time_1 - time_2) / time_1) 使用字典或者集合进行查找```python在 Pycharm 控制台中执行data_list = range(10000)data_set = set(data_list)data_dict = dict((i, 1) for i in data_list) %timeit -n 10000 100 in data_list%timeit -n 10000 100 in data_set%timeit -n 10000 100 in data_dict ### 3.4.4 使用列表解析和生成器表达式 列表解析比在列表循环更高效，将列表解析式中 [] 替换成 () 即为生成器表达式 ```python # 列表循环 def func_1(list): new_list = [] for i in range(100000): for w in list: new_list.append(w) # 列表解析 def dunc_2(list): for i in range(100000): new_list = [w for w in list] # 生成器表达式 def func_3(list): for i in range(100000): new_list = (w for w in list) if __name__ == &quot;__main__&quot;: data_list = range(100) time_start = time.time() func_1(data_list) time_1 = time.time() - time_start() func_2(data_list) time_2 = time.time() - time_start - time_1 func_3(data_list) time_3 = time.time() - time_start - time_2 - time_1 print(&#39;func_1: {0:.4f}, func_2: {0:.4f}, func_3: {0:.4f}&#39;.format( time_1, time_2, time_3)) 3.4.5 使用多进程或者线程池克服 GIL 的缺陷因为GIL的存在，Python很难充分利用多核CPU的优势。但是，可以通过内置的模块multiprocessing实现下面几种并行模式: 多进程：对于CPU密集型的程序，可以使用multiprocessing的Process,Pool等封装好的类，通过多进程的方式实现并行计算。但是因为进程中的通信成本比较大，对于进程之间需要大量数据交互的程序效率未必有大的提高。 多线程：对于IO密集型的程序，multiprocessing.dummy模块使用multiprocessing的接口封装threading，使得多线程编程也变得非常轻松(比如可以使用Pool的map接口，简洁高效)。 线程池和进程池 分布式：multiprocessing中的Managers类提供了可以在不同进程之共享数据的方式，可以在此基础上开发出分布式的程序。 不同的业务场景可以选择其中的一种或几种的组合实现程序性能的优化。 import time from threading import Thread from multiprocessing import Process from concurrent import futures # CPU密集型程序 def func(number): while(number&gt;0): number -= 1 print(number) def multi_thread(number_thread, function, params): thread_set = {} for i in range(number_thread): t = Thread(target=function, args=(params,)) t.start() thread_set[i] = t for j in range(number_thread): thread_set[j].join() def multi_process(number_process, function, params): process_set = {} for i in range(number_process): p = Process(target=function, args=(params,)) p.start() process_set[i] = p for j in range(number_process): process_set[j].join() def thread_pool(number_works, function, params): # with futures.ThreadPoolExecutor(number_works) as executor: # executor.map(function, params) with futures.ProcessPoolExecutor(number_works) as executor: executor.map(function, params) if __name__ == &#39;__main__&#39;: number = 10000000 number_thread = 2 number_process = 2 multi_work = 2 time_start_1 = time.time() func(number) time_1 = time.time() - time_start_1 time_start_2 = time.time() multi_thread(number_thread, func, number) time_2 = time.time() - time_start_2 time_start_3 = time.time() multi_process(number_process, func, number) time_3 = time.time() - time_start_3 time_start_4 = time.time() multi_process(number_process, func, number) time_4 = time.time() - time_start_4 print(&#39;Time of func_1 is {0:.4f} seconds&#39;.format(time_1)) print(&#39;Time of func_2 is {0:.4f} seconds&#39;.format(time_2)) print(&#39;Time of func_3 is {0:.4f} seconds&#39;.format(time_3)) print(&#39;Time of func_4 is {0:.4f} seconds&#39;.format(time_4)) 3.5 使用C扩展 Ctypes 对于关键的性能代码，Python本身也提供了一个API来调用C方法，主要通过 ctypes来实现，因为默认情况下python提供了预编译的标准c库。通常用于封装(wrap)C程序，让纯Python程序调用动态链接库（Windows中的dll或Unix中的so文件）中的函数。如果想要在python中使用已经有C类库，使用ctypes是很好的选择。 Cython Cython 是python的一个超集，用于简化编写C扩展的过程，允许通过调用C函数以及声明变量来提高性能。它将类Python代码编译成可以在python文件中调用的C库，后缀使用.pyx替代。 Cython的优点是语法简洁，可以很好地兼容numpy等包含大量C扩展的库。Cython的使得场景一般是针对项目中某个算法或过程的优化。 PyPy PyPy是用RPython(CPython的子集)实现的Python，根据官网的基准测试数据，它比CPython实现的Python要快6倍以上。快的原因是使用了Just-in-Time(JIT)编译器，即动态编译器，与静态编译器(如gcc,javac等)不同，它是利用程序运行的过程的数据进行优化。它的运行方式是立即可用的，因此没有疯狂的bash或者运行脚本，只需下载然后运行即可。 Python不同实现的性能比较 import time import random from ctypes import cdll def func_1(num): while num: yield random.randint(1, 10) num -= 1 def func_2(num): libc = cdll.msvcrt #windows while num: yield libc.rand(1, 10) num -= 1 if __name__ == &quot;__main__&quot;: numbers = 1000000 func_list = [func_1, func_2] for func_name in func_list: t_start = time.clock() new_list = sum(func_name(numbers)) t_end = time.clock() print(&quot;函数 %s 耗费时间 %.4f 秒&quot; % ( str(func_name).split(&#39; &#39;)[1], (t_end-t_start))) 4. 总结 Python程序相对较慢是由其语言自身的特性所决定。 利用Python的语言特性及其优化库中提供的功能可以对程序进行优化 程序80%的运算往往在20%的代码中，有针对性地优化该部分代码可以大大提高程序运行效率，例如将关键代码通过Cython或Numba等项目转换成C程序。 思考 优化你最贵的资源 把事情做完比快速地做事更加重要 任何除了瓶颈之外的改进都是错觉。 过早优化是万恶之源。 只是因为“快速”而选择语言是过早优化的最终形式。 选择一种语言/框架/架构来帮助你快速开发。不要仅仅因为它们的运行效率高。 当遇到性能问题时，请找到瓶颈所在。 你的瓶颈很可能不是 CPU 或者 Python 本身。 如果 Python 成为你的瓶颈，尝试优化你的算法，或者转向热门的 Cython 或者 C语言。 尽情享受可以快速做完事情的乐趣。]]></content>
      <categories>
        <category>基础知识</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[GCC编译简介]]></title>
    <url>%2F2019%2F01%2F01%2Fcompile-and-link%2F</url>
    <content type="text"><![CDATA[1. 编译 C 程序GCC (GNU Compiler Collection) 使用可移植性的C语言写成，能够对自身进行编译，因此很容易被移植到新系统上。编译是指把一个纯文本的源代码“程序”转变为机器码，即用于控制计算机的中央处理单元的 1 和 0 的序列。这种机器码被存放在称为“可执行文件”的文件中，有时也称为二进制文件。程序可以编译自单个源文件或多个源文件，还可以用到系统的库文件和头文件。假定已经编写好“Hello World”的C语言程序 #include &lt;stdio.h&gt; int main(void) { printf(&quot;Hello, world!&quot;); return 0; } 那么在linux系统 hello.c 程序文件路径下输入如下命令，即可对“Hello World”的纯文本源码进行编译。 $ gcc -Wall hello.c -o hello 这样就把“hello.c”中的源代码编译成机器码并存储在可执行文件“hello”中。用“-o”选项可以指定存储机器码的输出文件，该选项通常是命令上的最后一个参数。如果省略，输出将被保存到默认文件“a.out”中。“-Wall”选项打开所有最常用的编译警告，推荐总是使用该选项。 一个程序可以被分成多个源文件以便于编辑和理解，由其是在大型程序中。我们可以将 hello.c 文件分为 “main.c” “hello_func.c” 和 “hello.h”，其中“hello.h”文件中仅仅只有一行 void hello(const char *name) ，是对hello函数的原型进行声明。“main.c”为主程序 #include &quot;hello.h&quot; int main(void) { hello(&quot;world&quot;); return 0; } hello函数自身的定义包含在“hello_func.c”文件中 #include &lt;stdio.h&gt; #include &quot;hello.h&quot; void hello(const char *name) { print(&quot;Hello, %s !&quot;, name) } 其中 # include &quot;FILE.h&quot; 和 # include &lt;FILE.h&gt; 这两种 include 声明形式的含义是有差异的，前者是先在当前目录搜索&quot;FILE.h&quot;，然后再查看包含系统头文件的目录，而后者则是直接搜索系统目录的头文件，默认情况下不会再当前目录下去查找头文件。使用如下命令即可对多个源码文件进行编译 $ gcc -Wall main.c hello_func.c -o newhello 注意，头文件“hello.h”不需要在命令行上的源文件名列表中指定。“hello.h”源码文件中的# include指示符会指导编译器在合适的时候自动地包含它。程序中的所有部分已经被组合成单个的可执行文件，其象前面由单个源文件生成的可执行文件一样输出同样的结果。 2. 独立地编译文件如果整个程序代码被存储在单个源文件中，那么对某个函数的任何改变都需要将整个源码文件重新编译以生成一个新的可执行文件，而重新编译大型源码文件可能需要花费大量的时间。当程序被存储在一个个单独的源码文件中时，只有那些被修改过的源码文件才需要重新编译。通过将源文件分开一个个编译，然后再链接在一起，对大型程序进行修改时可以节约大量时间。在第一阶段，文件被编译但不生成可执行文件，编译的结果被称为对象文件（obj文件），用GCC时有 .o 的后缀名。在第二个阶段，各个对象文件由一个被称为连接器的单独程序合成在一起。连接器把所有的对象文件组合在一起生成单个的可执行文件。对象文件包含的是机器码，其中任何对在其他文件中的函数（或变量）的内存地址的引用都留着没有被解析。这样就允许在互相之间不直接引用的情况下编译各个源代码文件。连接器在生成可执行文件时会填写这些还缺少的地址。 2.1 从源文件生成对象文件命令行选项“-c”用于把源码文件编译成对象文件。例如，下面的命令将把源文件“main.c”编译成一个对象文件 $ gcc -Wall -c main.c 该命令会生成一个包含main函数机器码的对象文件“main.o”。它包含一个队外部函数hello的引用，但在这个阶段该对象文件中的对应的内存地址留着没有被解析（它将在后面链接时被填写）。编译源文件“hello_func.c” 的相应命令为： $ gcc -Wall -c hello_func.c 在这里不需要用 -o 选项来指定输出文件的文件名。当用 -c 来编译时，编译器会自动生成与源文件同名，但用 .o 来代替原来的扩展名的对象文件。由于 “main.c” 和 “hello_func.c”中的 # include 声明，“hello.h”会自动被包括进来，所以在命令行上不需要指定该头文件。 2.2 从对象文件生成可执行文件生成可执行文件的最后步骤是用gcc把各个对象文件链接在一起并补充缺失的外部函数的地址。要把对象文件链接在一起，只需要把他们简单的列在命令行上即可： $ gcc main.o hello_func.o -o hello 这是几个很少需要用到 “-Wall” 警告选项的场合之一，因为每个源文件已经被成功的被编译成对象文件了。一旦源文件被编译，链接是一个要么成功要么失败的明确过程（只有在有引用不能解析的情况下才会链接失败）。 2.3 对象文件的链接次序在类Unix系统上，传统上编译器和链接器搜索外部函数的次序是在命令行上指定的对象文件中从左到右的查找，这意味着包含函数定义的对象文件应当出现在调用这些函数的任何文件之后。例如 main.o 调用 hello_func.o 函数，因此包含hello函数的文件“hello_func.o”应该被放在“main.o”之后。 $ gcc main.o hello_func.o -o hello 如果次序搞反了，有的编译器或链接器会报错。虽然当前绝大部分编译器和链接器会不管次序搜索所有的对象文件，但由于不是所有的编译器都这么做，最好遵守从左到右排序对象文件的惯例。如果命令行上已经包括了所有必须的对象文件，但你还是碰到意料之外的未定义引用这种问题，那就应该想想这个问题。 2.4 与外部库文件链接2.4.1 静态库库是已经编译好并能被链接入程序的对象文件的集合。库通常被存储在扩展名为 .a 的特殊归档文件中，被称为静态库。标准的系统库通常能在 /usr/lib 和 /lib 目录下找到。例如在类Unix系统中， C的数学库常被放在文件 /usr/lib/math.a 中，而该库中的相应的函数的原型声明在头文件 /usr/include/math.h 中。下面是调用数学库 libm.a 中外部函数 sqrt 的一个例子，假定文件名为“calc.c”： #include &lt;math.h&gt; #include &lt;stdio.h&gt; int main(void) { double x = sart(2.0); printf(&quot;The square root of 2.0 is %f&quot;, x); return 0; } 试图只用该源文件就生成可执行文件会导致在链接阶段编译器报错 $ gcc -Wall calc.c -o calc 由于在没有外部数学库 libm.a 的情况下，对函数 sqrt 的引用不能解决。函数 sqrt 并不定义在源程序中或默认的C库 libc.a 中，而且除非 libm.a 被显示指定，否则编译器不会链接该库文件。为了使得编译器能够把 sqrt 函数链接到主程序 “calc.c”， 需要在命令行上显示地指定该库文件： $ gcc -Wall calc.c /usr/lib/libm.a -o calc 为了避免在命令行上指定长路径名，编译器提供了短选项 “-l” 用于链接库文件。例如去路径指定的苦命可用下述命令代替： $ gcc -Wall calc.c -lm -o calc 通常，编译器选项“-lName” 试图链接标准库目录下的文件名为“libName.a” 中的对象文件。另外可以通过命令行和环境变量指定的目录链接，在大型程序中通常会用到很多 -l 选项，来链接像数学库、图像库和网络等。 2.4.2 共享库虽然上面的例子程序可以被成功编译和链接，但生成的可执行文件要能被载入并运行，还缺少最后一步。如果你试图直接启动该可执行文件，在绝大部分系统上将报错：libgdbm.so.*: cannot open shared object file: No such file or directory。因为 GDBM 软件包提供的共享库在可执行文件运行以前必须先从磁盘上被载入。外部库通常用两种形式提供：静态库和共享库。静态库就是前面看到过的 .a 文件，当程序与一个静态库链接时改程序用到的外部函数（在静态库包含的对象文件中）的机器码被从库中复制到最终生成的可执行文件中。处理共享库（动态链接库）用的是一种更高级的链接形式，它会使得可执行文件比较小。共享库使用 .so 后缀名，表示 共享对象 (shared object)。一个与共享库链接的可执行文件仅仅包含它用到的函数相关的一个表格，而不是外部函数所在的对象文件的整个机器码。在可执行文件开始运行以前，外部函数的机器码由操作系统从磁盘上的该共享库中复制到内存中，这个过程被称为动态链接（dynamic linking）。因为一份库可以在多个程序间共享，所以动态链接使得可执行文件更小，也节省了磁盘空间。绝大部分操作系统提供了虚拟内存机制，该机制允许物理内存中的一份共享库被要用到该库的所有运行的程序共用，节省了内存和磁盘空间。此外，共享库使得升级库时不需要重新编译用到它的程序（只要库提供的接口不变就行）。由于上述优点，在绝大部分系统上gcc编译程序时默认链接到共享库。 2.4.3 链接库搜索路径在命令行上的库的次序遵照像对象文件中的同样的惯例——从左到右搜索，即包含函数定义的库应该出现在任何使用到该函数的源文件和对象文件之后，否则有的编译器会报错。使用库文件，为了得到函数参数和返回值正确类型的声明，必须包括相应的头文件。如果没有函数声明，可能传递错误类型的函数参数，从而导致错误的结果。在编译用到库的程序时，常碰到的一个问题是include的头文件头错误：FILE.h: No such file or directory。如果头文件不在GCC用到的标准库目录中，就会出现这样的错误。搜索文件的目录列表被称为 include路径，而搜索库的目录列表被称为 搜索路径 或 链接路径。在这些路径中的目录是按次序搜索的，例如 /usr/local/include 中找到的头文件优先于 /usr/include 中的同名文件。类似的， /usr/local/lib 中找到的库优先于 /usr/lib 中的同名库。当有其他库被安装到另外的目录中，为了能够按序找到这些库，需要扩展搜索路径。编译器选项 -I 和 -L 用于把新目录添加到各自的include路径和库搜索路径的头上。 通过shell中的环境可以控制头问价和库的搜索路径。除了可以在每次开始shell会话的相应登录文件“.bash_profile”中自动设置，还可以使用环境变量 C_INCLUDE_PATH（针对C的头文件）和 CPP_INCLUDE_PATH（针对C++的头文件）把其他目录添加到 include 路径中。例如，当编译C程序时，下面的命令会把 /opt/gdbm/include 添加到include路径中。 $ C_INCLUDE_PATH=/opt/gdbm/include $ export C_INCLUDE_PATH 该目录将在命令行上用选项 -I 指定的任何目录之后，但在标准默认目录 /usr/local/include 和 /usr/include 之前被搜索。 Shell命令 export 是必要的，以便shell以外的程序也能获得该环境变量。类似的，使用环境变量 LIBRARY_PATH 可以把另外的目录添加到链接路径中去。例如下面的命令会把 /opt/gdbm/lib 添加到链接路径中。该目录将在命令行上用选项 -L 指定的任何目录之后，但在标准默认目录 /usr/local/lib 和 /usr/lib 之前被搜索。 $ LIBRARY_PATH=/opt/gdbm/lib $ export LIBRARY_PATH 环境变量设置好之后，默认路径就包含了环境便令 C_INCLUDE_PATH 和 LIBRARY_PATH 中指定的目录。 遵循标准Unix搜索路径的规范，搜索目录可以在环境变量中用冒号分割的列表形式一起指定：DIR1:DIR2:DIR:3:...，这些目录被依次从左到右搜索。单个点 . 可以用来指示当前目录。在命令行上可以重复使用 -I 和 -L 选项来指定多个搜索路径的目录。在日常的使用情况中，通常用 -I 和 -L 选项把目录添加到搜索路径。当环境变量和命令行选项被同时使用时，编译器按照下面的次序搜索目录: 从左到右搜索由命令行 -I 和 -L 指定的目录 由环境变量指定的目录 默认的系统目录]]></content>
      <categories>
        <category>基础知识</category>
      </categories>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习简介]]></title>
    <url>%2F2018%2F12%2F13%2Fintroduction-ml%2F</url>
    <content type="text"><![CDATA[Tom Mitchell —— Meaching LearningA computer program is said to learn to perform a task T from experience E, if its performance at task T, as measured by a performance metric P, improves with experience E over time. Machine Learning is a subfield within Artificial Intelligence that builds algorithms that allow computers to learn to perform tasks from data instead of being explicitly programmed. 1. 机器学习的应用领域机器学习是人工智能的核心，是使计算机具有智能的根本途径，其应用遍及人工智能的各个领域。机器学习的应用领域十分广泛，例如图像中的目标检测 ( Object Detection )，光学字符识别 ( Optical Character Recognition，OCR )，文本分析中的垃圾邮件过滤 ( Spam Filtering ) 和情感分析 ( Sentiment Analysis )，数据挖掘中的异常值检测 ( Anomaly Detection )和聚类 ( Clustering ) 以及视频中的自动驾驶和机器人技术。 2. 机器学习算法如何工作机器学习是对能通过经验自动改进的计算机算法的研究。一个程序被认为能从经验E中学习，解决任务T，达到性能度量值P，而且仅当有了经验E后，经过P评判，程序在处理任务T时的性能有所提升。 A computer program is said to learn to perform a task T from experience E, if its performance at task T, as measured by a performance metric P, improves with experience E over time. 例如鼎鼎大名的阿尔法围棋 ( AlphaGo ) 通过大量的学习人类棋手的比赛 ( experience E )，最终在围棋比赛 ( task T ) 中战胜 ( performance P ) 了人类的顶尖围棋高手。那么我们可以说 AlphaGo 通过机器学习获得了人工智能 ( Artificial Intelligence )。 想象如下两幅画面：(a) 当你向智能系统输入一张特朗普的照片，系统会输出特朗普的名字；(b) 当你向智能系统输入一段歌声，系统会输出这段歌声所对应的歌词; 在例子(a)中，人脸识别系统的“经验E”就是由包含特朗普的照片和其他照片组成的图像集合，“性能P”就是判断特朗普照片正确的概率。同理，语音识别系统的”经验E”就是每个文字所对应的各种语音数据，“性能P”就是一段语音中文字识别的准确率。 因此为了能够通过机器学习算法训练“智能系统”，我们必须拥有包含许多训练样本 ( training examples ) 的数据集 ( dataset )。通常会将每一个样本 ( sample )表示成一些属性 (attribute) 或者特征 (feature) 的固定集合，以生成计算机能够理解的数据。 3. 机器学习算法的分类从是否使用训练数据标签的角度，机器学习算法可以分为监督学习和非监督学习两大类。监督学习的训练集要求包括输入数据和对应的输出标签，通过已有的训练样本（即已知数据及其对应的输出）去训练得到一个最优模型（这个模型属于某个函数的集合，最优表示某个评价准则下是最佳的），再利用这个模型将所有的输入映射为相应的输出。在非监督学习中，输入数据没有对应的输出标签，也就没有确定的正确结果，需要根据样本间的相似性去发现数据本身的内在规律。在监督学习中，最常用的两种方法是分类 ( Classification ) 和 回归 ( Regression )，分类和回归最本质的区别就是其输出的标签值是否是连续的。假设明天的天气为有雨和没有雨，那么可以将有雨的情况视为0，没有雨的情况视为1，那么输出的值就是离散的( 0和1 )。那么根据今天的天气预测明天有没有雨就可以视为分类。假设不能准确判断明天是否下雨或者不下雨，但是可以根据今天的天气给出明天下雨的概率值，概率值可以在[0, 1]范围内的连续值，值越大，下雨的概率越高。因此根据今天的天气判断明天下雨的概率为0.8就可以视为回归。在非监督学习中，最常用的方法是聚类 ( Clustering )。例如有一堆苹果和橘子装在一个黑箱子里，假设我们事先不知道盒子里是橘子和苹果，那么我们可以根据水果的大小将其分为两类，但是并不清楚每一类究竟是什么水果，这个过程可以视为聚类。 4. 常见机器学习算法的优缺点4.1 朴素贝叶斯算法朴素贝叶斯属于生成式模型（关于生成模型和判别式模型，主要还是在于是否是要求联合分布），如果满足条件独立性假设，朴素贝叶斯分类器的收敛速度将快于判别模型，例如逻辑回归。即使NB条件独立假设不成立，NB分类器在实践中仍然表现的很出色。它的主要缺点是它不能学习特征间的相互作用，比如，虽然你喜欢 A 和 B 主演的电影，但是它不能学习出你不喜欢 A 和 B 在一起演的电影。 优点： 朴素贝叶斯模型发源于古典数学理论，有着坚实的数学基础，以及稳定的分类效率。 对小规模的数据表现很好，能个处理多分类任务，适合增量式训练。 对缺失数据不太敏感，算法也比较简单，常用于文本分类。 缺点： 需要计算先验概率。 分类决策存在错误率。 对输入数据的表达形式很敏感。 4.2 逻辑回归算法逻辑回归 ( Logistic Regression ) 属于判别式模型，有很多正则化模型的方法 (例如 L0、 L1 和 L2 范数等，而且不用像朴素贝叶斯方法那样假设特征之间互不相关。与决策树与支持向量机相比，得到的概率结果可以进行解释，而且可以方便地利用新数据来更新模型。 优点： 实现简单，广泛的应用于工业问题上。 分类时计算量非常小，速度很快，存储资源低。 便利的观测样本概率分数。 对逻辑回归而言，多重共线性并不是问题，它可以结合L2正则化来解决该问题。 缺点： 当特征空间很大时，逻辑回归的性能不是很好。 容易欠拟合，一般准确度不太高。 不能很好地处理大量多类特征或变量。 4.3 线性回归算法线性回归是用于回归的，而不像Logistic回归是用于分类，其基本思想是用梯度下降法对最小二乘法形式的误差函数进行优化，当然也可以使用正规方程 ( normal equation ) 直接求得参数的解。 优点： 实现简单，计算简单。 缺点： 不能拟合非线性数据。 4.4 最近邻算法最近邻 ( K Nearest Neighbor，KNN ) 算法简单易行，具有较强的一致性结果。随着数据趋于无限，算法保证错误率不会超过贝叶斯算法错误率的两倍。对于一些好的K值，K近邻保证错误率不会超过贝叶斯理论误差率。需要根据数据的实际情况选择合适的 K 值。一般情况下，在分类时较大的K值能够减小噪声的影响。但会使类别之间的界限变得模糊。一个较好的K值可通过各种启发式技术来获取，比如，交叉验证。另外噪声和非相关性特征向量的存在会使K近邻算法的准确性减小。 算法的基本流程： 计算训练样本和测试样本中每个样本点的距离； 对所有计算的距离值进行排序； 选前k个最小距离的样本； 根据这k个样本的类别标签进行投票，得到最后的分类类别。 优点： 理论成熟，思想简单，既可以用来做分类也可以用来做回归。 可用于非线性分类。 训练时间复杂度为O(n)。 对数据没有假设，准确度高，对离群点(outlier)不敏感。 缺点： 计算量大。 样本不平衡问题（例如类别A的样本数量很多，而类别B的样本数量则很少）。 需要大量的内存。 4.5 决策树算法决策树易于解释，可以轻松地处理特征间的交互关系，但是不支持在线学习，当有新样本时，决策树需要全部重建。而且决策树容易过拟合，但是随机森林算法通过集成多个决策树可以解决过拟合的问题。 优点： 计算简单，易于理解，可解释性强。 比较适合处理有缺失属性的样本。 能够处理不相关的特征。 在相对短的时间内能够对大型数据源做出可行且效果良好的结果。 缺点： 容易发生过拟合（随机森林可以很大程度上减少过拟合）。 忽略数据之间的相关性。 样本不平衡问题 (决策树的信息增益偏向于具有更多数值的特征)。 4.6 Adaboosting算法Adaboost是一种加和模型，每个模型都是基于上一次模型的错误率来建立的，过分关注分错的样本，而对正确分类的样本减少关注度，逐次迭代之后，可以得到一个相对较好的模型。 优点： Adaboost是一种有很高精度的分类器。 可以使用各种方法构建子分类器，Adaboost算法提供的是框架。 当使用简单分类器时，计算出的结果是可以理解的，并且弱分类器的构造极其简单。 不用做特征筛选，不容易过拟合。 缺点： 对离群点(outlier)较为敏感。 4.7 支持向量机算法支持向量机（Support Vector Machine，SVM）是一种监督式学习的方法，可广泛地应用于统计分类以及回归分析。支持向量机将向量映射到一个更高维的空间里，在这个空间里建立有一个最大间隔超平面。在分开数据的超平面的两边建有两个互相平行的超平面，分隔超平面使两个平行超平面的距离最大化。假定平行超平面间的距离或差距越大，分类器的总误差越小。支持向量机准确率高，为避免过拟合提供了很好的理论保证，而且即使数据在原特征空间线性不可分，也可以通过核函数将原特征空间映射到更高维线性可分的空间，但是内存消耗大，难以解释，运行和调参比较麻烦。 优点： 可以解决高维特征空间问题。 能够处理非线性特征。 无需依赖整个数据。 泛化能力强。 缺点： 数据样本很多时，效率不高。 对非线性问题没有通用解决方案( 很难找到恰当的核函数 )。 对缺失数据敏感。 4.8 神经网络神经网络（Neural Network）是一种运算模型，由大量的节点（或称神经元）相互联接构成。每个节点代表一种特定的输出函数，称为激励函数（activation function）。每两个节点间的连接都代表一个对于通过该连接信号的加权值，称之为权重。网络的输出则依赖于网络的连接方式，权重值和激励函数的不同而不同。而网络自身通常都是对自然界某种算法或者函数的逼近，也可能是对一种逻辑策略的表达。 优点： 分类的准确度高。 并行分布处理能力强，分布存储及学习能力强。 对噪声神经有较强的鲁棒性和容错能力，能充分逼近复杂的非线性关系。 具备联想记忆的功能。 缺点： 神经网络需要大量的参数，如网络拓扑结构、权值和阈值的初始值。 不能观察之间的学习过程，输出结果难以解释，会影响到结果的可信度和可接受程度。 学习时间过长。 4.9 K-Means聚类算法K均值聚类( K-Means Clustering )算法是一种基于样本间相似性度量的间接聚类方法，属于非监督学习方法。该算法以K为参数，把N个对象分为K个簇，以使簇内具有较高的相似度，而且簇间的相似度较低。相似度的计算根据一个簇中对象的平均值（簇的重心）来进行。首先随机选择K个对象，每个对象代表一个聚类的质心。对于其余的每一个对象，根据该对象与各聚类质心之间的距离，把它分配到与之最相似的聚类中。然后，计算每个聚类的新质心。重复上述过程，直到准则函数（例如误差的平方和）收敛。 优点： 算法简单，容易实现。 对处理大数据集，该算法是相对可伸缩的和高效率的，因为它的复杂度大约是O(NKT)，其中N是所有样本的数量，K是簇的类别数，T是迭代次数。 当簇是密集的、球状或团状的，且簇与簇之间区别明显时，聚类效果较好。 缺点： 对数据类型要求较高，适合数值型数据。 可能收敛到局部最小值，在大规模数据上收敛较慢。 难以选取合适的K值。 对初值的簇心值敏感，对于不同的初始值，可能会导致不同的聚类结果。 对于孤立点数据敏感，少量的该类数据能够对平均值产生极大影响。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>meachine learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[你的模型运行快吗？]]></title>
    <url>%2F2018%2F12%2F05%2Fmodel-computation%2F</url>
    <content type="text"><![CDATA[模型占用的存储空间、模型运行时消耗的内存空间、模型运行的速度 1. CNN不同层的计算量了解模型计算量的一种简单方法就是计算这个模型总共做了多少次浮点运算。除了计算量，内存带宽也是影响计算效率的重要因素。 1.1 乘积相加神经网络中的绝大多数操作都是浮点数的乘法进行求和。例如： y = w[0]*x[0] + w[1]*x[1] + w[2]*x[2] + ... + w[n-1]*x[n-1] 上式中，输入 w 和 x 是两个向量，输出 y 是一个标量 ( 实数 )。在神经网络的卷积层和全连接层中， w 是层学习到的参数，x 就是层的输入，而 y 则是层的输出的一部分，因为典型的层结构有多个输出。其中 w[0]*x[0] + ... 称为一次 乘积相加操作(multiply-accumulate operations), 因此两个 n 维向量的点积含有 n 个 MACCs(乘积相加) 计算单元。 Technically speaking there are only n - 1 additions in the above formula, one less than the number of multiplications. Think of the number of MACCs as being an approximation, just like Big-O notation is an approximation of the complexity of an algorithm. 从每秒浮点运算 ( floating point operations per second, FLOPS ) 的角度来看，一次点积操作包含 2n-1 个 FLOPS，因为其中含有 n 次乘法运算和 n-1 次加法运算。 1.2 全连接层在全连接层，所有的输入单元和输出单元相互连接。对于含有 I 个输入值和 J 个输出值的的层，权重 W 存储在 I×J 的矩阵中，因此全连接层的计算可以写作： y = matmul(x, W) + b 矩阵乘法是有一系列点乘组合而成的。每一个点乘由输入 x 和矩阵W 的每一列运算得到。因此运算 matmul(x, W) 包含 I×J 个 MACCs 单元，和权重矩阵的元素个数相同。例如卷积层的最后输出为 (512, 7, 7), 那么经过 faltten 操作后，输入 I=512x7x7。 1.3 激活函数通常层的后面会跟随非线性激活函数，例如 ReLU 或者 sigmoid 函数。由于激活函数没有乘法运算，因此使用 FLOPS 来衡量计算时间。不同激活函数的计算量是不同的。ReLU 激活函数的表达式为： y = max(x, 0) 在 GPU 上只有一次操作。假设全连接层有 J 个输出单元，ReLU函数进行了 J 次最大值操作，因此含有 J 个 FLOPS 单元。Sigmoid 激活函数表达式为： y = 1 / (1 + exp(-x)) 由于 Sigmoid 函数包含幂运算，因此计算量比较复杂。通常将加法运算、减法运算、乘法运算、除法运算、幂运算和平方根运算称为一次 FLOPS。Sigmoid 函数中包含四种不同的运算操作，因此含有 4 次 FLOPS。假设全连接层有 J 个输出单元，那么Sigmoid 函数含有 4xJ 个 FLOPS 单元。通常激活函数只占模型总运算量的很小一部分。 1.3 卷积层卷积层的输入和输出不是向量，而是三维 ( Height*Width*Channels ) 的特征图。假定正方形的卷积核边长为 k，那么卷积层不考虑偏置和激活函数的 MACCs 为： k × k × Channels_in × Height_out × Width_out × Channels_out 这里使用输出的 Height 和 Width 是因为考虑到卷积时的stride, dilation factors, padding, etc。对于卷积核为为 (3, 3, 128) 且输入为 (112, 112, 64) 的卷积计算， 它的 MACCs 为： 3 × 3 × 64 × 112 × 112 × 128 = 924844032 In this example, we used “same” padding and stride = 1, so that the output feature map has the same size as the input feature map. It’s also common to see convolutional layers use stride = 2, which would have chopped the output feature map size in half, and we would’ve used 56 × 56 instead of 112 × 112 in the above calculation. 1.4 深度可分离卷积深度可分离卷积 ( depthwise-separable convolution ) 首先在 Xception 中被使用，它将常规的卷积操作分解为 depthwise 卷积与 pointwise 卷积两个部分。该结构和常规提取特征的卷积操作类似，但是参数量和运算成本较低，在轻量级网络 ( MobileNet )中十分常见。 假设输入层为 (112, 112, 64)，经过 (3, 3, 128)的卷积核，假定使用 same padding 并且 stride=1，使得输入输出特征图大小相同，那么最终得到 (112, 112, 128) 的特征图。常规卷积示意图如下所示。 MACCs 次数为： 3 × 3 × 64 × 112 × 112 × 128 = 924844032 Depthwise Convolution 的每个卷积核只与输入的一个通道进行卷积，卷积核的数量与上一层的通道数相同。因此输入图像经过 depthwise 卷积之后生成 3 个单通道的特征图，如下图所示。 MACCs = K × K × Channels_in × Height_out × Width_out = 3 × 3 × 64 × 112 × 112 = 7225344 Depthwise Convolution 完成后的特征图数量与输入层的通道数相同，无法扩展特征图的数量，而且无法有效利用不同通道在相同空间位置上的特征信息。因此在 depthwise 卷积之后需要 pointwise Convolution 将特征图进行组合生成新的特征图。Pointwise Convolution 的卷积核大小为 1x1。 MACCs = 1 × 1 × Channels_in × Height_out × Width_out x Channels_out = 1 × 1 × 64 × 112 × 112 x 128 = 102760448 因此 depthwise-separable convolution 的 MACCs 为： MACCs = (K × K × Channels_in × Height_out × Width_out) + (Channels_in × Height_out × Width_out × Channels_out) = Channels_in × Height_out × Width_out × (K × K + Channel_out) The exact factor is K × K × Cout / (K × K + Cout) . It should be pointed out that depthwise convolutions sometimes have a stride &gt; 1, which reduces the dimensions of their output feature map. But a pointwise layer usually has stride = 1, and so its output feature map will always have the same dimensions as the depthwise layer’s. 1.5 批量归一化层批量归一化层 ( Batch normalization Layer) 每一个输出的函数表达式可以写为： z = gamma * (y - mean) / sqrt(variance + epsilon) + beta 其中 y 是上一层输出特征图的一个元素，mean 为均值，variance 为方差，epsilon 确保分母不为0，gamma为尺度因子，beta 为偏置。每一个通道都有其对应的值，因此对于通道为 c 的卷积输出层，batch normalization layer 学习的参数量为 4c。 z = gamma * ((x[0]*w[0] + x[1]*w[1] + ... + x[n-1]*w[n-1] + b) - mean) / sqrt(variance + epsilon) + beta 由于在预测过程中移除了 batch normlization layer，因此考虑模型的计算量时可以不用关注正则化层的影响。 This trick only works when the order of the layers is: convolution, batch norm, ReLU — but not when it is: convolution, ReLU, batch norm. The ReLU is a non-linear operation, which messes up the math. 1.6 池化层对于 112, 112, 128) 的特征图，如果最大池化的 pooling size = 2 并且 stride = 2，那么 FLOPS 操作数为 112 × 112 × 128 = 1605632。可以看到，池化层的操作数远远少于卷积层的操作数，因此池化层也是网络计算复杂度的舍入误差。 2. 模型耗费的内存在模型的每一层计算中，硬件设备需要从主存储中读取输入向量或者特征图的值，从主存储中读取权重参数并与输入计算点积，将得到的新向量或者特征图作为结果写入主存储中。这些操作都涉及到大量的内存读写，耗费的时间可能远远大于计算的次数。 2.1 权重的内存层将权重保存在主存储中，这意味着权重参数越少，模型运行速度越快。如前文所述，输入为 I 个神经元和输出为 J 个神经元之间的权重参数为 I x J，加上偏置向量，总的参数为 ( I + 1) x J。对于大小为 k，输入通道数为 Channels_in，输出通道数为 Channels_out 的卷积层的参数为 k x k Channels_in x Channels_out 加上偏置向量参数 Channels_out。对于输入 4096 输出为 4096的全连接层，其权重参数量为 (4096 + 1) x 4096 = 16781312。对于输入为 (64, 64, 32) 卷积核为 (3, 3, 48) 的卷积层，其权重参数量为 (3 x 3 x 32 x 48 + 48 = 13872)。可以看到，相比于卷积层，全连接层的参数量相对更多。 Fully-connected and convolutional layers are actually very similar. A convolutional layer is basically a fully-connected layer with the vast majority of the connections set to 0 — each output is only connected to K × K inputs rather than all of them, and all the outputs use the same values for these connections. This is why convolutional layers are so much more efficient about memory, since they don’t store the weights for connections that are not used.]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Centos7使用Docker快速搭建深度学习环境]]></title>
    <url>%2F2018%2F12%2F04%2Fdocker-env%2F</url>
    <content type="text"><![CDATA[1. 什么是 Docker 解决 “在我的机器上可以正常工作，为什么在你的机器上不能工作” 的问题。 随着深度学习技术的飞速发展，各种深度学习框架都有大量的粉丝。如何在一台电脑上安装多个深度学习框架？同一深度学习框架的不同版本依赖于不同的GPU版本，但是一台服务器只可能安装唯一版本的GPU。当多名开发人员在统一服务器上使用不同的深度学习框架进行开发时，往往会产生环境冲突。最好的解决方案就是采用虚拟技术。 Docker 是世界领先的软件容器平台，也是目前最流行的 Linux 容器解决方案。Docker 将应用程序与该程序的依赖，打包在一个文件里面。运行这个文件，就会生成一个虚拟容器。程序在这个虚拟容器里运行，就好像在真实的物理机上运行一样。有了 Docker，就不用担心环境问题。而且 Docker 的接口相当简单，用户可以方便地创建和使用容器，把自己的应用放入容器。容器还可以进行版本管理、复制、分享、修改，就像管理普通的代码一样。和虚拟机相比，由于没有臃肿的从操作系统，Docker可以节省大量的磁盘空间以及其他系统资源。 虚拟机通常用于彻底隔离整个运行环境。例如，云服务提供商通常采用虚拟机技术隔离不同的用户。而Docker通常用于隔离不同的应用，例如前端，后端以及数据库。 2. 安装 Docker(a) 安装依赖包 $ yum install -y yum-utils device-mapper-persistent-data (b) 配置稳定仓库 $ yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo (c) 安装(默认为最新版) $ yum install docker-ce (4) 修改docker运行时的根目录, 解决存储不足的问题 $ vim /lib/systemd/system/docker.service # 在 ExecStart=/usr/bin/dockerd 后添加 --graph=/home/docker $ ExecStart=/usr/bin/dockerd --graph=/home/docker (5) 重新启动docker服务 $ systemctl daemon-reload $ systemctl status docker $ systemctl start docker (7) 测试Docker是否正确安装 $ docker version $ docker run hello-world 3. 安装 nvidia-docker深度学习框架需要使用 GPU 加入计算，如果不安装 nvidia-docker 工具，那么在容器中将会无法调用宿主机上的 GPU 硬件设备。 (a) 移除旧版本 nvidia-GPU 和已经存在的 GPU 容器 $ docker volume ls -q -f driver=nvidia-docker | xargs -r -I{} -n1 docker ps -q -a -f volume={} | xargs -r docker rm -f $ sudo yum remove nvidia-docker (b) 安装依赖包 $ distribution=$(. /etc/os-release;echo $ID$VERSION_ID) $ curl -s -L https://nvidia.github.io/nvidia-docker/$distribution/nvidia-docker.repo | \ sudo tee /etc/yum.repos.d/nvidia-docker.repo (c) 安装nvidia-docker $ sudo yum install -y nvidia-docker2 $ sudo pkill -SIGHUP dockerd (d) 测试 nvidia-docker 是否安装成功 docker run --runtime=nvidia --rm nvidia/cuda:9.0-base nvidia-smi 4. CUDA9-CUDNN7-python3.6 源码安装 Caffe、Caffe2、Tensorflow、 Detectron 和 Darknet(a) 启动 GPU container 并登录 $ nvidia-docker run -tid --name TestMyGPU --net=&#39;host&#39; nvidia/cuda:9.0-cudnn7-devel-centos7 /bin/bash # 启动容器 $ docker exec -it TestMyGPU /bin/bash # 登录容器 (b) 配置变量 $ export http_proxy=http://xx.xx.xx.xx:8080 # 设置网络 $ export https_proxy=https://xx.xx.xx.xx:8080 $ export PYINSTALL=/usr/local/python3 # 设置python3.6安装路径 $ export PATH=$PYINSTALL/bin:$PATH (c) 安装 python3.6 和 Caffe，参照上一篇博客 Centos7 安装 Caffe。 (d) 在 /home 下安装 Caffe2、Tensorflow、 Detectron 和 Darknet # 安装依赖包和 opencv $ yum install -y cmake3 &amp;&amp; pip3 install cython opencv-python==3.4.2.16 # 安装 nccl，GPU分布式通信函数库 $ cd /home &amp;&amp; git clone https://github.com/NVIDIA/nccl.git $ cd nccl &amp;&amp; make -j8 src.build CUDA_HOME=&#39;/usr/local/cuda-9.0/&#39; NVCC_GENCODE=&quot;-gencode=arch=compute_70,code=sm_70&quot; $ yum install -y rpm-build rpmdevtools &amp;&amp; make -j8 pkg.redhat.build &amp;&amp; make install $ echo &#39;export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/lib&#39; &gt;&gt; /root/.bashrc $ cd /home &amp;&amp; rm -rf nccl &amp;&amp; source ~/.bashrc # 验证nccl是否安装成功 # cd /home &amp;&amp; git clone https://github.com/NVIDIA/nccl-tests.git \ # cd nccl-tests &amp;&amp; make -j8 &amp;&amp; ./build/all_reduce_perf -b 8 -e 256M -f 2 -g 1 \ # cd /home &amp;&amp; rm -rf nccl-tests # 安装 darknet $ cd /home &amp;&amp; git clone https://github.com/pjreddie/darknet.git &amp;&amp; cd darknet \ # 修改 Makefile 文件，令 GPU=1，CUDNN=1，OPENCV=1 $ sed -i &#39;s/GPU=0/GPU=1/&#39; Makefile $ sed -i &#39;s/CUDNN=0/CUDNN=1/&#39; Makefile $ sed -i &#39;s/OPENCV=0/OPENCV=1/&#39; Makefile $ make -j32 # 验证 darknet 是否安装成功 # 执行 ./darknet 输出 usage: ./darknet &lt;function&gt; # 安装 Tensorflow 和 Keras $ cd /home &amp;&amp; pip3 install tensorflow-gpu==1.10 keras==2.2.0 # 安装caffe2 $ pip3 install pyyaml future hypothesis pydot $ cd /home &amp;&amp; git clone https://github.com/pytorch/pytorch.git $ cd pytorch &amp;&amp; git submodule update --init --recursive # 解决编译时 cmake 的版本问题： # 将文件 `pytorch/tools/build_pytorch_libs.sh` 复制到 `/home` 路径下 # 修改./tools/build_pytorch_libs.sh 第31和32行 CMAKE_VERSION、CMAKE3_VERSION # CMAKE_VERSION=$(cmake --version | grep &#39;version&#39; | awk &#39;{print $3}&#39; | awk -F. &#39;{print $1&quot;.&quot;$2&quot;.&quot;$3}&#39;) # CMAKE3_VERSION=$(cmake3 --version | grep &#39;version&#39; | awk &#39;{print $3}&#39; | awk -F. &#39;{print $1&quot;.&quot;$2&quot;.&quot;$3}&#39;) $ rm -f /home/pytorch/tools/build_pytorch_libs.sh $ cp -f /home/build_pytorch_libs.sh /home/pytorch/tools/build_pytorch_libs.sh $ rm -f /home/build_pytorch_libs.sh $ python setup.py install $ cd /home &amp;&amp; rm -rf pytorch 验证 caffe2 是否安装成功， python 命令窗口执行 &gt;&gt;&gt; import torch &gt;&gt;&gt; import caffe2 &gt;&gt;&gt; exit() 验证是否能使用 GPU $ cd /home &amp;&amp; python -c &#39;from caffe2.python import core&#39; 2&gt;/dev/null &amp;&amp; echo &quot;Success&quot; || echo &quot;Failure&quot; $ python -c &#39;from caffe2.python import workspace; print(workspace.NumCudaDevices())&#39; $ python /usr/local/anaconda3/lib/python3.6/site-packages/caffe2/python/operator_test/rnn_cell_test.py 安装 COCO-API $ cd /home &amp;&amp; git clone https://github.com/pdollar/coco $ pip3 install setuptools==18.4 &amp;&amp; yum install -y tkinter $ cd coco/PythonAPI &amp;&amp; make -j8 &amp;&amp; make install &amp;&amp; python setup.py install $ cd /home &amp;&amp; rm -rf coco 验证 coco-api 是否安装成功, python命令窗口执行 from pycocotools.coco import COCO 安装detectron $ cd /home &amp;&amp; git clone https://github.com/facebookresearch/detectron $ cd detectron &amp;&amp; make -j8 # 验证 detectron 是否正确安装 $ cd /home/detectron &amp;&amp; python detectron/tests/test_spatial_narrow_as_op.py $ python tools/infer_simple.py \ --cfg configs/12_2017_baselines/e2e_mask_rcnn_R-101-FPN_2x.yaml \ --output-dir tmp/detectron-visualizations \ --image-ext jpg \ --wts https://s3-us-west-2.amazonaws.com/detectron/35861858/12_2017_baselines/e2e_mask_rcnn_R-101-FPN_2x.yaml.02_32_51.SgT4y1cO/output/train/coco_2014_train:coco_2014_valminusminival/generalized_rcnn/model_final.pkl \ demo 5. 使用 Anaconda3 安装 Caffe、Pytorch 和 TensorflowAnaconda 是一个开源的Python发行版本，包含了conda、Python等180多个科学包及其依赖项，是当前最流行的 Python 数据科学开发平台。 因为包含了大量的科学包，Anaconda 的下载文件比较大（约 531 MB），如果只需要某些包，或者需要节省带宽或存储空间，也可以使用 Miniconda 发行版 ( 仅包含 conda 和 Python )。 Anaconda 当前集成了 caffe 和 pytorch，可以利用 Anaconda 快速安装 caffe 和 caffe2 ( 集成在 pytorch ) 中。由于 Docker 容器需要应用程序占用内存尽可能小，因此采用 Miniconda 代替 Anaconda。完成的 Dockerfile 文件如下，在宿主机中含有 Dockfile 文件的当前路径下运行 docker build -t deep_learning_environment:v0.1 . 即可生成满足深度学习环境对应要求的镜像。 FROM nvidia/cuda:9.0-cudnn7-runtime-centos7 ENV LANG=en_US.UTF-8 ARG http_proxy=http://xx.xx.xx.xx:8080 ARG https_proxy=https://xx.xx.xx.xx:8080 # 安装Miniconda3 RUN cd /home \ # 安装依赖包 &amp;&amp; yum install -y epel-release-7-11.noarch wget git make bzip2 &amp;&amp; pip install cython \ # 安装nccl &amp;&amp; cd /home &amp;&amp; git clone https://github.com/NVIDIA/nccl.git \ &amp;&amp; cd nccl &amp;&amp; make -j8 src.build CUDA_HOME=&#39;/usr/local/cuda-9.0/&#39; NVCC_GENCODE=&quot;-gencode=arch=compute_70,code=sm_70&quot; \ &amp;&amp; yum install -y rpm-build rpmdevtools &amp;&amp; make -j8 pkg.redhat.build &amp;&amp; make install \ &amp;&amp; echo &#39;export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/nccl/build/lib&#39; &gt;&gt; /root/.bashrc \ &amp;&amp; cd /home &amp;&amp; rm -rf nccl &amp;&amp; source ~/.bashrc \ # 验证nccl是否安装成功 # cd /home &amp;&amp; git clone https://github.com/NVIDIA/nccl-tests.git \ # cd nccl-tests &amp;&amp; make -j8 &amp;&amp; ./build/all_reduce_perf -b 8 -e 256M -f 2 -g 1 \ # cd /home &amp;&amp; rm -rf nccl-tests &amp;&amp; wget https://repo.anaconda.com/miniconda/Miniconda3-4.3.30-Linux-x86_64.sh \ &amp;&amp; bash Miniconda3-4.3.30-Linux-x86_64.sh -p /usr/local/miniconda3 -b \ # 将 miniconda 添加到系统路径 &amp;&amp; echo &#39;export PATH=/usr/local/miniconda3/bin:$PATH&#39; &gt;&gt; /root/.bashrc \ &amp;&amp; echo &#39;export LD_LIBRARY_PATH=/usr/local/miniconda3/lib:$LD_LIBRARY_PATH&#39; &gt;&gt; /root/.bashrc \ &amp;&amp; source ~/.bashrc &amp;&amp; rm -rf Miniconda3-4.3.30-Linux-x86_64.sh \ # 修改yum的链接问题 &amp;&amp; ln -s -f /usr/lib64/liblzma.so.5 /usr/local/miniconda3/lib/liblzma.so.5 \ # conda 安装 caffe-gpu &amp;&amp; conda install -y caffe-gpu protobuf \ # conda 安装 caffe2 # 直接安装下载速度非常慢，而且有可能失败 &amp;&amp; conda install -y pytorch-nightly -c pytorch \ &amp;&amp; pip install future hypothesis pydot \ # 验证 caffe 和 caffe2 是否安装成功 # $ python &amp;&amp; import torch &amp;&amp; import caffe &amp;&amp; import caffe2 # python -c &#39;from caffe2.python import core&#39; 2&gt;/dev/null &amp;&amp; echo &quot;Success&quot; || echo &quot;Failure&quot; # python -c &#39;from caffe2.python import workspace; print(workspace.NumCudaDevices())&#39; # python /usr/local/anaconda3/lib/python3.6/site-packages/caffe2/python/operator_test/rnn_cell_test.py # 安装 detectron &amp;&amp; cd /home &amp;&amp; git clone https://github.com/facebookresearch/detectron \ &amp;&amp; cd detectron &amp;&amp; pip install cython &amp;&amp; make -j8 \ # 验证 detectron 是否安装正确 # cd /home/detectron &amp;&amp; python detectron/tests/test_spatial_narrow_as_op.py # 安装darknet, 从 github 上下载darknet源码, 修改 Makefile 文件，令 GPU=1，CUDNN=1，OPENCV=1。 &amp;&amp; cd /home &amp;&amp; git clone https://github.com/pjreddie/darknet.git &amp;&amp; cd darknet \ &amp;&amp; sed -i &#39;s/GPU=0/GPU=1/&#39; Makefile \ &amp;&amp; sed -i &#39;s/CUDNN=0/CUDNN=1/&#39; Makefile \ &amp;&amp; sed -i &#39;s/OPENCV=0/OPENCV=1/&#39; Makefile \ &amp;&amp; make -j8 \ # 验证 darknet 是否安装成功 # 执行 ./darknet 输出 usage: ./darknet &lt;function&gt; # 安装tensorflow和keras &amp;&amp; cd /home &amp;&amp; pip install tensorflow-gpu==1.10 keras==2.2.0 -i https://pypi.douban.com/simple/ \ # 验证 tensorflow 和 keras 是否安装成功 # $ python &amp;&amp; import tensorflow &amp;&amp; import keras # 删除 minicond3/pkgs 里面的安装包, 降低内存占用 &amp;&amp; cd /usr/local/miniconda3/ &amp;&amp; rm -rf pkgs]]></content>
      <categories>
        <category>安装教程</category>
      </categories>
      <tags>
        <tag>centoe7 深度学习环境</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Centos7 安装 Caffe]]></title>
    <url>%2F2018%2F12%2F03%2Fcentos-caffe%2F</url>
    <content type="text"><![CDATA[1. Caffe 是什么Caffe 全称 Convolutional Architecture for Fast Feature Embedding，是一种常用的深度学习框架，主要应用在视频、图像处理方面的应用上。得益于RCNN框架的影响力，当前主流的目标检测模型 ( 例如 Faster-RCNN 和 SSD ) 的作者源码都是基于 Caffe 编写的。 2. Centos7 安装 Caffe虽然网上已经有很多相关的安装教程，但是大多数都是基于 Ubantu 系统的，而且网上的教程在安装过程中往往会报出各种莫名其妙的 bug。经过笔者多次血泪实践，发现大多数错误都是因为未能正确安装 boost 和 protouf 工具包。假设 Centos7 已经正确安装 Nvidia GPU 驱动程序和 CUDA9+CUDNN7的加速包，按照如下教程即可正确编译 Caffe 的 Python3.6 接口。如果电脑没有安装 GPU 驱动，请先参照 Nvidia 官网安装教程 正确安装 GPU。 # 安装依赖包 $ yum clean all &amp;&amp; yum makecache &amp;&amp; yum install -y epel-release-7-11.noarch $ yum -y install zlib-devel openssl-devel bzip2-devel expat-devel $ yum -y install gdbm-devel readline-devel sqlite-devel $ yum -y install wget git make unzip libSM libXrender libXext # 安装 Python3.6 $ cd /home &amp;&amp; wget https://www.python.org/ftp/python/3.6.6/Python-3.6.6.tgz $ tar -xvf Python-3.6.6.tgz &amp;&amp; cd Python-3.6.6 $ ./configure --prefix=$PYINSTALL &amp;&amp; make -j32 &amp;&amp; make install $ ln -s $PYINSTALL/bin/python3 $PYINSTALL/bin/python $ cd /home &amp;&amp; rm -rf Python-3.6.6.tgz Python-3.6.6 # 安装 scikit-image $ pip3 install numpy scikit-image -i http://pypi.douban.com/simple/ --trusted-host pypi.douban.com # 安装 caffe 依赖包 $ yum -y install leveldb-devel snappy-devel opencv-devel hdf5-devel $ yum -y install gflags-devel glog-devel lmdb-devel openblas-devel python36-devel # 编译 boost 修复 libboost_python3.so 无法连接的错误 $ cd /home &amp;&amp; wget https://dl.bintray.com/boostorg/release/1.67.0/source/boost_1_67_0.tar.gz $ tar -xvf boost_1_67_0.tar.gz $ cd boost_1_67_0 &amp;&amp; ./bootstrap.sh --with-toolset=gcc $ ./b2 cflags=&#39;-fPIC&#39; cxxflags=&#39;-fPIC&#39; include=/usr/include/python3.6m &amp;&amp; ./b2 install $ ln -s /usr/local/lib/libboost_python36.so /usr/lib64/libboost_python3.so $ echo /usr/local/lib &gt;&gt; /etc/ld.so.conf.d/caffe.conf &amp;&amp; ldconfig $ cd /home &amp;&amp; rm -rf boost_1_67_0.tar.gz boost_1_67_0 # 安装 protobuf $ echo &#39;export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/lib&#39; &gt;&gt; /root/.bashrc $ source ~/.bashrc $ cd /home &amp;&amp; wget https://github.com/protocolbuffers/protobuf/releases/download/v3.5.1/protobuf-cpp-3.5.1.zip $ unzip protobuf-cpp-3.5.1.zip $ cd protobuf-3.5.1 &amp;&amp; ./configure &amp;&amp; make -j32 &amp;&amp; make install $ cd /home &amp;&amp; rm -rf protobuf-cpp-3.5.1.zip protobuf-3.5.1 $ wget https://github.com/protocolbuffers/protobuf/releases/download/v3.5.1/protobuf-python-3.5.1.zip $ unzip protobuf-python-3.5.1.zip $ cd protobuf-3.5.1/python &amp;&amp; python setup.py build &amp;&amp; python setup.py install $ cd /home &amp;&amp; rm -rf protobuf-python-3.5.1.zip protobuf-3.5.1 安装 caffe， 需要修改配置文件 Makefile.config # 安装 caffe $ cd /home &amp;&amp; git clone https://github.com/bvlc/caffe.git # 将 caffe/Makefile.config.example 文件复制到 /home 路径下，命名为 Makefile.config 并进行修改 # 第05行改为 USE_CUDNN := 1 # 第11行改为 USE_OPENCV := 1 # 第39行改为 CUDA_ARCH := -gencode arch=compute_30,code=sm_30 \ # 第51行改为 BLAS := open # 第55行改为 BLAS_INCLUDE := /usr/include/openblas # 第56行改为 BLAS_LIB := /usr/lib64 $ cp Makefile.config caffe/Makefile.config &amp;&amp; rm -f Makefile.config $ cd caffe &amp;&amp; make -j32 &amp;&amp; make pycaffe -j32 $ cp -r python/caffe /usr/local/python3/lib/python3.6/site-packages $ cp .build_release/lib/* /usr/lib64 在 python 命令窗口中执行 import caffe 查看 caffe 的 python 接口是否编译成功。 # 验证 caffe 是否安装成功 &gt;&gt;&gt; import caffe]]></content>
      <categories>
        <category>安装教程</category>
      </categories>
      <tags>
        <tag>centos</tag>
        <tag>caffe</tag>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hello World——利用hexo和github快速搭建个人博客]]></title>
    <url>%2F2018%2F12%2F01%2Fhello-world%2F</url>
    <content type="text"><![CDATA[一、hexo和github简介 Hexo 生成静态网页，Github 托管网页，Markdown 编辑博客。 1. hexo是什么？Hexo 是一款基于 Node.js 语言、快速、简洁且高效的博客框架。通过使用Markdown（或其他渲染引擎）解析文章，即使是前端小白也可利用 hexo 框架的靓丽主题快速生成相当专业的静态网页。 2. github是什么？GitHub 是一个面向开源及私有软件项目的托管平台，除了git 代码仓库托管及基本的 Web 管理界面以外，还提供了订阅、讨论组、文本渲染、在线文件编辑器、协作图谱（报表）、代码片段分享（Gist）等功能，是当前最活跃的“程序猿交友平台”。 3. markdown是什么？Markdown 是一种可以使用普通文本编辑器编写的标记语言，通过简单的标记语法，它可以使普通文本内容具有一定的格式。Markdown的语法简洁易学，功能比纯文本更强大，世界上最流行的博客平台 WordPress 能很好的支持Markdown。 二、搭建博客环境1. 安装 Node.jsHexo 博客框架基于 Node.js 语言，首先下载 Node.js安装包，选择对应的版本进行安装。默认安装过程会配置环境变量及 npm 包，安装完成后在命令窗口（例如 windows 系统的 cmd 窗口）输入 node -v 即可验证是否安装成功。 2. 安装 GitGit 是开源的分布式版本控制系统，可以将本地编辑完成的博客同步到 Github 服务器上。首先下载 Git安装包，安装完成后在命令窗口输入 git -v 即可验证是否安装成功。 3. 安装 HexoHexo是个人博客网站的框架，安装步骤参考 官网文档。首先在本地建立名为blog的文件夹（文件夹名任意），然后在blog文件夹当前路径下开启命令窗口，通过 npm 命令即可完成安装。 $ npm install -g hexo-cli 安装完成后，在命令窗口中初始化博客。 $ hexo init blog 初始化完成后，分别下述命令检测博客环境是否正常。 $ hexo generate # 生成博客 $ hexo server # 启动本地服务器 hexo 3.0把服务器独立成个别模块，需要单独安装npm i hexo-server。如果没有报错，接下来就是见证奇迹的时刻了。在浏览器中输入网址 http://localhost:4000，就可以看到诞生的第一篇博客。 4. 上传到 Github首先到 官网注册，假定注册的用户名为 user_name，注册的邮箱为 user_email，然后创建一个仓库，设置该仓库的主页面，得到你的github主页面网址 http://user_name.github.io。其他用户在浏览器中输入该网址，就能看到你的主页面。最后编辑站点配置文件 /blog/_config.yml，在该文件的末尾加入： deploy: type: git repository: https://github.com/user_name/user_name.github.io branch: master 在命令窗口运行代码 npm install hexo-deployer-git --save 安装 git 命令部署插件后，执行如下代码： $ git config --global user.name &quot;user_name&quot; # 指定 git 上传的仓库 $ git config --global user.email user_email $ hexo clean # 清理缓存 $ hexo generate # 生成博客 $ hexo deploy # 同步到 github 主页面 5. 绑定个人域名待续 6. 图床加速待续 7. Markdown 编辑工具当前有许多好用的 Markdown 编辑工具，有的收费，有的免费，相对而言收费工具的体验较好。Markdown 文件的后缀名为.md，对于一名程序员来说，最友好的Markdown 编辑界面当然是 IDE 自带的 Markdown 编辑插件。在 Pycharm 中添加 Markdown 插件的步骤如下，File-&gt;Settings-&gt;Plugins-&gt;Install JetBrains Plugins-&gt;输入Markdown-&gt;选择插件-&gt;Install-&gt;安装完成后重启PyCharm。编辑界面如下图所示，黑色背景，支持预览，所见即所得。 Pycharm Markdown 插件编辑效果图 在 VSCodescode 中支持 Markdown 语法，只需要下载 Markdown 预览插件即可。在 VSCode 中添加插件的步骤如下，选择左边栏第四个图标 Extensions，在输入框搜索 Markdown Preview Enhanced，安装成功后重启 VSCode。编辑界面如下图所示，黑色背景，支持预览，所见即所得。 VSCode Markdown 插件编辑效果图]]></content>
      <categories>
        <category>博客</category>
      </categories>
      <tags>
        <tag>hexo 博客</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[为什么写博客]]></title>
    <url>%2F2018%2F11%2F28%2Fwhy-blog%2F</url>
    <content type="text"><![CDATA[刘未鹏 —— 《暗时间》写一个博客有很多的好处，却没有任何明显的坏处。更明确的说：用博客的形式来记录下你有价值的思考，会带来很多好处，却没有任何明显的坏处。写一个长期的价值博客最大的几点好处: 能够交到很多志同道合的朋友。 书写是为了更好地思考。 “教”是最好的“学”。如果一件事情你不能讲清楚，十有八九你还没有完全理解。 激励你去持续学习和思考。 学会持之以恒地做一件事情。 一个长期的价值博客是一份很好的简历。 谨以博客记录算法菜鸟的“攻城狮”之路。]]></content>
  </entry>
</search>
