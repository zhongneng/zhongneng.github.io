<?xml version="1.0" encoding="utf-8"?>
<search> 
  
    
    <entry>
      <title>Pycharm远程调试服务器上的程序</title>
      <link href="/2019/03/16/pycharm-remote-debug/"/>
      <url>/2019/03/16/pycharm-remote-debug/</url>
      
        <content type="html"><![CDATA[<h1 id="1-Pycharm-专业版破解方法"><a href="#1-Pycharm-专业版破解方法" class="headerlink" title="1. Pycharm 专业版破解方法"></a>1. Pycharm 专业版破解方法</h1><blockquote><p>(a) <a href="https://www.jetbrains.com/pycharm/download/#section=windows" target="_blank" rel="noopener">官网</a>下载 pycharm 软件安装包。<br>(b) 下载激<a href="https://blog.csdn.net/weixin_42534818/article/details/84929473" target="_blank" rel="noopener">活码和补丁</a><br>(c) 修改路径 <code>C:\Windows\System32\drivers\etc</code> 下的 <code>host</code> 文件，在最后一行添加 <code>0.0.0.0 account.jetbrains.com</code><br>(d) 安装pycharm，选择 <code>Activation Code</code> 激活选项，粘贴激活码。<br>(e) 在 Pycharm 安装路径 <code>F:\Program Files\JetBrains\PyCharm\bin</code> 下粘贴补丁文件 <code>JetbrainsCrack-release-enc.jar</code>，并分别在文件 <code>pycharm.exe.vmoptions</code> 和 <code>pycharm64.exe.vmoptions</code> 的最后一行追加文本 <code>-javaagent:F:\Program Files\JetBrains\PyCharm\bin\JetbrainsCrack-release-enc.jar</code>，然后保存。<br>(f) 重启 Pycharm，点击工具栏 <code>Help -&gt; About</code>，显示激活时间到 <code>12-31-2099</code> 则表示破解成功。</p></blockquote><h1 id="2-Pycharm-连接服务器"><a href="#2-Pycharm-连接服务器" class="headerlink" title="2. Pycharm 连接服务器"></a>2. Pycharm 连接服务器</h1><p>在服务器上使用 vim 编辑器修改代码十分不方便，因此使用 pycharm 连接服务器，实现在 windows 环境的 pycharm 中修改代码并上传到服务器上执行。首先在服务器上配置 SSH 服务，实现远程登录功能。然后点击 pycharm 工具栏 <code>Tools -&gt; Deployment -&gt; Configuration</code>，在参数配置页面输入服务器用户名和登录密码，设置路径。其中 <code>Connection</code> 页面的 <code>Root Path</code> 和 <code>Mappings</code> 页面的 <code>Deployment path</code> 组成待远程调试代码的全路径。</p><div align="center"><img src="/2019/03/16/pycharm-remote-debug/1.png" alt="Deployment选项" width="400" height="300"></div><div align="center"><img src="/2019/03/16/pycharm-remote-debug/2.png" alt="Connection参数配置页" width="400" height="300"></div><div align="center"><img src="/2019/03/16/pycharm-remote-debug/3.png" alt="Mappings参数配置页" width="400" height="300"></div><p>Deployment配置之后，相当于pycharm拥有了ftp工具可以连接到服务器上，从而可以查看和修改服务器上的文件。点击工具栏 <code>Tools -&gt; Deplotment -&gt; Browse Remote Host</code> 来打开相应的 RemoteHost面板，这个面板显示的就是服务器上的文件，显示的范围是你在Deployment中的Connection选项卡下配置的Root path路径下的文件及文件夹。</p><div align="center"><img src="/2019/03/16/pycharm-remote-debug/4.png" alt="Mappings参数配置页" width="400" height="300"></div><p>此时，可以直接在RemoteHost面板里双击红框中的绿色文件 (绿色文件夹即为上一步设置的工作代码全路径) 进行编辑修改，双击某个文件后你可以看到编辑区域的顶部有一个横条，并且横条的右边有三个按钮，分别是 <code>比较、撤回和提交</code>。当修改完代码文件之后，可以直接点击上传按钮，就会提交到服务器了。</p><h1 id="3-Pycharm-配置服务器上的-python-解释器"><a href="#3-Pycharm-配置服务器上的-python-解释器" class="headerlink" title="3. Pycharm 配置服务器上的 python 解释器"></a>3. Pycharm 配置服务器上的 python 解释器</h1><p>点击pycharm工具栏 <code>File -&gt; Settings -&gt; Project:项目名 -&gt; Project Interpreter -&gt; Project Interpreter</code>，点击右上角齿轮按钮，选择 <code>Add Remote</code> 选项，在配置窗口选择 <code>SSH Credentials</code>，填写服务器和用户信息，确定服务器上的python路径，完成解释器配置。使用虚拟环境中的 Python 解释器时将 python 路径修改为所对应的虚拟环境中的python路径。</p><div align="center"><img src="/2019/03/16/pycharm-remote-debug/5.png" alt="Python解释器参数配置页" width="400" height="300"></div>经过以上步骤，Pycharm的远程解释器就配置好了。此时，就可以直接点击小三角按钮，调用远程服务器上的python解释器来运行代码。代码的输出信息将会在Pycharm上显示，但是执行确是在服务器上。# 4. 具体使用流程首先，在pycharm中通过 `File -> open...` 来打开一个新的项目，项目路径就选择上面 `Mappings`页面的 `local path` 路径。在Project窗口中可以看到该项目还是空的，里面什么也没有。然后，打开RemoteHost小窗口，在 `62/pycharm_remote/math_test.py` 文件上右键，再选择 `Download from Here`，文件就会下载到本地。返回Project小窗口，即可以看到 `math_test.py`。其他的文件类似，如果有父目录，也会将父目录一起下载，所有文件和路径都会和服务器保持一致。此时即可在 Pycharm 上运行该代码。<div align="center"><img src="/2019/03/16/pycharm-remote-debug/6.png" alt="服务器代码文件下载" width="400" height="300"></div><div align="center"><img src="/2019/03/16/pycharm-remote-debug/7.png" alt="本地运行" width="400" height="300"></div>]]></content>
      
      
      <categories>
          
          <category> 安装教程 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> pycharm </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Docker容器配置Jupyter服务</title>
      <link href="/2019/03/16/docker-jupyter/"/>
      <url>/2019/03/16/docker-jupyter/</url>
      
        <content type="html"><![CDATA[<p>在服务器的 Docker 容器中配置 Jupyter 服务，实现在 Windows 浏览器上远程调试代码的功能。</p><pre><code class="lang-bash"># 启动容器时绑定主机和docker的接口，jupyter服务的默认端口是8888NV_GPU=0 nvidia-docker run -tid -v /home/code_directory jupyter:/home/code_directory -p 8877:8888 --name jupyter_serverce centos:7.5 /bin/bash# 登录容器docker exec -it jupyter_serverce /bin/bash --login# 安装 jupyterpip install jupyter notebook# 配置jupyter notebookjupyter notebook --generate-config# 修改配置文件vim ~/.jupyter/jupyter_notebook_config.py# 第83行，允许远程访问c.NotebookApp.allow_remote_access = True# 第85行，允许root启动c.NotebookApp.allow_root = True# 第205行，监听任意的访问IP地址c.NotebookApp.ip = &#39;*&#39;# 第263行，加载默认的notebook文件夹，即容器启动时挂载的主机代码目录c.NotebookApp.notebook_dir = &#39;/home/code_directory&#39;# 第354行，设置默认tokenc.NotebookApp.token = &#39;1357&#39;# 启动notebook# 容器内启动jupyter notebook &amp;# 主机内启动docker exec jupyter_serverce jupyter notebook &amp;# Windows浏览器打开容器中的代码目录# http://[主机IP]:[绑定容器的端口]/tree?token=[配置文件中所设置的c.NotebookApp.token值]http://10.37.2.190:8877/tree?token=1357</code></pre>]]></content>
      
      
      <categories>
          
          <category> 安装教程 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> docker </tag>
            
            <tag> jupyter </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>windows 安装 coco API</title>
      <link href="/2019/03/13/windows-coco-api/"/>
      <url>/2019/03/13/windows-coco-api/</url>
      
        <content type="html"><![CDATA[<p>Linux 安装 COCO API 极为简单，但是由于官方的 COCO API 安装包不提供 Windows 支持，因此使用<a href="https://github.com/philferriere/cocoapi" target="_blank" rel="noopener">大神</a>修改过的支持 Windows 版本的安装包。</p><pre><code class="lang-bash"># 下载$ wget https://github.com/philferriere/cocoapi/archive/master.zip# 解压$ unzip master.zip -d cocoapi_master# 编译$ cd coco/PythonAPI$ python setup.py build_ext --inplace# install pycocotools to the Python site-packages$ python setup.py build_ext install</code></pre>]]></content>
      
      
      <categories>
          
          <category> 目标检测 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> coco API </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Detectron 目标检测结果可视化</title>
      <link href="/2019/03/02/detectron-plot-loss/"/>
      <url>/2019/03/02/detectron-plot-loss/</url>
      
        <content type="html"><![CDATA[<h1 id="1-绘制-detectron-目标检测的-loss-accuracy-曲线"><a href="#1-绘制-detectron-目标检测的-loss-accuracy-曲线" class="headerlink" title="1.  绘制 detectron 目标检测的 loss-accuracy 曲线"></a>1.  绘制 detectron 目标检测的 loss-accuracy 曲线</h1><p>首先使用 <code>tee</code> 命令将训练日志重定向到日志文件 <code>train_info.log</code></p><pre><code class="lang-bash">$ python tools/train_net.py --cfg experiment/fast_rcnn_resnet50_FPN.yaml OUTPUT_DIR experiment/detectron_result | tee experiment/train_info.log</code></pre><p>训练日志的主要内容为</p><pre><code>json_stats: {    &quot;accuracy_cls&quot;: 0.000000,    &quot;eta&quot;: &quot;29 days, 16:00:50&quot;,    &quot;iter&quot;: 0,    &quot;loss&quot;: 6.594362,    ...    &quot;lr&quot;: 0.003333,    &quot;mb_qsize&quot;: 64,    &quot;mem&quot;: 9376,    &quot;time&quot;: 14.240279    }</code></pre><p>根据训练日志绘制 <code>loss-accuracy</code> 曲线的 python 脚本如下：</p><pre><code class="lang-python">import jsonimport reimport matplotlib.pyplot as pltimport numpy as nplog_file = &#39;experiment/train_info.log&#39;with open(log_file) as f:    # 提取信息    pattern = re.compile(&#39;json_stats: {.*}&#39;)    info_list = pattern.findall(f.read())    parsed = None    try:        parsed = [json.loads(&#39;{&#39; + string.split(&#39;{&#39;)[1]) for string in info_list]    except:        print(&#39;Json format is not correct !!!&#39;)        exit(1)    if parsed:        iter = np.array([int(string[&#39;iter&#39;]) for string in parsed])        loss = np.array([int(string[&#39;loss&#39;]) for string in parsed])        accuracy = np.array([float(string[&#39;accuracy_cls&#39;]) for string in parsed])        # 绘制图形        plt.figure(&#39;Figure of Loss-Accuracy&#39;, figsize=(8,6), frameon=True)        plt.plot(iter, accuracy, color=&#39;red&#39;, linestyle=&#39;--&#39;, linewidth=1, label=&quot;accuracy&quot;)        plt.plot(iter, loss, &#39;b-&#39;, lw=1, label=&#39;loss&#39;)        plt.xlim((iter[0], len(iter)))        plt.ylim((0, max(loss)))        plt.title(&#39;The Information of Loss and Accuracy&#39;, color=&#39;green&#39;)        plt.legend(loc=&#39;upper right&#39;)        plt.show()    else:        print(&#39;There is no loss information in file &quot;{:s}&quot;.format(log_name)&#39;)</code></pre><p><code>loss-accuracy</code> 曲线的图形示例如下</p><div align="center"><img src="/2019/03/02/detectron-plot-loss/loss_accuracy.png" alt="Loss-Accuracy 曲线图" width="400" height="300"></div><h1 id="2-检测结果可视化"><a href="#2-检测结果可视化" class="headerlink" title="2. 检测结果可视化"></a>2. 检测结果可视化</h1><pre><code class="lang-bash">$ python tools/visualize_results.py \      --dataset experiment_val \      --detections experiment/detections.pkl \      --output-dir experiment/result_jpg</code></pre>]]></content>
      
      
      <categories>
          
          <category> 目标检测 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> detectron </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>创建私有PIP源</title>
      <link href="/2019/02/16/creat-pip/"/>
      <url>/2019/02/16/creat-pip/</url>
      
        <content type="html"><![CDATA[<h1 id="1-pip-使用国内源"><a href="#1-pip-使用国内源" class="headerlink" title="1. pip 使用国内源"></a>1. pip 使用国内源</h1><p>PIP 命令默认使用国外源，但是下载速度太慢，不仅浪费时间，而且可能出现下载后安装出错的情况。因此将pip安装源替换成国内镜像，不仅可以大幅降低下载时间，还能够提高安装成功率。国内主要镜像源如下，注意当http下载失败时尝试更换https。</p><blockquote><p>阿里源：<a href="http://mirrors.aliyun.com/pypi/simple" target="_blank" rel="noopener">http://mirrors.aliyun.com/pypi/simple</a><br>豆瓣源：<a href="http://pypi.douban.com/simple" target="_blank" rel="noopener">http://pypi.douban.com/simple</a><br>清华源：<a href="https://pypi.tuna.tsinghua.edu.cn/simple" target="_blank" rel="noopener">https://pypi.tuna.tsinghua.edu.cn/simple</a>  </p></blockquote><p>临时使用国内源 <code>pip install numpy -i http://pypi.douban.com/simple</code>，如果报错则在后面添加 <code>trusted-host</code> 参数。</p><pre><code class="lang-bash">$ pip install numpy -i http://pypi.douban.com/simple --trusted-host pypi.douban.com</code></pre><p>如果不想每次使用国内源都添加参数，那么可以修改 pip 配置文件，一劳永逸。首先创建 pip 命令的配置文件 <code>~/.pip/pip.conf</code></p><pre><code class="lang-bash">$ cd ~ &amp;&amp; mkdir .pip &amp;&amp; touch .pip/pip.conf</code></pre><p>然后添加下载源信息并保存，之后即可正常使用命令从国内源下载并安装 python 包，例如 <code>pip install numpy</code></p><blockquote><p>[global]<br>index-url = <a href="http://pypi.douban.com/simple" target="_blank" rel="noopener">http://pypi.douban.com/simple</a><br>[install]<br>trusted-host = pypi.douban.com</p></blockquote><h1 id="2-创建私有的-pip-源"><a href="#2-创建私有的-pip-源" class="headerlink" title="2. 创建私有的 pip 源"></a>2. 创建私有的 pip 源</h1><p>出于安全因素，企业内多数服务器不能访问外网，这些服务可能有安装某些 python package 的需求，所以很有必要搭建企业私有的 <code>pypi</code> 源。</p><h2 id="2-1-使用-pypi-server-创建-pip-源"><a href="#2-1-使用-pypi-server-创建-pip-源" class="headerlink" title="2.1 使用 pypi-server 创建 pip 源"></a>2.1 使用 pypi-server 创建 pip 源</h2><p>连接外网的计算机作为服务器，使用 pypi-server 创建私有 pip 源。该方法简便快捷，但是需要手动下载所需要的安装包，适合独立的小项目使用。</p><pre><code class="lang-bash"># 安装$ pip install pypiserver# 创建安装包存储文件夹$ mkdir /home/packages# copy some source packages or eggs to this directory$ cd /home/packages &amp;&amp; pip download numpy# 启动，不添加参数默认使用 8080 端口和 packages 文件夹$ pypi-server# 指定端口和文件夹启动服务$ pypi-server -p 9090 /home/packages</code></pre><p>无法连接外网的内网计算机作为客户端使用服务器的pip源下载包，假设服务器的 IP 地址为 <code>555.666.777.888</code></p><pre><code class="lang-bash">$ pip install numpy -i http://555.666.777.888:9090/simple --trusted-host 555.666.777.888</code></pre><h2 id="2-2-使用-bandersnatch-创建-pip-源"><a href="#2-2-使用-bandersnatch-创建-pip-源" class="headerlink" title="2.2 使用 bandersnatch 创建 pip 源"></a>2.2 使用 bandersnatch 创建 pip 源</h2><p>bandersnatch 能够自动下载和同步官网所有的安装包到本地 packages，需要非常大的存储空间，适合作为公司的私有源供内部多个项目使用。</p><pre><code class="lang-bash"># 安装$ pip install bandersnatch# 生成配置文件$ bandersnatch mirror# 修改配置文件 bandersnatch.conf# 第3行 directory 参数值同步的 package 在本地存放的位置# 第11行参数 master 指所同步的下载源$ sed -i &quot;3c directory = /home/pypi&quot; /etc/bandersnatch.conf$ sed -i &quot;11c master = http://pypi.python.org&quot; /etc/bandersnatch.conf</code></pre><p>Download and Synchronize, 并将官网源修改为豆瓣源。bandersnatch/master.py 第39行get函数的作用是下载文件，复制 get 函数并修改为 get_cn 函数</p><pre><code class="lang-python">def get_cn(self, path, required_serial, **kw):    logger.debug(f&quot;Getting {path} (serial {required_serial})&quot;)    if not path.startswith((&quot;https://&quot;, &quot;http://&quot;)):        path = self.url + path    # r = self.session.get(path, timeout=self.timeout, **kw)    # url_cn = &quot;http://pypi.douban.com&quot;    url_cn = &quot;http://mirrors.aliyun.com/pypi/simple&quot;    path = path.replace(self.url, url_cn)    r = self.session.get(path, timeout=self.timeout, **kw)    r.raise_for_status()</code></pre><p>将 bandersnatch/package.py 中第357行调用 master.get 方法修改为 master.get_cn</p><pre><code class="lang-python">r = self.mirror.master.get_cn(url, required_serial=None, stream=True)</code></pre><p>重新启动服务</p><pre><code class="lang-bash">$ bandersnatch mirror</code></pre><p>下载和同步需要大量的时间，有时候网速不好可能会中断下载。因此需要监控bandersnatch服务，当下载进程中断时重新启动。<br>创建监控脚本 <code>/home/monitor/bander_snatch.sh</code>，添加如下内容，然后执行该脚 <code>./bander_snatch.sh</code>，如果希望不中断的进行监控，那么执行命令 <code>nohup bander_snatch.sh &amp;</code></p><pre><code class="lang-shell">#!/bin/bashbase_path = $(cd `dirname $0`; pwd)while truedo    process_number = `ps -ef | grep &quot;bandersnatch&quot; | grep -v grep | wc -l`    if [ $process_number -eq 0 ]        then        bandersnatch mirror  # 需要重新执行的命令        echo `date +%Y-%m-%d` `date +%H:%M:%S` &quot;restart bandersnatch&quot; &gt;&gt; $base_path/bandersnatch.log    fi    sleep 300  # 每300秒监控一次done</code></pre><h1 id="3-使用-nginx-发布本地-pip-源"><a href="#3-使用-nginx-发布本地-pip-源" class="headerlink" title="3. 使用 nginx 发布本地 pip 源"></a>3. 使用 nginx 发布本地 pip 源</h1><p>在容器中挂载 pypi 下载包的存储路径，服务器的 IP 地址为 <code>555.666.777.888</code>，容器绑定的主机端口号为 <code>7788</code>，启动 nginx 服务</p><pre><code class="lang-bash">$ docker run -tid -v /home/pypi:/mnt/pypi -p 7788:80 --name nginx_pypi centos:7.5 /bin/bash$ docker exec -it nginx_pypi /bin/bash --login</code></pre><p>进入容器之后，安装 nginx，修改配置文件，添加 pypi 文件夹，并启动 nginx 服务。</p><h2 id="3-1-安装-nginx"><a href="#3-1-安装-nginx" class="headerlink" title="3.1 安装 nginx"></a>3.1 安装 nginx</h2><pre><code class="lang-bash">$ yum install nginx</code></pre><p>在默认情况下，由于 Centos7 中没有 nginx 的下载源，直接使用 yum 无法安装成功。有如下三种解决方案，推荐使用第一种。</p><h3 id="3-1-1-官方yum源"><a href="#3-1-1-官方yum源" class="headerlink" title="3.1.1 官方yum源"></a>3.1.1 官方yum源</h3><pre><code class="lang-bash">$ rpm -Uvh http://nginx.org/packages/centos/7/noarch/RPMS/nginx-release-centos-7-0.el7.ngx.noarch.rpm</code></pre><p>安装完成后，可以在 <code>/etc/yum.repos.d</code> 目录中看到名为 <code>nginx.repo</code> 的文件，其内容如下</p><pre><code class="lang-bash">$ cat /etc/yum.repos.d/nginx.repo# 内容如下    [nginx]    name=nginx repo    baseurl=http://nginx.org/packages/centos/7/$basearch/    gpgcheck=0    enabled=1# 重新安装 nginx$ yum install -y nginx</code></pre><p>安装成功之后即可使用命令 <code>whereis nginx</code> 检查安装目录，nginx各种文件默认路径如下：</p><blockquote><ul><li>(1) Nginx 配置路径： /etc/nginx  </li><li>(2) PID目录：/var/run/nginx.pid  </li><li>(3) 错误日志：/var/log/nginx/error.log  </li><li>(4) 访问日志：/var/log/nginx/access.log  </li><li>(5) 默认站点目录：/usr/share/nginx/html  </li></ul></blockquote><p>事实上，只需要知道 Nginx 配置文件的路径，其他路径均可在 <code>/etc/nginx/nginx.conf</code> 和 <code>/etc/nginx/conf.d/default.conf</code> 中查询到。</p><h3 id="3-1-2-官网方法"><a href="#3-1-2-官网方法" class="headerlink" title="3.1.2 官网方法"></a>3.1.2 官网方法</h3><p>参照 <a href="http://nginx.org/en/linux_packages.html#RHEL-CentOS" target="_blank" rel="noopener">官网</a> 的方法，<br>首先安装依赖文件 <code>yum install yum-utils</code>，然后新建文件 <code>/etc/yum.repos.d/nginx.repo</code>，和4.1中文件相似，添加如下内容。</p><pre><code>[nginx-stable]name=nginx stable repobaseurl=http://nginx.org/packages/centos/$releasever/$basearch/gpgcheck=0enabled=1gpgkey=https://nginx.org/keys/nginx_signing.key[nginx-mainline]name=nginx mainline repobaseurl=http://nginx.org/packages/mainline/centos/$releasever/$basearch/gpgcheck=0enabled=0gpgkey=https://nginx.org/keys/nginx_signing.key</code></pre><p>默认使用 <code>nginx-stable</code> 稳定版本，若要使用 <code>nignx-mainline</code>，则需要修改参数 <code>enabled</code> 的值，若参数 <code>gpgcheck</code> 的值不为0，则需要验证 GPG key。<br>此时，即可重新使用命令 <code>yum install nginx</code> 安装 nginx。</p><h3 id="3-1-3-源码安装"><a href="#3-1-3-源码安装" class="headerlink" title="3.1.3 源码安装"></a>3.1.3 源码安装</h3><pre><code class="lang-bash"># 先检查是否已经安装$ find -name nginx# 卸载$ yum remove nginx# 依赖包$ yum install gcc-c++$ yum install pcre pcre-devel$ yum install zlib zlib-devel$ yum install openssl openssl-devel# 下载源码包$ wget http://nginx.org/download/nginx-1.12.2.tar.gz$ tar -zxvf nginx-1.12.2.tar.gz$ cd nginx-1.12.2$ ./configure &amp;&amp; make &amp;&amp; make install# 启动 nginx$ cd /usr/local/nginx/sbin &amp;&amp; ./nginx</code></pre><h2 id="3-2-Nginx常用命令"><a href="#3-2-Nginx常用命令" class="headerlink" title="3.2 Nginx常用命令"></a>3.2 Nginx常用命令</h2><pre><code class="lang-bash"># 启动$ nginx# 测试Nginx配置是否正确$ nginx -t# 优雅重启$ nginx -s reload# 查看nginx进程号$ ps -ef | grep nginx# 停止$ nginx -s stop</code></pre><p>如果启动 Nginx 时端口被占用，使用如下命令关闭 nginx 进程</p><pre><code class="lang-bash">$ yum install psmisc$ killall -9 nginx</code></pre><h2 id="3-3-配置-nginx-服务"><a href="#3-3-配置-nginx-服务" class="headerlink" title="3.3 配置 nginx 服务"></a>3.3 配置 nginx 服务</h2><p>修改文件 <code>/etc/nginx/nginx.conf</code>，在 http 服务器模块添加 <code>server{}</code> 中的内容</p><pre><code class="lang-bash">http {    ...    # gzip on;    # include /etc/nginx/conf.d/*.conf;    server {        listen *:80;               # 监听端口        server_name localhost;     # 服务器IP地址        root /home/pypi/web;       # python包存储路径        autoindex on;        charset utf-8;    }   }</code></pre><p>配置完成后即可启动 nginx 服务</p><pre><code class="lang-bash">$ nginx</code></pre><h1 id="4-客户端-pip-配置文件"><a href="#4-客户端-pip-配置文件" class="headerlink" title="4. 客户端 pip 配置文件"></a>4. 客户端 pip 配置文件</h1><p>创建 pip 命令配置文件 <code>~/.pip/pip.conf</code></p><pre><code class="lang-bash">$ cd &amp;&amp; mkdir .pip &amp;&amp; touch .pip/pip.conf</code></pre><p>添加下载源</p><blockquote><p>[global]<br>index-url = <a href="http://555.666.777.888:7788/simple" target="_blank" rel="noopener">http://555.666.777.888:7788/simple</a><br>[install]<br>trusted-host = 555.666.777.888</p></blockquote><p>在没有连接外网的客户端使用 pip 命令下载 python 包</p><pre><code class="lang-bash">$ pip install numpy</code></pre>]]></content>
      
      
      <categories>
          
          <category> 安装教程 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> python </tag>
            
            <tag> pip </tag>
            
            <tag> nginx </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>使用setuptools对Python进行打包分发</title>
      <link href="/2019/01/19/python-setuptools/"/>
      <url>/2019/01/19/python-setuptools/</url>
      
        <content type="html"><![CDATA[<h1 id="1-分发工具-setuptools"><a href="#1-分发工具-setuptools" class="headerlink" title="1. 分发工具 setuptools"></a>1. 分发工具 setuptools</h1><p>由于 distutils 无法定义包之间的依赖关系，Python 研究人员开发了增强版的打包工具 setuptools 以便更好的创建和分发 python 包。setuptools 通过添加基本的依赖系统和相关功能，提供了自动包查询程序，用来自动获取包之间的依赖关系，并完成这些包的安装，大大降低了安装各种包的难度。  </p><p>通常安装 python 时会自带 setuptools，如果没有可以使用 pip 安装。setuptools 简单易用，只需写一个简短的 <code>setup.py</code> 安装文件，就可以将开发完成的 Python 应用打包。</p><pre><code class="lang-bash">$ pip install setuptools</code></pre><h1 id="2-使用安装包创建-wheel-文件"><a href="#2-使用安装包创建-wheel-文件" class="headerlink" title="2. 使用安装包创建 wheel 文件"></a>2. 使用安装包创建 wheel 文件</h1><p>在当前目录下新建文件 setup.py，然后创建包 hello 模拟需要打包分发的源码包</p><pre><code>.+-- hello|   +-- hello_world.py|   +-- __init__.py+-- setup.py</code></pre><pre><code class="lang-bash">.├── hello│   ├── hello_world.py    # hello function │   └── __init__.py└── setup.py</code></pre><p>一个最简单的 setup.py 文件内容如下：</p><pre><code class="lang-python">from setuptools import setupsetup(    name = &#39;firstPKG&#39;,    version = &#39;0.01&#39;,    packages=[&#39;hello&#39;],)</code></pre><p>编辑 <code>hello_world.py</code> 文件</p><pre><code class="lang-python">def hello():    print(&#39;Hello world, welcome to setuptools!&#39;)</code></pre><p>wheel 是官方现在推荐的打包方式，首先安装 wheel</p><pre><code class="lang-bash">$ pip install wheel</code></pre><p>然后使用 bdist_wheel 打包</p><pre><code class="lang-bash">$ python setup.py bdist_wheel</code></pre><p>执行成功后，目录下除了 dist 和 *.egg-info 目录外，还有一个 build 目录用于存储打包中间数据。dist 文件夹中 wheel 包的名称如 <code>firstPKG-0.0.1-py3-none-any.whl</code>，其中 py3 指明只支持 Python3。  </p><p>可以使用参数 <code>--universal</code>，包名如 <code>firstPKG-0.0.1-py2.py3-none-any.whl</code>，表明 wheel 包同时支持 Python2 和 Python3。使用 universal 也成为通用 wheel 包，反之称为纯 wheel 包。</p><pre><code class="lang-bash">$ python setup.py bdist_wheel --universal</code></pre><p>最终的目录树如下：</p><pre><code>.├── build│   ├── bdist.linux-x86_64│   └── lib│       └── my_pkgs│           ├── greet.py│           └── __init__.py├── dist│   └── firstPKG-0.0.1-py2.py3-none-any.whl├── firstPKG.egg-info│   ├── dependency_links.txt│   ├── PKG-INFO│   ├── SOURCES.txt│   └── top_level.txt├── my_pkgs│   ├── greet.py│   ├── __init__.py│   └── __pycache__│       ├── greet.cpython-36.pyc│       └── __init__.cpython-36.pyc└── setup.py</code></pre><h1 id="3-安装-wheel-文件"><a href="#3-安装-wheel-文件" class="headerlink" title="3. 安装 wheel 文件"></a>3. 安装 wheel 文件</h1><p>创建 wheel 文件之后，使用 pip 安装到本地 Python 的 site-packages 目录。</p><pre><code class="lang-bash">$ cd dist$ pip install firstPKG-0.0.1-py2.py3-none-any.whl</code></pre><p>进入 python 交互界面，就可以使用安装好的包</p><pre><code class="lang-python">&gt;&gt;&gt; import my_pkgs&gt;&gt;&gt; from my_pkgs import greet&gt;&gt;&gt; from my_pkgs.greet import hello&gt;&gt;&gt; hello&lt;function hello at 0x7fe2cd92b598&gt;&gt;&gt;&gt; hello()Hello, welcome to setuptools!</code></pre><p>应用开发过程中会频繁变更，每次安装都需要先卸载旧版本很麻烦。如果使用 <code>develop</code> 开发模式安装，实际代码不会拷贝到 site-packages 目录下，而是在 site-packages 目录下生成一个指向当前应用的链接 <code>firstPKG.egg-link</code>，使得当前位置的源码改动能够立刻反映到 site-packages 中。</p><pre><code class="lang-bash">$ python setup.py develop</code></pre><p>卸载安装好的包</p><pre><code class="lang-bash">$ pip uninstall firstPKG</code></pre><h1 id="4-将-wheel-文件上传到-PyPI"><a href="#4-将-wheel-文件上传到-PyPI" class="headerlink" title="4. 将 wheel 文件上传到 PyPI"></a>4. 将 wheel 文件上传到 PyPI</h1><p>wheel 包可以自己使用和传输给其他人使用，但是维护更新不方便，而 PyPI 作为 Python 的软件仓库，可以让所有人方便的上传和下载，以及管理三方库。  </p><p>首先登录 <a href="https://pypi.org/" target="_blank" rel="noopener">PyPI官网</a>，注册账号。虽然 setuptools 支持使用 <code>setup.py upload</code> 上传包文件到 PyPI，但只支持 HTTP 而被新的 <font color="#008000">twine</font> 取代。</p><pre><code class="lang-bash">$ pip install twine$ twine upload dist/*</code></pre><p>输入 username 和 password 即上传至 PyPI。如果不想每次输入账号密码，可以在当前目录下创建 .pypirc 文件，内容如下：</p><pre><code>[distutils]index-servers =    pypi    pypitest[pypi]username: password: [pypitest]repository: https://test.pypi.org/legacy/username: password:</code></pre><p>填上自己的账号密码即可，这里配置了官方的 pypi 和 pypitest，若要配置其他仓库，按格式添加。上传成功之后即回到 <a href="https://pypi.org/" target="_blank" rel="noopener">PyPI官网</a> 用户主页即可看到上传的 <code>firstPKG-0.0.1</code>文件。</p><blockquote><p>PyPI 主页显示会有延迟，所以不能马上搜索到结果，pip search 也可能   搜索不到，但已经可以使用 pip 安装。</p></blockquote><h1 id="5-setup-参数详细解释"><a href="#5-setup-参数详细解释" class="headerlink" title="5. setup() 参数详细解释"></a>5. setup() 参数详细解释</h1><h2 id="5-1-完整的-setup-py-示例"><a href="#5-1-完整的-setup-py-示例" class="headerlink" title="5.1 完整的 setup.py 示例"></a>5.1 完整的 setup.py 示例</h2><pre><code class="lang-python">from setuptools import setupfrom setuptools import find_packageswith open(&#39;README.rst&#39;, &#39;r&#39;, encoding=&#39;utf-8&#39;) as rd:    long_description = rd.read()setup(    # 在 PyPI 上搜索的项目名称。    name=&quot;HelloWorld&quot;,     # 项目版本号，一般由三部分组成：MAJOR, MINOR, MAINTENANCE    version=&quot;0.0.1&quot;,    # 列出项目内需要被打包的所有 package。一般使用 setuptools.find_packages() 自动发现    # exclude 用于排除不打包的 package    packages=find_packages(exclude=[&#39;contrib&#39;, &#39;docs&#39;, &#39;tests*&#39;]),    # Project uses reStructuredText, so ensure that the docutils get    # installed or upgraded on the target machine    # 项目依赖的 Python 库，使用 pip 安装本项目时会自动检查和安装依赖。    install_requires=[&#39;docutils&gt;=0.3&#39;],    # 指定项目依赖的 Python 版本    python_requires=&#39;&gt;=3&#39;,    # 项目依赖数据文件，数据文件必须放在项目目录内且使用相对路径。    # 如果不指定作为目录的键为空串，则代表对所有模块操作    package_data={        # If any package contains *.txt or *.rst files, include them:        &#39;&#39;: [&#39;*.txt&#39;, &#39;*.rst&#39;],        # And include any *.msg files found in the &#39;hello&#39; package, too:        &#39;hello&#39;: [&#39;*.msg&#39;],    },    # 如果数据文件存在于项目外，则可以使用 data_files 参数或者 MANIFEST.in 文件进行管理。    # 如果用于源码包，则使用 MANIFEST.in；如果用于 wheel，则使用 data_files。    # 上述设置将在打包 wheel 时，将 data/conf.yml 文件添加至 mydata 目录。    data_files=[(‘mydata’, [‘data/conf.yml’])],    # metadata for upload to PyPI    # 作者信息    author=&quot;example&quot;,    author_email=&quot;example@example.com&quot;,    # 项目的简短描述，一般一句话就好，会显示在 PyPI 上名字下端。    description=&quot;This is an Example Package&quot;,    # 对项目的完整描述，使用 long_description。如果此字符串是 rst 格式的，PyPI 会自动渲染成 HTML 显示。    long_description = long_description,    # 项目许可证    license=&quot;GNU GPLv3&quot;,    # 项目关键词列表    keywords=&quot;hello world example examples&quot;,    # project home page 通常为 GitHub上 的链接或者 readthedocs 的链接。    url=&quot;http://example.com/HelloWorld/&quot;,     # 项目相关额外连接，如代码仓库，文档地址等    project_urls={        &quot;Bug Tracker&quot;: &quot;https://bugs.example.com/HelloWorld/&quot;,        &quot;Documentation&quot;: &quot;https://docs.example.com/HelloWorld/&quot;,        &quot;Source Code&quot;: &quot;https://code.example.com/HelloWorld/&quot;,    })</code></pre><h2 id="5-2-其他初始化文件"><a href="#5-2-其他初始化文件" class="headerlink" title="5.2 其他初始化文件"></a>5.2 其他初始化文件</h2><p>在阅读 Github 上的 Python 库时，除了最基本核心的 setup.py 文件和主程序之外，还会看到其他一些文件。本节将介绍它们的作用和使用方法。</p><p>(1) setup.cfg<br>包含了构建时候的一些默认参数，例如在使用 bdist_wheel 的时候设置默认的 —universal 参数。</p><pre><code>[bdist_wheel]universal = 1</code></pre><p>(2) README.rst/README.md<br>项目说明文档，使用 reStrutruedText 可以在 PyPI 上很好的渲染，但 Markdown 则支持不够好。</p><p>(3) MANIFEST.in<br>决定 setuptools 打包源码时还需要额外打包哪些文件。</p><pre><code># Include the READMEinclude *.md# Include the license fileinclude LICENSE.txt# Include the data filesrecursive-include data *</code></pre><p>(4) LICENSE.txt<br>项目许可说明文件</p><p>setuptools 默认打包的文件包括 <code>README.rst/README.md</code>、<code>setup.cfg</code> 和 <code>MANIFEST.in</code>，所有其他文件，如 <code>LICENSE.txt</code>, 使用源码包时需要在 MANIFEST.in 里添加 include，使用 wheel 包时需要在 setup.cfg 添加</p><pre><code>[metadata]license_file = LICENSE.txt</code></pre><p>PyPI 上传推荐配置</p><pre><code>.├── 项目文件夹├── LICENSE.txt├── MANIFEST.in├── README.rst├── setup.cfg└── setup.py    -- name    -- version    -- author    -- author_email    -- url    -- packages    -- description    -- package_data/data_files</code></pre>]]></content>
      
      
      <categories>
          
          <category> 基础知识 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> python </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>创建python虚拟环境</title>
      <link href="/2019/01/16/python-virtual-env/"/>
      <url>/2019/01/16/python-virtual-env/</url>
      
        <content type="html"><![CDATA[<ol><li>使用 Anaconda 创建 python 虚拟环境</li></ol><p>基于 Python-3.6 版本创建自己的深度学习环境，命名为 MY_DL_ENV，离线环境下需要使用 —offine参数，同时 pip 还是主环境的 pip</p><pre><code class="lang-bash"># 默认安装路径 ../Ananconda3/envs$ conda create --name MY_DL_ENV python=3.6 --offline# 查看所创建的环境并激活，命令行前会出现 &#39;(虚拟环境名)&#39;$ conda env list$ conda activate MY_DL_ENV# 安装所需要的包$ pip install tensorflow-gpu==1.10.0# 退出环境$ conda deactivate MY_DL_ENV# 删除虚拟环境中的包$ conda remove --name MY_DL_ENV numpypip# 删除环境$ conda remove --name MY_DL_ENV --all</code></pre><ol><li>使用 Virtualenv 创建 python 虚拟环境</li></ol><p>Virtualenv 是创建隔绝的Python环境的工具，通过创建一个包含所有必要的可执行文件的文件夹来使用Python工程所需的包。  </p><pre><code class="lang-bash"># 安装 virtualenv$ pip install virtualenv# 创建虚拟环境目录名$ mkdir deep_env$ virtualenv -p /usr/bin/python3 deep_env# 创建完全隔离的Python环境， 设置 --no-site-packages 参数取消对系统 Python 库的引用# 只包含 setuptools、pip、wheel 和 easy_install.py$ virtualenv --no-site-packages deep_env# 激活虚拟环境, 命令行前会出现 &#39;(虚拟环境名)&#39;$ source deep_env/bin/activate# 安装所需要的包$ pip install tensorflow-gpu==1.10.0# 删除安装包$ pip uninstall tensorflow# 退出虚拟环境$ deactivate# 删除虚拟环境$ rm -rf deep_env</code></pre>]]></content>
      
      
      <categories>
          
          <category> 基础教程 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> python </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>提高Python程序性能的建议</title>
      <link href="/2019/01/02/coding-effective-python/"/>
      <url>/2019/01/02/coding-effective-python/</url>
      
        <content type="html"><![CDATA[<h1 id="1-Python为什么慢？"><a href="#1-Python为什么慢？" class="headerlink" title="1. Python为什么慢？"></a>1. Python为什么慢？</h1><p>编程语言的效率：<br>（1） 开发效率（程序员完成编码的时间）；<br>（2） 运行效率（计算机完成计算任务的时间）。</p><div style="text-align:center">    <img src="/2019/01/02/coding-effective-python/1_1.png" alt="漫画" width="400" height="400"></div><h2 id="1-1-Python是动态语言"><a href="#1-1-Python是动态语言" class="headerlink" title="1.1 Python是动态语言"></a>1.1 Python是动态语言</h2><p>动态语言是指程序运行时可以根据某些条件改变自身结构，例如新的函数、对象、甚至代码可以被引进，已有的函数可以被删除或是其他结构上的变化。 运行时结构不可变的语言就是静态语言。</p><div style="text-align:center">    <img src="/2019/01/02/coding-effective-python/1_2.png" alt="数据结构示意图" width="600" height="300"></div><p>在程序执行时，解释器并不知道变量的类型，只知道该变量是某种Python对象。因此解释器必须检查每个变量的PyObject_HEAD才能知道变量类型，然后执行对应的数据操作，最后要创建一个新的Python对象来保存返回值。</p><p>(1) 计算 $a+b$ 的 C++ 命令</p><pre><code class="lang-c++">int a = 1int b = 2int c = a + b</code></pre><p>编译器始终知道a和b是整型，在执行相加运算时，流程如下:</p><pre><code>(a) 首先把1赋值给a，把2赋值给b；  (b) 然后调用 binary_add(a,b)； (c) 最后把结果赋值给c。</code></pre><p>(2) 实现同样功能的Python命令如下:</p><pre><code class="lang-python">a = 1b = 2c = a + b</code></pre><p>编译器始终不知道 a 和 b 的数据类型，在执行相加运算时，流程如下: </p><pre><code>(a) 首先把1赋值给a。     - 设置a-&gt;PyObject_HEAD-&gt;typecode为整型；    - 设置a-&gt;val = 1。(b) 接着把2赋值给b。(c) 然后调用binary_add(a, b)。    - a-&gt;PyObject_HEAD获取类型编码，a为整型；值为a-&gt;val。    - 同理b。    - 调用binary_add(a-&gt;val,b-&gt;val)，结果为整型并存在result中。(c) 最后创建对象c。    - 设c-&gt;PyObject_HEAD-&gt;typecode为整型。    - 设置c-&gt;val为result。</code></pre><p>动态类型意味着任何操作都会涉及更多的步骤，每一个简单的操作都需要大量的指令才能完成。这也是Python等动态语言对数值操作比C语言慢的主要原因。</p><h2 id="1-2-Python中一切都是对象"><a href="#1-2-Python中一切都是对象" class="headerlink" title="1.2 Python中一切都是对象"></a>1.2 Python中一切都是对象</h2><p>Python的对象模型会导致内存效率较低。</p><div style="text-align:center">    <img src="/2019/01/02/coding-effective-python/1_3.png" alt="列表对象存储示意图" width="600" height="300"></div><p>最简单的NumPy数组是根据C语言的数据结构创建的Python对象，它有一个指向连续数据缓存区的指针。而Python的list虽然具有指向连续的指针缓冲区的指针，但是每一个指针都指向一个整数类型的Python对象。如上图所示，如果正在执行按顺序逐步完成数据的操作，numpy的内存布局比Python的内存布局更为高效，因为存储成本和访问的时间成本都更低。</p><h2 id="1-3-Python全局解释器锁"><a href="#1-3-Python全局解释器锁" class="headerlink" title="1.3 Python全局解释器锁"></a>1.3 Python全局解释器锁</h2><p>全局解释器锁（Global Interpreter Lock, GIL）并不是Python的特性，它是在实现Python解析器(CPython)时所引入的概念，Python完全可以不依赖于GIL。每个线程在执行的过程都需要先获取GIL，保证同一时刻只有一个线程对共享资源进行存取，使得CPython中的多线程并不能真正的并发。</p><pre><code>首先了解一下并发和并行的概念：什么是并发什么是并行，他们的区别是什么? 你吃饭吃到一半，电话来了，你一直到吃完了以后才去接，这就说明你不支持并发也不支持并行。你吃饭吃到一半，电话来了，你停了下来接了电话，接完后电话以后继续吃饭，这说明你支持并发。你吃饭吃到一半，电话来了，你一边打电话一边吃饭，这说明你支持并行。并发：交替处理多个任务的能力。并发的关键是你有处理多个任务的能力，不一定要同时。  并行：同时处理多个任务的能力。并行的关键是你有同时处理多个任务的能力，强调的是同时。  </code></pre><p>所以它们最大的区别就是：是否是<font color="#008000">同时</font>处理任务。对于一个多核cpu来说并行显然要比并发快的多，使用多线程和多进程写程序的目的是为了让多核cup发挥最大的功效实现并行处理。  </p><p>为了更有效的利用多核处理器的性能，多线程编码方式应运而生。解决多线程之间数据完整性和状态同步困难问题的最简单方法自然就是加锁。全局解释器锁GIL来控制线程的执行，每一个时刻只允许一个线程执行。Python中并没有实现线程调度，其多线程调度完全依赖于操作系统。所以python多线程编程中没有线程优先级等概念。  </p><p>尽量使用线程进行并发I/O操作，在进程中进行并行计算。  </p><pre><code>（1）在处理像科学计算等需要持续使用cpu的任务时，单线程会比多线程快；（2）在处理像IO操作等可能引起阻塞的任务时，多线程会比单线程快；</code></pre><pre><code class="lang-python">import timefrom threading import Threadfrom multiprocessing import Processfrom concurrent import futures# CPU密集型程序def func(number):    while(number&gt;0):        number -= 1    print(number)def multi_thread(number_thread, function, params):    thread_set = {}    for i in range(number_thread):        t = Thread(target=function, args=(params,))        t.start()        thread_set[i] = t    for j in range(number_thread):        thread_set[j].join()def multi_process(number_process, function, params):    process_set = {}    for i in range(number_process):        p = Process(target=function, args=(params,))        p.start()        process_set[i] = p    for j in range(number_process):        process_set[j].join()def thread_pool(number_works, function, params):    with futures.ThreadPoolExecutor(number_works) as executor:         executor.map(function, params)    # with futures.ProcessPoolExecutor(number_works) as executor:    #    executor.map(function, params)if __name__ == &quot;__main__&quot;:    number = 10000000    number_thread = 2    number_process = 2    number_work = 2    time_start_1 = time.time()    func(number)    time_1 = time.time() - time_start_1    multi_thread(number_thread, func, number)    time_2 = time.time() - time_start_1    multi_process(number_process, func, number)    time_3 = time.time() - time_start_1 - time_start_2    multi_process(number_process, func, number)    time_4 = time.time() - time_start_1 - time_start_2 - time_start_3    print(&#39;Time of func_1 is {0:.4f} seconds&#39;.format(time_1))    print(&#39;Time of func_2 is {0:.4f} seconds&#39;.format(time_2))    print(&#39;Time of func_3 is {0:.4f} seconds&#39;.format(time_3))    print(&#39;Time of func_4 is {0:.4f} seconds&#39;.format(time_4))</code></pre><h1 id="2-Python性能分析方法"><a href="#2-Python性能分析方法" class="headerlink" title="2. Python性能分析方法"></a>2. Python性能分析方法</h1><p>虽然运行速度慢是 Python 与生俱来的特点，大多数时候我们用 Python 就意味着放弃对性能的追求。很多时候，我们将自己的代码运行缓慢地原因<br>归结于python本来就很慢，从而心安理得地放弃深入探究。但是，事实真的是这样吗？面对python代码，你有分析下面这些问题吗：  </p><pre><code>程序运行的速度如何？程序运行时间的瓶颈在哪里？能否稍加改进以提高运行速度呢？</code></pre><p>为了更好了解python程序，我们需要一套工具和方法，方便彻底了解代码，能够记录代码的运行时间，生成性能分析报告，从而对代码进行针对性的优化。</p><h2 id="2-1-什么是性能分析"><a href="#2-1-什么是性能分析" class="headerlink" title="2.1 什么是性能分析"></a>2.1 什么是性能分析</h2><p>性能分析就是分析代码和它正在使用的资源之间有着怎样的关系。例如,性能分析可以告诉你一个指令占用了多少CPU时间,或者整个程序消耗了多少内存。<br>性能分析是通过使用一种被称为性能分析器(profiler)的工具，对程序或者二进制可执行文件的源代码进行调整来完成的。<br>性能分析软件有两类方法论：基于事件的性能分析(event-based profiling)和统计式性能分析(statistical profiling)。</p><p>基于事件的性能分析器(也称为轨迹性能分析器，tracing profiler)是通过收集程序执行过程中的具体事件进行工作的。性能分析器会产生大量的数据，导致其不太实用，在开始对程序进行性能分析时也不是首选。但是,当其他性能分析方法不够用或者不够精确时,它们可以作为最后的选择。</p><p>统计式性能分析器以固定的时间间隔对程序计数器(program counter)进行抽样统计，这样做可以让开发者掌握目标程序在每个函数上消耗的时间。由于它对程序计数器进行抽样,所以数据结果是对真实值的统计近似，不仅能够分析程序的性能细节，查出性能的瓶颈所在，而且分析的数据更少，<br>对性能造成的影响更小。  </p><p>性能分析并不是每个程序都要做的事情，因为其需要花费时间，而且只有在程序中发现了错误的时候才有用。但是，在执行程序之前进行性能分析，可以捕获潜在的bug，为后续的程序调试节省时间。</p><h2 id="2-2-性能分析的内容"><a href="#2-2-性能分析的内容" class="headerlink" title="2.2 性能分析的内容"></a>2.2 性能分析的内容</h2><p>程序的西能分析可以归纳为四个基本问题：  </p><pre><code>（1）它运行的有多块？（2）哪里是速度的瓶颈？（3）它使用了多少内存？（4）哪里发生了内存泄漏？</code></pre><h3 id="2-2-1-运行时间"><a href="#2-2-1-运行时间" class="headerlink" title="2.2.1 运行时间"></a>2.2.1 运行时间</h3><h4 id="2-2-1-1-使用-profile-进行时间分析"><a href="#2-2-1-1-使用-profile-进行时间分析" class="headerlink" title="2.2.1.1 使用 profile 进行时间分析"></a>2.2.1.1 使用 profile 进行时间分析</h4><p>对代码优化的前提是需要了解性能瓶颈在什么地方，程序运行的主要时间是消耗在哪里，对于比较复杂的代码可以借助一些工具来定位，python内置了丰富的性能分析工具，如profile，cProfile与hotshot等。其中Profiler是python自带的一组程序，能够描述程序运行时候的性能，并提供各种统计帮助用户定位程序的性能瓶颈。使用非常简单，只需要在使用之前进行 import 即可。</p><pre><code class="lang-python">def profile_test():    value_list = []    total = 1    for i in range(10):        total = total * (i + 1)        value_list.append(total)    return value_listif __name__ == &quot;__main__&quot;:    import cProfile    cProfile.run(&#39;profile_test()&#39;, &#39;profile_test.txt&#39;)    import pstats    p = pstats.Stats(&#39;profile_test.txt&#39;)    p.sort_stats(&#39;time&#39;).print_stats()</code></pre><p>其中输出每列的具体解释如下:  </p><pre><code>ncalls：表示函数调用的次数；tottime：表示指定函数的总的运行时间，除掉函数中调用子函数的运行时间；percall：（第一个 percall）等于tottime/ncalls；cumtime：表示该函数及其所有子函数的调用运行的时间，即函数开始调用到返回的时间；percall：（第二个 percall）即函数运行一次的平均时间，等于 cumtime/ncalls；filename:lineno(function)：每个函数调用的具体信息；</code></pre><p>如果需要将输出以日志的形式保存，只需要在调用的时候加入另外一个参数。如 profile.run(“profileTest()”,”testprof”)，调用 pstats 模块即可读取日志。</p><div style="text-align:center">    <img src="/2019/01/02/coding-effective-python/1_4.png" alt="profile性能分析结果" width="800" height="200"></div><h4 id="2-2-1-2-使用-line-profile-进行时间分析"><a href="#2-2-1-2-使用-line-profile-进行时间分析" class="headerlink" title="2.2.1.2 使用 line_profile 进行时间分析"></a>2.2.1.2 使用 line_profile 进行时间分析</h4><p>开源工具line_profiler可以统计脚本中每行代码的运行时间和执行次数。通过pip安装该python包：  </p><pre><code class="lang-bash">$ pip install line_profiler</code></pre><p>安装完成之后得到名为 <code>line_profiler</code> 的新模组和 <code>kernprof.py</code> 可执行脚本。使用时不需要导入任何模组，只需要在源代码中被测量的函数上装饰@profile装饰器。kernprof.py脚本将会在执行的时候将模组自动注入到运行脚本中。</p><pre><code class="lang-python">@profiledef primes(n):    if n == 2:        return [2]    elif n &lt; 2:        return []    s = list(range(3, n + 1, 2))    m_root = n ** 0.5    half = (n + 1) / 2 - 1    i = 0    m = 3    while m &lt;= m_root:        if s[i]:            j = int((m * m - 3) / 2)            s[j] = 0            while j &lt; half:                s[j] = 0                j += m        i = i + 1        m = 2 * i + 3    return [2] + [x for x in s if x]</code></pre><p><code>$ kernprof -l -v primes.py</code><br><code>-l</code> 选项通知 kernprof 注入 @profile 装饰器到执行脚本，<code>-v</code> 选项通知kernprof在脚本执行完毕的时候显示统计信息。输出每列的含义如下：  </p><pre><code>Line: 行号Hits: 当前行执行的次数Time: 当前行执行耗费的时间Per Hit: 平均执行一次耗费的时间%Time: 当前行执行时间占总时间的比例Line Contents: 当前行的代码</code></pre><p>具有高Hits值或高Time值的行就是可以通过优化带来最大性能改善的地方。</p><div style="text-align:center">    <img src="/2019/01/02/coding-effective-python/1_5.png" alt="line_profile性能分析结果" width="600" height="400"></div><h3 id="2-2-2-内存资源"><a href="#2-2-2-内存资源" class="headerlink" title="2.2.2 内存资源"></a>2.2.2 内存资源</h3><p>除了运行时间之外，程序所消耗的内存资源也是性能分析需要考虑的问题。内存消耗不仅仅是关注程序使用了多少内存,还应该考虑控制程序使用内存的数量。跟踪程序内存的消耗情况比较简单。最基本的方法就是使用操作系统的任务管理器。它会显示很多信息,包括程序占用的内存数量或者占用总内存的百分比。任务管理器也是检查CPU时间使用情况的好工具。</p><h4 id="2-2-2-1-使用-memory-profile-进行内存分析"><a href="#2-2-2-1-使用-memory-profile-进行内存分析" class="headerlink" title="2.2.2.1 使用 memory_profile 进行内存分析"></a>2.2.2.1 使用 memory_profile 进行内存分析</h4><p>开源工具memory_profiler可以统计脚本所占用的内存以及每行代码所增加的占用内存。通过pip安装该python包:  </p><pre><code class="lang-bash">$ pip install memory_profiler</code></pre><p>安装完成之后得到名为 <code>memory_profiler</code> 的新模组和 <code>memory_profiler.py</code>可执行脚本。<br>安装psutil包：<code>pip install psutil</code> ，因为它可以大大改善 <code>memory_profiler</code> 的性能使用方法和 <code>line_profiler</code> 类似，只需要在感兴趣的函数上面添加@profile装饰器：</p><pre><code class="lang-python">@profiledef primes(n):    pass</code></pre><div style="text-align:center">    <img src="/2019/01/02/coding-effective-python/1_6.png" alt="memory_profile性能分析结果" width="600" height="400"></div><h4 id="2-2-2-2-使用-objgraph-分析内存泄漏"><a href="#2-2-2-2-使用-objgraph-分析内存泄漏" class="headerlink" title="2.2.2.2 使用 objgraph 分析内存泄漏"></a>2.2.2.2 使用 objgraph 分析内存泄漏</h4><p>cPython解释器使用引用计数做为记录内存使用的主要方法。这意味着每个对象都包含一个计数器，当某处对该对象的引用被存储时计数器增加，当引用被删除时计数器递减。当计数器到达零时，cPython解释器认为该对象不再被使用，就会删除对象，释放所占用的内存。如果程序中不再<br>被使用的对象的引用一直被占有，那么就可能会发生内存泄漏。</p><p>开源工具objgraph可以有效查找“内存泄漏”，它允许查看内存中对象的数量，定位含有该对象的引用的所有代码的位置。  </p><pre><code>a. 显示占据python程序内存的头N个对象b. 显示一段时间以后哪些对象被删除，哪些对象增加了c. 显示脚本中某个给定对象的所有引用</code></pre><pre><code class="lang-bash">$ pip install objgraph</code></pre><pre><code class="lang-python">import objgraphif __name__ == &quot;__main__&quot;:    x = [&#39;a&#39;, &#39;1&#39;, [2, 3]]    objgraph.show_refs([x], filename=&#39;test.png&#39;)    objgraph.show_most_common_types()</code></pre><div style="text-align:center">    <img src="/2019/01/02/coding-effective-python/1_7.png" alt=" objgraph 对象引用示意图" width="400" height="300"></div><h1 id="3-实用优化技巧"><a href="#3-实用优化技巧" class="headerlink" title="3. 实用优化技巧"></a>3. 实用优化技巧</h1><h2 id="3-1-编码规范"><a href="#3-1-编码规范" class="headerlink" title="3.1 编码规范"></a>3.1 编码规范</h2><h3 id="3-1-1-了解代码优化的基本原则"><a href="#3-1-1-了解代码优化的基本原则" class="headerlink" title="3.1.1 了解代码优化的基本原则"></a>3.1.1 了解代码优化的基本原则</h3><ol><li><p>优先保证代码是可以工作的<br> 过早优化是编程中一切“罪恶”的根源，过早优化可能会忽视对总体性能指标的把握，忽略可移植性、可读性等</p></li><li><p>权衡优化的代价（质量、时间和成本）<br> 优化是有代价的，想解决所有性能问题几乎是不可能的</p></li><li><p>定义性能指标，集中力量解决首要问题<br> 在进行优化之前，针对问题进行主次排列，集中力量解决主要问题</p></li><li><p>不要忽略可读性<br> 实际应用中，经常运行的代码可能只占很少部分，但是几乎所有代码都需要维护，因此优化时需要考虑可读性和可维护性。</p></li></ol><h3 id="3-1-2-编写函数的四个原则"><a href="#3-1-2-编写函数的四个原则" class="headerlink" title="3.1.2 编写函数的四个原则"></a>3.1.2 编写函数的四个原则</h3><ol><li>函数设计要尽量短小，嵌套层次不宜过深</li><li>函数申明应该做到合理、简单、易于使用</li><li>函数参数设计应该考虑向下兼容</li><li>一个函数制作一件事，尽量保证函数语句粒度的一致性</li></ol><h3 id="3-1-3-在代码中适当添加注释"><a href="#3-1-3-在代码中适当添加注释" class="headerlink" title="3.1.3 在代码中适当添加注释"></a>3.1.3 在代码中适当添加注释</h3><ol><li>使用块注释或者行注释的时候只注释复杂的操作和算法</li><li>注释和代码隔开一定的距离</li><li>给外部可访问的函数和方法添加文档注释</li><li>在文件开头包含版权申明、模块描述和变更记录等信息</li></ol><h2 id="3-2-语法技巧"><a href="#3-2-语法技巧" class="headerlink" title="3.2 语法技巧"></a>3.2 语法技巧</h2><h3 id="3-2-1-数据交换值不推荐使用中间变量"><a href="#3-2-1-数据交换值不推荐使用中间变量" class="headerlink" title="3.2.1 数据交换值不推荐使用中间变量"></a>3.2.1 数据交换值不推荐使用中间变量</h3><p>Python表达式赋值的时候右边操作数先于左边的进行计算，首先创建元组(y, x)，x和y初始化已在内存中，然后通过解压缩将元组依次分配给左边的标识符。</p><pre><code class="lang-python">from timeit import Timertime_1 = Timer(&#39;temp=x; x=y; y=temp&#39;, &#39;x=2; y=3&#39;).timeit()time_2 = Timer(&#39;x, y = y, x&#39;, &#39;x=2; y=3&#39;).timeit()print((time_1-time_2)/time_1)</code></pre><h3 id="3-2-2-充分利用-Lazy-Evaluation-的特性"><a href="#3-2-2-充分利用-Lazy-Evaluation-的特性" class="headerlink" title="3.2.2 充分利用 Lazy Evaluation 的特性"></a>3.2.2 充分利用 Lazy Evaluation 的特性</h3><p>延迟计算仅仅在真正需要执行的时候才计算表达式的值<br>(a) 避免不必要的计算，带来性能上的提升  </p><pre><code class="lang-python">import timedef method(word_list, word_set):    time_start = time.time()    for i in range(1000000):        for word in word_list:            if word in word_set:                pass    time_1 = time.time() - time_start    for i in range(1000000):        for word in word_list:            if word[-1] == &#39;.&#39; and word in word_set:                pass    time_2 = time.time() - time_start - time_1    return (time_1 - time_2) / time_1if __name__ == &quot;__main__&quot;:    word_set = [&#39;aa.&#39;, &#39;bb.&#39;, &#39;cc.&#39;, &#39;dd.&#39;, &#39;ee.&#39;, &#39;ff.&#39;, &#39;gg.&#39;, &#39;hh.&#39;, &#39;ii.&#39;, &#39;jj.&#39;]    word_list = [&#39;aa&#39;, &#39;bb.&#39;, &#39;cc&#39;, &#39;dd.&#39;, &#39;ee&#39;, &#39;ff.&#39;, &#39;gg&#39;]    print(method(word_list, word_set))</code></pre><p>(b) 节省空间，使得无限循环成为可能</p><pre><code class="lang-python">from itertools import islice# 斐波那契数列def fibonacci():    a, b = 0, 1    while True:        yield a        a, b = b, a+bif __name__ == &quot;__main__&quot;:    print(list(islice(fibonacci(), 5)))</code></pre><h3 id="3-2-3-有节制地使用-from…import-语句"><a href="#3-2-3-有节制地使用-from…import-语句" class="headerlink" title="3.2.3 有节制地使用 from…import 语句"></a>3.2.3 有节制地使用 from…import 语句</h3><ol><li>尽量优先使用 <code>import a</code> 形式，使用 a.B 访问模块中的对象</li><li>有节制地使用 <code>from a import B</code> 形式，直接访问 B</li><li>避免使用 <code>from a import *</code>，会污染命名空间，无法清晰地显示导入对象</li></ol><h3 id="3-2-4-使用-with-自动关闭资源"><a href="#3-2-4-使用-with-自动关闭资源" class="headerlink" title="3.2.4 使用 with 自动关闭资源"></a>3.2.4 使用 with 自动关闭资源</h3><p>对文件的操作完成后应该立即关闭文件</p><pre><code class="lang-python">f = open(&#39;test.txt&#39;, &#39;w&#39;)f.write(&#39;test&#39;)with open(&#39;test.txt&#39;, &#39;w&#39;) as f:    f.write(&#39;test&#39;)</code></pre><h3 id="3-2-5-连接字符串应该优先使用-join-而不是"><a href="#3-2-5-连接字符串应该优先使用-join-而不是" class="headerlink" title="3.2.5 连接字符串应该优先使用 join 而不是 +"></a>3.2.5 连接字符串应该优先使用 join 而不是 +</h3><p>python字符串为不可变对象，使用 <code>+</code> 连接字符串时会复制原有的字符串，从而直接导致链接效率降低。</p><pre><code class="lang-python">def func_string(string, string_list):    new_string = string    time_start = time.time()    for i in range(10000):        for sub_string in string_list:            new_string += sub_string    time_1 = time.time() - time_start    for i in range(10000):        new_string += &#39;&#39;.join(string_list)    time_2 = time.time() - time_start - time_1    return ((time_1-time_2)/time_1)if __name__ == &quot;__main__&quot;:    string = &quot;&quot;    string_list = [&#39;aa.&#39;, &#39;bb.&#39;, &#39;cc.&#39;, &#39;dd.&#39;, &#39;ee.&#39;, &#39;ff.&#39;, &#39;gg.&#39;, &#39;hh.&#39;, &#39;ii.&#39;, &#39;jj.&#39;]    print(func_string(string, string_list))</code></pre><div style="text-align:center">    <img src="/2019/01/02/coding-effective-python/1_8.png" alt="字符串连接示意图" width="450" height="250">    <p style="color:green">操作符'+'连接字符串示意图</p></div><h3 id="3-2-6-格式化字符串时尽量使用-format-而不是"><a href="#3-2-6-格式化字符串时尽量使用-format-而不是" class="headerlink" title="3.2.6 格式化字符串时尽量使用 .format 而不是 %"></a>3.2.6 格式化字符串时尽量使用 .format 而不是 %</h3><p>格式化字符串四指根据所规定的转换说明符返回格式化后的字符串  </p><ol><li>format 方式参数的顺序与格式化的顺序不必完全相同</li><li>format 方式可以方便地作为参数传递</li><li>% 最终会被 .format 方式所取代<pre><code class="lang-python"># 在 Pycharm 控制台中执行string_1 = &#39;xxx&#39;string_2 = &#39;yyy&#39;%timeit -n 10000 (&#39;abc%s%s&#39; % (string_1, string_2))%timeit -n 10000 (&#39;abc{0}{1}&#39;.format(string_1, string_2))</code></pre></li></ol><h2 id="3-3-库"><a href="#3-3-库" class="headerlink" title="3.3 库"></a>3.3 库</h2><h3 id="3-3-1-使用-copy-模块拷贝对象"><a href="#3-3-1-使用-copy-模块拷贝对象" class="headerlink" title="3.3.1 使用 copy 模块拷贝对象"></a>3.3.1 使用 copy 模块拷贝对象</h3><p>浅拷贝：构造一个新的符合对象并将从原对象中发现的引用插入该对象中。<br>深拷贝：构造一个新的符合对象，但是遇到引用会继续递归拷贝其所指向的具体内容。</p><pre><code class="lang-python"># 在 Pycharm 控制台中执行import copya = range(100000)%timeit -n 100 copy.copy(a)%timeit -n 100 copy.deepcopy(a)</code></pre><div style="text-align:center">    <img src="/2019/01/02/coding-effective-python/1_9.png" alt="对象copy示意图" width="600" height="400">    <p style="color:green">对象copy示意图</p></div><h3 id="3-3-2-使用-Counter-进行计数统计"><a href="#3-3-2-使用-Counter-进行计数统计" class="headerlink" title="3.3.2 使用 Counter 进行计数统计"></a>3.3.2 使用 Counter 进行计数统计</h3><p>Counter 类属于字典类的子类，是一个容器对象，主要用来统计散列对象，支持集合操作 <code>+ - &amp; |</code></p><pre><code class="lang-python">import timefrom collections import Counterdef count_frequency(data_list):    time_start = time.time()    for i in range(10000):        count_dict = dict()        for item in data_list:            if item in count_dict:                count_dict[item] += 1            else:                count_dict[item] = 1    time_dict = time.time() - time_start    for i in range(10000):        count_set = set(data_list)        count_list = []        for item in count_set:            count_list.append((item, data_list.count(item)))    time_set = time.time() - time_start - time_dict    for i in range(10000):        Counter(data_list)    time_counter = time.time() - time_start - time_dict - time_set    print(&#39;time_dict: {dict:.6}\ntime_set: {set:.6}\ntime_counter: {counter:.6}&#39;.format(        dict=time_dict, set=time_set, counter=time_counter))if __name__ == &#39;__main__&#39;:    data_list = [&#39;a&#39;, &#39;2&#39;, 2, 4, 5, &#39;2&#39;, &#39;b&#39;, 4, 7, &#39;a&#39;, 5, &#39;d&#39;, &#39;a&#39;, &#39;z&#39;]    count_frequency(data_list)</code></pre><h3 id="3-3-3-使用-argparse-处理命令行参数"><a href="#3-3-3-使用-argparse-处理命令行参数" class="headerlink" title="3.3.3 使用 argparse 处理命令行参数"></a>3.3.3 使用 argparse 处理命令行参数</h3><p>相比于参数配置文件，命令行参数更加灵活，用户的学习成本更低。Python标准库中有 getopt、optparse 和 argparse 三个模块实现命令行参数。</p><pre><code class="lang-python">import argparseparser = argparse.ArgumentParser()parser.add_argument(&#39;-o&#39;, &#39;--output&#39;)parser.add_argument(&#39;-v&#39;, dest=&#39;verbose&#39;, action=&#39;store_true&#39;)args = parser.parse_args()</code></pre><h3 id="3-3-4-使用-pandas-处理大型-CSV-文件"><a href="#3-3-4-使用-pandas-处理大型-CSV-文件" class="headerlink" title="3.3.4 使用 pandas 处理大型 CSV 文件"></a>3.3.4 使用 pandas 处理大型 CSV 文件</h3><p>csv模块无法处理大型 CSV 文件，而 pandas 提供了丰富的数据模型，支持多种文件格式处理，包括 CSV、HDF5 和 HTML 等，能够提供高效的大数据处理能力</p><pre><code class="lang-python">import csvf = open(&#39;large.csv&#39;, &#39;w&#39;)f.seek(2**30 - 1)f.write(&quot;\0&quot;)f.close()with open(&#39;large.csv&#39;, &#39;r&#39;) as csv_file:    my_csv = csv.reader(csv_file)    for row in my_csv:        pass</code></pre><p>上述代码会报错，因为csv无力处理大数据，使用pandas处理则不会报错。</p><pre><code class="lang-python">import pandas as pddf = pd.read_csv(&#39;large.csv&#39;)</code></pre><h3 id="3-3-5-使用-ElementTree-解析-XML-文件"><a href="#3-3-5-使用-ElementTree-解析-XML-文件" class="headerlink" title="3.3.5 使用 ElementTree 解析 XML 文件"></a>3.3.5 使用 ElementTree 解析 XML 文件</h3><p><code>xml.dom.minidom</code> 和 <code>xml.sax</code> 作为解析 XML 文件的两种实现，DOM 需要将整个XML文件加载到内存中解析，占用内存多，性能不占优势； SAX 不需要全部载入 XML 文件，但处理过程较为复杂。</p><pre><code class="lang-python">import xml.etree.ElementTree as ETtree = ET.ElementTree(file=&#39;test.xml&#39;)root = tree.getroot()print(root.tag)</code></pre><h3 id="3-3-6-使用-pickle-和-JSON-模块进行序列化"><a href="#3-3-6-使用-pickle-和-JSON-模块进行序列化" class="headerlink" title="3.3.6 使用 pickle 和 JSON 模块进行序列化"></a>3.3.6 使用 pickle 和 JSON 模块进行序列化</h3><p>序列化是指把内存中的数据结构在不丢失其身份和类型信息的情况下转换成对象文本或二进制表示的过程。Python中的序列化模块包括 pickle、json、marshal 和 shelve 等。</p><ol><li><p>pickle 是最通用的序列化模块<br> a. 接口简单，通过 dump() 和 load() 即可轻易实现序列化和反序列化<br> b. pickle 存储格式可以跨平台通用<br> c. 支持的数据类型广泛</p></li><li><p>相比于 pickle 模块，JSON 具有如下优势：<br> a. 文档构成简单，仅存在键值对集合和值的有序列表两种数据结构<br> b. 存储格式可读性更为友好，容易修改<br> c. 支持跨平台，同时也可被其他语言解析<br> d. 用户可以对默认不支持的序列化类型进行扩展</p></li></ol><p>Python 中标准模块 JSON 的性能弱于 pickle 模块</p><pre><code class="lang-python"># 在 Pycharm 控制台中执行import jsonimport pickles_pickle = pickle.dump(range(10000))s_json = json.dump(list(range(10000)))%timeit -n 100 x=pickle.load(s_pickle)%timeit -n 100 x=json.loads(s_json)</code></pre><h2 id="3-4-循环和数据结构"><a href="#3-4-循环和数据结构" class="headerlink" title="3.4 循环和数据结构"></a>3.4 循环和数据结构</h2><h3 id="3-4-1-掌握循环优化的基本技巧"><a href="#3-4-1-掌握循环优化的基本技巧" class="headerlink" title="3.4.1 掌握循环优化的基本技巧"></a>3.4.1 掌握循环优化的基本技巧</h3><ol><li>尽量减少循环过程中的计算量，多重循环时尽量将内层计算提到上一层<br>```python<br>import math<br>import time</li></ol><p>def func_1(iter, number):<br>    sum = 0<br>    for i in range(iter):<br>        d = math.sqrt(number)<br>        sum = i + d</p><pre><code>return sum</code></pre><p>def func_2(iter, number):<br>    sum = 0<br>    d = math.sqrt(number)<br>    for i in range(iter):<br>        sum = i + d</p><pre><code>return sum</code></pre><p>if <strong>name</strong> == “<strong>main</strong>“:<br>    iter = 100000<br>    number = 100</p><pre><code>time_start = time.time()func_1(iter, number)time_1 = time.time() - time_start()func_2(iter, number)time_2 = time.time() - time_start - time_1print((time_1 - time_2) / time_1)</code></pre><pre><code>2. 将显示循环改为隐式循环，需要添加恰当的注释保持代码的可读性```pythonimport mathimport timedef func_1(number):    sum = 0    for i in range(number+1):        sum += i     return sumdef func_2(number):    sum = number * (number+1) / 2    return sumif __name__ == &quot;__main__&quot;:    iter = 1000000    time_start = time.time()    func_1(number)    time_1 = time.time() - time_start()    func_2(number)    time_2 = time.time() - time_start - time_1    print((time_1 - time_2) / time_1)</code></pre><ol><li>在循环中尽量引用局部变量，命名空间中搜索局部变量比全局变量更快<br>```python<br>import math<br>import time</li></ol><p>def func_1(number, list):<br>    for i in range(number+1):<br>        [math.sin(k) for k in list] </p><p>def func_2(number, list):<br>    local_sin = math.sin<br>    for i in range(number+1):<br>        [local.sin(k) for k in list]</p><p>if <strong>name</strong> == “<strong>main</strong>“:<br>    iter = 10000<br>    data_list = range(100)</p><pre><code>time_start = time.time()func_1(number, data_list)time_1 = time.time() - time_start()func_2(number, data_list)time_2 = time.time() - time_start - time_1print((time_1 - time_2) / time_1)</code></pre><pre><code>### 3.4.2 选择合适的数据结构&lt;div style=&quot;text-align:center&quot;&gt;    &lt;img src=&quot;coding-effective-python/1_10.png&quot; alt=&quot;数据结构时间复杂度&quot; width=&quot;600&quot; height=&quot;300&quot;&gt;    &lt;p style=&quot;color:green&quot;&gt;Python数据结构常见操作的时间复杂度&lt;/p&gt;&lt;/div&gt;1. 将列表的交集、并集或者差集等问题转换为集合再运算```pythonimport timedef func_1(list_1, list_2):    intersection = []    for i in range(100000):        for a in list_1:            for b in list_2:                if a == b:                    intersection.append(a)    return intersectiondef func_2(list_1, list_2):    intersection = []    for i in range(100000):        intersection = list(set(list_1) &amp; set(list_2))    return intersectiionif __name__ == &quot;__main__&quot;:    list_1 = [1, 2, 3, 4, 5, 6, 7, 8, 9, 13, 34, 53, 42, 44]    list_2 = [2, 4, 6, 9, 23]    time_start = time.time()    func_1(list_1, list_2)    time_1 = time.time() - time_start()    func_2(list_1, list_2)    time_2 = time.time() - time_start - time_1    print((time_1 - time_2) / time_1)</code></pre><ol><li>使用字典或者集合进行查找<br>```python<h1 id="在-Pycharm-控制台中执行"><a href="#在-Pycharm-控制台中执行" class="headerlink" title="在 Pycharm 控制台中执行"></a>在 Pycharm 控制台中执行</h1>data_list = range(10000)<br>data_set = set(data_list)<br>data_dict = dict((i, 1) for i in data_list)</li></ol><p>%timeit -n 10000 100 in data_list<br>%timeit -n 10000 100 in data_set<br>%timeit -n 10000 100 in data_dict</p><pre><code>### 3.4.4 使用列表解析和生成器表达式列表解析比在列表循环更高效，将列表解析式中 [] 替换成 () 即为生成器表达式```python# 列表循环def func_1(list):    new_list = []    for i in range(100000):        for w in list:            new_list.append(w)# 列表解析def dunc_2(list):    for i in range(100000):        new_list = [w for w in list]# 生成器表达式def func_3(list):    for i in range(100000):        new_list = (w for w in list)if __name__ == &quot;__main__&quot;:    data_list = range(100)    time_start = time.time()    func_1(data_list)    time_1 = time.time() - time_start()    func_2(data_list)    time_2 = time.time() - time_start - time_1    func_3(data_list)    time_3 = time.time() - time_start - time_2 - time_1    print(&#39;func_1: {0:.4f}, func_2: {0:.4f}, func_3: {0:.4f}&#39;.format(        time_1, time_2, time_3))</code></pre><h3 id="3-4-5-使用多进程或者线程池克服-GIL-的缺陷"><a href="#3-4-5-使用多进程或者线程池克服-GIL-的缺陷" class="headerlink" title="3.4.5 使用多进程或者线程池克服 GIL 的缺陷"></a>3.4.5 使用多进程或者线程池克服 GIL 的缺陷</h3><p>因为GIL的存在，Python很难充分利用多核CPU的优势。但是，可以通过内置的模块multiprocessing实现下面几种并行模式:  </p><ol><li>多进程：对于CPU密集型的程序，可以使用multiprocessing的Process,Pool等封装好的类，通过多进程的方式实现并行计算。但是因为进程中的通信成本比较大，对于进程之间需要大量数据交互的程序效率未必有大的提高。</li><li>多线程：对于IO密集型的程序，multiprocessing.dummy模块使用multiprocessing的接口封装threading，使得多线程编程也变得非常轻松(比如可以使用Pool的map接口，简洁高效)。</li><li>线程池和进程池</li><li>分布式：multiprocessing中的Managers类提供了可以在不同进程之共享数据的方式，可以在此基础上开发出分布式的程序。</li></ol><p>不同的业务场景可以选择其中的一种或几种的组合实现程序性能的优化。</p><pre><code class="lang-python">import timefrom threading import Threadfrom multiprocessing import Processfrom concurrent import futures# CPU密集型程序def func(number):    while(number&gt;0):        number -= 1    print(number)def multi_thread(number_thread, function, params):    thread_set = {}    for i in range(number_thread):        t = Thread(target=function, args=(params,))        t.start()        thread_set[i] = t    for j in range(number_thread):        thread_set[j].join()def multi_process(number_process, function, params):    process_set = {}    for i in range(number_process):        p = Process(target=function, args=(params,))        p.start()        process_set[i] = p    for j in range(number_process):        process_set[j].join()def thread_pool(number_works, function, params):    # with futures.ThreadPoolExecutor(number_works) as executor:    #     executor.map(function, params)    with futures.ProcessPoolExecutor(number_works) as executor:        executor.map(function, params)if __name__ == &#39;__main__&#39;:    number = 10000000    number_thread = 2    number_process = 2    multi_work = 2    time_start_1 = time.time()    func(number)    time_1 = time.time() - time_start_1    time_start_2 = time.time()    multi_thread(number_thread, func, number)    time_2 = time.time() - time_start_2    time_start_3 = time.time()    multi_process(number_process, func, number)    time_3 = time.time() - time_start_3    time_start_4 = time.time()    multi_process(number_process, func, number)    time_4 = time.time() - time_start_4    print(&#39;Time of func_1 is {0:.4f} seconds&#39;.format(time_1))    print(&#39;Time of func_2 is {0:.4f} seconds&#39;.format(time_2))    print(&#39;Time of func_3 is {0:.4f} seconds&#39;.format(time_3))    print(&#39;Time of func_4 is {0:.4f} seconds&#39;.format(time_4))</code></pre><h2 id="3-5-使用C扩展"><a href="#3-5-使用C扩展" class="headerlink" title="3.5 使用C扩展"></a>3.5 使用C扩展</h2><ol><li><p>Ctypes<br> 对于关键的性能代码，Python本身也提供了一个API来调用C方法，主要通过 ctypes来实现，因为默认情况下python提供了预编译的标准c库。通常用于封装(wrap)C程序，让纯Python程序调用动态链接库（Windows中的dll或Unix中的so文件）中的函数。如果想要在python中使用已经有C类库，使用ctypes是很好的选择。</p></li><li><p>Cython<br> Cython 是python的一个超集，用于简化编写C扩展的过程，允许通过调用C函数以及声明变量来提高性能。它将类Python代码编译成可以在python文件中调用的C库，后缀使用.pyx替代。 Cython的优点是语法简洁，可以很好地兼容numpy等包含大量C扩展的库。Cython的使得场景一般是针对项目中某个算法或过程的优化。</p></li><li><p>PyPy<br> PyPy是用RPython(CPython的子集)实现的Python，根据官网的基准测试数据，它比CPython实现的Python要快6倍以上。快的原因是使用了Just-in-Time(JIT)编译器，即动态编译器，与静态编译器(如gcc,javac等)不同，它是利用程序运行的过程的数据进行优化。它的运行方式是立即可用的，因此没有疯狂的bash或者运行脚本，只需下载然后运行即可。</p></li></ol><div style="text-align:center">    <img src="/2019/01/02/coding-effective-python/1_11.png" alt="数据结构时间复杂度" width="600" height="300">    <p style="color:green">Python不同实现的性能比较</p></div><pre><code class="lang-python">import timeimport randomfrom ctypes import cdlldef func_1(num):    while num:        yield random.randint(1, 10)        num -= 1def func_2(num):    libc = cdll.msvcrt  #windows    while num:        yield libc.rand(1, 10)        num -= 1if __name__ == &quot;__main__&quot;:    numbers = 1000000    func_list = [func_1, func_2]    for func_name in func_list:        t_start = time.clock()        new_list = sum(func_name(numbers))        t_end = time.clock()        print(&quot;函数 %s 耗费时间 %.4f 秒&quot; % (            str(func_name).split(&#39; &#39;)[1], (t_end-t_start)))</code></pre><h1 id="4-总结"><a href="#4-总结" class="headerlink" title="4. 总结"></a>4. 总结</h1><ol><li>Python程序相对较慢是由其语言自身的特性所决定。</li><li>利用Python的语言特性及其优化库中提供的功能可以对程序进行优化</li><li>程序80%的运算往往在20%的代码中，有针对性地优化该部分代码可以大大提高程序运行效率，例如将关键代码通过Cython或Numba等项目转换成C程序。</li></ol><blockquote><p><strong>思考</strong>   </p><ul><li>优化你最贵的资源  </li><li>把事情做完比快速地做事更加重要 </li><li>任何除了瓶颈之外的改进都是错觉。  </li><li><p>过早优化是万恶之源。  </p></li><li><p>只是因为“快速”而选择语言是过早优化的最终形式。    </p></li><li>选择一种语言/框架/架构来帮助你快速开发。不要仅仅因为它们的运行效率高。  </li><li>当遇到性能问题时，请找到瓶颈所在。  </li><li>你的瓶颈很可能不是 CPU 或者 Python 本身。  </li><li>如果 Python 成为你的瓶颈，尝试优化你的算法，或者转向热门的 Cython 或者 C语言。  </li><li>尽情享受可以快速做完事情的乐趣。</li></ul></blockquote>]]></content>
      
      
      <categories>
          
          <category> 基础知识 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> python </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>GCC编译简介</title>
      <link href="/2019/01/01/compile-and-link/"/>
      <url>/2019/01/01/compile-and-link/</url>
      
        <content type="html"><![CDATA[<h1 id="1-编译-C-程序"><a href="#1-编译-C-程序" class="headerlink" title="1. 编译 C 程序"></a>1. 编译 C 程序</h1><p>GCC (GNU Compiler Collection) 使用可移植性的C语言写成，能够对自身进行编译，因此很容易被移植到新系统上。编译是指把一个纯文本的源代码“程序”转变为机器码，即用于控制计算机的中央处理单元的 1 和 0 的序列。这种机器码被存放在称为“可执行文件”的文件中，有时也称为二进制文件。程序可以编译自单个源文件或多个源文件，还可以用到系统的库文件和头文件。<br>假定已经编写好“Hello World”的C语言程序</p><pre><code class="lang-c">#include &lt;stdio.h&gt;int main(void)    {        printf(&quot;Hello, world!&quot;);        return 0;    }</code></pre><p>那么在linux系统 <code>hello.c</code> 程序文件路径下输入如下命令，即可对“Hello World”的纯文本源码进行编译。  </p><pre><code class="lang-bash">$ gcc -Wall hello.c -o hello</code></pre><p>这样就把“hello.c”中的源代码编译成机器码并存储在可执行文件“hello”中。用“-o”选项可以指定存储机器码的输出文件，该选项通常是命令上的最后一个参数。如果省略，输出将被保存到默认文件“a.out”中。“-Wall”选项打开所有最常用的编译警告，推荐总是使用该选项。  </p><p>一个程序可以被分成多个源文件以便于编辑和理解，由其是在大型程序中。我们可以将 <code>hello.c</code> 文件分为 “main.c” “hello_func.c” 和 “hello.h”，其中“hello.h”文件中仅仅只有一行 <code>void hello(const char *name)</code> ，是对hello函数的原型进行声明。“main.c”为主程序</p><pre><code class="lang-c">#include &quot;hello.h&quot;int main(void)    {        hello(&quot;world&quot;);        return 0;    }</code></pre><p>hello函数自身的定义包含在“hello_func.c”文件中</p><pre><code class="lang-c">#include &lt;stdio.h&gt;#include &quot;hello.h&quot;void hello(const char *name)    {        print(&quot;Hello, %s !&quot;, name)    }</code></pre><p>其中 <code># include &quot;FILE.h&quot;</code> 和 <code># include &lt;FILE.h&gt;</code> 这两种 include 声明形式的含义是有差异的，前者是先在当前目录搜索<code>&quot;FILE.h&quot;</code>，然后再查看包含系统头文件的目录，而后者则是直接搜索系统目录的头文件，默认情况下不会再当前目录下去查找头文件。使用如下命令即可对多个源码文件进行编译</p><pre><code class="lang-bash">$ gcc -Wall main.c hello_func.c -o newhello</code></pre><p>注意，头文件“hello.h”不需要在命令行上的源文件名列表中指定。“hello.h”源码文件中的<code># include</code>指示符会指导编译器在合适的时候自动地包含它。程序中的所有部分已经被组合成单个的可执行文件，其象前面由单个源文件生成的可执行文件一样输出同样的结果。</p><h1 id="2-独立地编译文件"><a href="#2-独立地编译文件" class="headerlink" title="2. 独立地编译文件"></a>2. 独立地编译文件</h1><p>如果整个程序代码被存储在单个源文件中，那么对某个函数的任何改变都需要将整个源码文件重新编译以生成一个新的可执行文件，而重新编译大型源码文件可能需要花费大量的时间。当程序被存储在一个个单独的源码文件中时，只有那些被修改过的源码文件才需要重新编译。通过将源文件分开一个个编译，然后再链接在一起，对大型程序进行修改时可以节约大量时间。<br>在第一阶段，文件被编译但不生成可执行文件，编译的结果被称为对象文件（obj文件），用GCC时有 <code>.o</code> 的后缀名。在第二个阶段，各个对象文件由一个被称为连接器的单独程序合成在一起。连接器把所有的对象文件组合在一起生成单个的可执行文件。<br>对象文件包含的是机器码，其中任何对在其他文件中的函数（或变量）的内存地址的引用都留着没有被解析。这样就允许在互相之间不直接引用的情况下编译各个源代码文件。连接器在生成可执行文件时会填写这些还缺少的地址。</p><h2 id="2-1-从源文件生成对象文件"><a href="#2-1-从源文件生成对象文件" class="headerlink" title="2.1 从源文件生成对象文件"></a>2.1 从源文件生成对象文件</h2><p>命令行选项“-c”用于把源码文件编译成对象文件。例如，下面的命令将把源文件“main.c”编译成一个对象文件</p><pre><code class="lang-bash">$ gcc -Wall -c main.c</code></pre><p>该命令会生成一个包含main函数机器码的对象文件“main.o”。它包含一个队外部函数hello的引用，但在这个阶段该对象文件中的对应的内存地址留着没有被解析（它将在后面链接时被填写）。编译源文件“hello_func.c” 的相应命令为：</p><pre><code class="lang-bash">$ gcc -Wall -c hello_func.c</code></pre><p>在这里不需要用 <code>-o</code> 选项来指定输出文件的文件名。当用 <code>-c</code> 来编译时，编译器会自动生成与源文件同名，但用 <code>.o</code> 来代替原来的扩展名的对象文件。由于 “main.c” 和 “hello_func.c”中的 <code># include</code> 声明，“hello.h”会自动被包括进来，所以在命令行上不需要指定该头文件。</p><h2 id="2-2-从对象文件生成可执行文件"><a href="#2-2-从对象文件生成可执行文件" class="headerlink" title="2.2 从对象文件生成可执行文件"></a>2.2 从对象文件生成可执行文件</h2><p>生成可执行文件的最后步骤是用gcc把各个对象文件链接在一起并补充缺失的外部函数的地址。要把对象文件链接在一起，只需要把他们简单的列在命令行上即可：</p><pre><code class="lang-bash">$ gcc main.o hello_func.o -o hello</code></pre><p>这是几个很少需要用到 “-Wall” 警告选项的场合之一，因为每个源文件已经被成功的被编译成对象文件了。一旦源文件被编译，链接是一个要么成功要么失败的明确过程（只有在有引用不能解析的情况下才会链接失败）。  </p><h2 id="2-3-对象文件的链接次序"><a href="#2-3-对象文件的链接次序" class="headerlink" title="2.3 对象文件的链接次序"></a>2.3 对象文件的链接次序</h2><p>在类Unix系统上，传统上编译器和链接器搜索外部函数的次序是在命令行上指定的对象文件中从左到右的查找，这意味着包含函数定义的对象文件应当出现在调用这些函数的任何文件之后。<br>例如 <code>main.o</code> 调用 <code>hello_func.o</code> 函数，因此包含hello函数的文件“hello_func.o”应该被放在“main.o”之后。</p><pre><code class="lang-bash">$ gcc main.o hello_func.o -o hello</code></pre><p>如果次序搞反了，有的编译器或链接器会报错。虽然当前绝大部分编译器和链接器会不管次序搜索所有的对象文件，但由于不是所有的编译器都这么做，最好遵守从左到右排序对象文件的惯例。如果命令行上已经包括了所有必须的对象文件，但你还是碰到意料之外的未定义引用这种问题，那就应该想想这个问题。</p><h2 id="2-4-与外部库文件链接"><a href="#2-4-与外部库文件链接" class="headerlink" title="2.4 与外部库文件链接"></a>2.4 与外部库文件链接</h2><h3 id="2-4-1-静态库"><a href="#2-4-1-静态库" class="headerlink" title="2.4.1 静态库"></a>2.4.1 静态库</h3><p>库是已经编译好并能被链接入程序的对象文件的集合。库通常被存储在扩展名为 <code>.a</code> 的特殊归档文件中，被称为静态库。标准的系统库通常能在 <code>/usr/lib</code> 和 <code>/lib</code> 目录下找到。例如在类Unix系统中， C的数学库常被放在文件 <code>/usr/lib/math.a</code> 中，而该库中的相应的函数的原型声明在头文件 <code>/usr/include/math.h</code> 中。<br>下面是调用数学库 <code>libm.a</code> 中外部函数 sqrt 的一个例子，假定文件名为“calc.c”：</p><pre><code class="lang-c">#include &lt;math.h&gt;#include &lt;stdio.h&gt;int main(void)    {        double x = sart(2.0);        printf(&quot;The square root of 2.0 is %f&quot;, x);        return 0;    }</code></pre><p>试图只用该源文件就生成可执行文件会导致在链接阶段编译器报错</p><pre><code class="lang-bash">$ gcc -Wall calc.c -o calc</code></pre><p>由于在没有外部数学库  <code>libm.a</code> 的情况下，对函数 sqrt 的引用不能解决。函数 sqrt 并不定义在源程序中或默认的C库 <code>libc.a</code> 中，而且除非  <code>libm.a</code> 被显示指定，否则编译器不会链接该库文件。为了使得编译器能够把 sqrt 函数链接到主程序 “calc.c”， 需要在命令行上显示地指定该库文件：</p><pre><code class="lang-bash">$ gcc -Wall calc.c /usr/lib/libm.a -o calc</code></pre><p>为了避免在命令行上指定长路径名，编译器提供了短选项 “-l” 用于链接库文件。例如去路径指定的苦命可用下述命令代替：</p><pre><code class="lang-bash">$ gcc -Wall calc.c -lm -o calc</code></pre><p>通常，编译器选项“-l<em>Name</em>” 试图链接标准库目录下的文件名为“lib<em>Name</em>.a” 中的对象文件。另外可以通过命令行和环境变量指定的目录链接，在大型程序中通常会用到很多 <code>-l</code> 选项，来链接像数学库、图像库和网络等。  </p><h3 id="2-4-2-共享库"><a href="#2-4-2-共享库" class="headerlink" title="2.4.2 共享库"></a>2.4.2 共享库</h3><p>虽然上面的例子程序可以被成功编译和链接，但生成的可执行文件要能被载入并运行，还缺少最后一步。如果你试图直接启动该可执行文件，在绝大部分系统上将报错：<code>libgdbm.so.*: cannot open shared object file: No such file or directory</code>。因为 GDBM 软件包提供的共享库在可执行文件运行以前必须先从磁盘上被载入。<br>外部库通常用两种形式提供：静态库和共享库。静态库就是前面看到过的 <code>.a</code> 文件，当程序与一个静态库链接时改程序用到的外部函数（在静态库包含的对象文件中）的机器码被从库中复制到最终生成的可执行文件中。<br>处理共享库（动态链接库）用的是一种更高级的链接形式，它会使得可执行文件比较小。共享库使用 <code>.so</code> 后缀名，表示 <em>共享对象 (shared object)</em>。<a href="">一个与共享库链接的可执行文件仅仅包含它用到的函数相关的一个表格，而不是外部函数所在的对象文件的整个机器码</a>。在可执行文件开始运行以前，外部函数的机器码由操作系统从磁盘上的该共享库中复制到内存中，这个过程被称为<em>动态链接（dynamic linking）</em>。<br>因为一份库可以在多个程序间共享，所以动态链接使得可执行文件更小，也节省了磁盘空间。绝大部分操作系统提供了虚拟内存机制，该机制允许物理内存中的一份共享库被要用到该库的所有运行的程序共用，节省了内存和磁盘空间。此外，共享库使得升级库时不需要重新编译用到它的程序（只要库提供的接口不变就行）。由于上述优点，在绝大部分系统上gcc编译程序时默认链接到共享库。</p><h3 id="2-4-3-链接库搜索路径"><a href="#2-4-3-链接库搜索路径" class="headerlink" title="2.4.3 链接库搜索路径"></a>2.4.3 链接库搜索路径</h3><p>在命令行上的库的次序遵照像对象文件中的同样的惯例——从左到右搜索，即包含函数定义的库应该出现在任何使用到该函数的源文件和对象文件之后，否则有的编译器会报错。使用库文件，为了得到函数参数和返回值正确类型的声明，必须包括相应的头文件。如果没有函数声明，可能传递错误类型的函数参数，从而导致错误的结果。<br>在编译用到库的程序时，常碰到的一个问题是include的头文件头错误：<code>FILE.h: No such file or directory</code>。如果头文件不在GCC用到的标准库目录中，就会出现这样的错误。搜索文件的目录列表被称为 <em>include</em>路径，而搜索库的目录列表被称为 <em>搜索路径</em> 或 <em>链接路径</em>。在这些路径中的目录是按次序搜索的，例如 <code>/usr/local/include</code> 中找到的头文件优先于 <code>/usr/include</code> 中的同名文件。类似的， <code>/usr/local/lib</code> 中找到的库优先于 <code>/usr/lib</code> 中的同名库。当有其他库被安装到另外的目录中，为了能够按序找到这些库，需要扩展搜索路径。编译器选项 <code>-I</code> 和 <code>-L</code> 用于把新目录添加到各自的include路径和库搜索路径的头上。</p><p>通过shell中的环境可以控制头问价和库的搜索路径。除了可以在每次开始shell会话的相应登录文件“.bash_profile”中自动设置，还可以使用环境变量 C_INCLUDE_PATH（针对C的头文件）和 CPP_INCLUDE_PATH（针对C++的头文件）把其他目录添加到 include 路径中。例如，当编译C程序时，下面的命令会把 <code>/opt/gdbm/include</code> 添加到include路径中。</p><pre><code class="lang-bash">$ C_INCLUDE_PATH=/opt/gdbm/include$ export C_INCLUDE_PATH</code></pre><p>该目录将在命令行上用选项 <code>-I</code> 指定的任何目录之后，但在标准默认目录 <code>/usr/local/include</code> 和 <code>/usr/include</code> 之前被搜索。 Shell命令 <code>export</code> 是必要的，以便shell以外的程序也能获得该环境变量。<br>类似的，使用环境变量 LIBRARY_PATH 可以把另外的目录添加到链接路径中去。例如下面的命令会把 <code>/opt/gdbm/lib</code> 添加到链接路径中。该目录将在命令行上用选项 <code>-L</code> 指定的任何目录之后，但在标准默认目录 <code>/usr/local/lib</code> 和 <code>/usr/lib</code> 之前被搜索。</p><pre><code class="lang-bash">$ LIBRARY_PATH=/opt/gdbm/lib$ export LIBRARY_PATH</code></pre><p>环境变量设置好之后，默认路径就包含了环境便令 C_INCLUDE_PATH 和 LIBRARY_PATH 中指定的目录。 遵循标准Unix搜索路径的规范，搜索目录可以在环境变量中用冒号分割的列表形式一起指定：<em><code>DIR1:DIR2:DIR:3:...</code></em>，这些目录被依次从左到右搜索。单个点 <code>.</code> 可以用来指示当前目录。在命令行上可以重复使用 <code>-I</code> 和 <code>-L</code> 选项来指定多个搜索路径的目录。在日常的使用情况中，通常用 <code>-I</code> 和 <code>-L</code> 选项把目录添加到搜索路径。当环境变量和命令行选项被同时使用时，编译器按照下面的次序搜索目录:</p><blockquote><ol><li>从左到右搜索由命令行 <code>-I</code> 和 <code>-L</code> 指定的目录</li><li>由环境变量指定的目录</li><li>默认的系统目录</li></ol></blockquote>]]></content>
      
      
      <categories>
          
          <category> 基础知识 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> linux </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>机器学习简介</title>
      <link href="/2018/12/13/introduction-ml/"/>
      <url>/2018/12/13/introduction-ml/</url>
      
        <content type="html"><![CDATA[<blockquote><p> <strong>Tom Mitchell —— <a href="https://en.wikipedia.org/wiki/Machine_learning" target="_blank" rel="noopener">Meaching Learning</a></strong><br>A computer program is said to learn to perform a task T from experience E, if its performance at task T, as measured by a performance metric P, improves with experience E over time.  </p><p>Machine Learning is a subfield within Artificial Intelligence that builds algorithms that allow computers to learn to perform tasks from data instead of being explicitly programmed.</p></blockquote><h1 id="1-机器学习的应用领域"><a href="#1-机器学习的应用领域" class="headerlink" title="1. 机器学习的应用领域"></a>1. 机器学习的应用领域</h1><p>机器学习是人工智能的核心，是使计算机具有智能的根本途径，其应用遍及人工智能的各个领域。机器学习的应用领域十分广泛，例如图像中的目标检测 ( Object Detection )，光学字符识别 (  Optical Character Recognition，OCR )，文本分析中的垃圾邮件过滤 ( Spam Filtering ) 和情感分析 ( Sentiment Analysis )，数据挖掘中的异常值检测 ( Anomaly Detection )和聚类 ( Clustering ) 以及视频中的自动驾驶和机器人技术。</p><h1 id="2-机器学习算法如何工作"><a href="#2-机器学习算法如何工作" class="headerlink" title="2. 机器学习算法如何工作"></a>2. 机器学习算法如何工作</h1><p>机器学习是对能通过经验自动改进的计算机算法的研究。一个程序被认为能从经验E中学习，解决任务T，达到性能度量值P，而且仅当有了经验E后，经过P评判，程序在处理任务T时的性能有所提升。</p><blockquote><p>A computer program is said to learn to perform a task T from experience E, if its performance at task T, as measured by a performance metric P, improves with experience E over time.</p></blockquote><p>例如鼎鼎大名的阿尔法围棋 ( AlphaGo ) 通过大量的学习人类棋手的比赛 ( experience E )，最终在围棋比赛 ( task T ) 中战胜 ( performance P ) 了人类的顶尖围棋高手。那么我们可以说 AlphaGo 通过机器学习获得了人工智能 ( Artificial Intelligence )。  </p><p>想象如下两幅画面：<br>(a) 当你向智能系统输入一张特朗普的照片，系统会输出特朗普的名字；<br>(b) 当你向智能系统输入一段歌声，系统会输出这段歌声所对应的歌词;  </p><p>在例子(a)中，人脸识别系统的“经验E”就是由包含特朗普的照片和其他照片组成的图像集合，“性能P”就是判断特朗普照片正确的概率。同理，语音识别系统的”经验E”就是每个文字所对应的各种语音数据，“性能P”就是一段语音中文字识别的准确率。</p><p>因此为了能够通过机器学习算法训练“智能系统”，我们必须拥有包含许多训练样本 ( training examples ) 的数据集 ( dataset )。通常会将每一个样本 ( sample )表示成一些属性 (attribute) 或者特征 (feature) 的固定集合，以生成计算机能够理解的数据。 </p><h1 id="3-机器学习算法的分类"><a href="#3-机器学习算法的分类" class="headerlink" title="3. 机器学习算法的分类"></a>3. 机器学习算法的分类</h1><p>从是否使用训练数据标签的角度，机器学习算法可以分为监督学习和非监督学习两大类。监督学习的训练集要求包括输入数据和对应的输出标签，通过已有的训练样本（即已知数据及其对应的输出）去训练得到一个最优模型（这个模型属于某个函数的集合，最优表示某个评价准则下是最佳的），再利用这个模型将所有的输入映射为相应的输出。在非监督学习中，输入数据没有对应的输出标签，也就没有确定的正确结果，需要根据样本间的相似性去发现数据本身的内在规律。<br>在监督学习中，最常用的两种方法是<a href="https://en.wikipedia.org/wiki/Statistical_classification" target="_blank" rel="noopener">分类 ( Classification )</a> 和 <a href="https://en.wikipedia.org/wiki/Regression_analysis" target="_blank" rel="noopener"> 回归 ( Regression )</a>，分类和回归最本质的区别就是其输出的标签值是否是连续的。假设明天的天气为有雨和没有雨，那么可以将有雨的情况视为0，没有雨的情况视为1，那么输出的值就是离散的( 0和1 )。那么根据今天的天气预测明天有没有雨就可以视为分类。假设不能准确判断明天是否下雨或者不下雨，但是可以根据今天的天气给出明天下雨的概率值，概率值可以在[0, 1]范围内的连续值，值越大，下雨的概率越高。因此根据今天的天气判断明天下雨的概率为0.8就可以视为回归。在非监督学习中，最常用的方法是<a href="https://en.wikipedia.org/wiki/Cluster_analysis" target="_blank" rel="noopener">聚类 ( Clustering )</a>。例如有一堆苹果和橘子装在一个黑箱子里，假设我们事先不知道盒子里是橘子和苹果，那么我们可以根据水果的大小将其分为两类，但是并不清楚每一类究竟是什么水果，这个过程可以视为聚类。</p><h1 id="4-常见机器学习算法的优缺点"><a href="#4-常见机器学习算法的优缺点" class="headerlink" title="4. 常见机器学习算法的优缺点"></a>4. 常见机器学习算法的优缺点</h1><h2 id="4-1-朴素贝叶斯算法"><a href="#4-1-朴素贝叶斯算法" class="headerlink" title="4.1 朴素贝叶斯算法"></a>4.1 朴素贝叶斯算法</h2><p>朴素贝叶斯属于生成式模型（关于生成模型和判别式模型，主要还是在于是否是要求联合分布），如果满足条件独立性假设，朴素贝叶斯分类器的收敛速度将快于判别模型，例如逻辑回归。即使NB条件独立假设不成立，NB分类器在实践中仍然表现的很出色。它的主要缺点是它不能学习特征间的相互作用，比如，虽然你喜欢 A 和 B 主演的电影，但是它不能学习出你不喜欢 A 和 B 在一起演的电影。</p><blockquote><p>优点：  </p><ul><li>朴素贝叶斯模型发源于古典数学理论，有着坚实的数学基础，以及稳定的分类效率。  </li><li>对小规模的数据表现很好，能个处理多分类任务，适合增量式训练。</li><li>对缺失数据不太敏感，算法也比较简单，常用于文本分类。  </li></ul><p>缺点：  </p><ul><li>需要计算先验概率。</li><li>分类决策存在错误率。  </li><li>对输入数据的表达形式很敏感。</li></ul></blockquote><h2 id="4-2-逻辑回归算法"><a href="#4-2-逻辑回归算法" class="headerlink" title="4.2 逻辑回归算法"></a>4.2 逻辑回归算法</h2><p>逻辑回归 ( Logistic Regression ) 属于判别式模型，有很多正则化模型的方法 (例如 <code>L0</code>、 <code>L1</code> 和 <code>L2</code> 范数等，而且不用像朴素贝叶斯方法那样假设特征之间互不相关。与决策树与支持向量机相比，得到的概率结果可以进行解释，而且可以方便地利用新数据来更新模型。</p><blockquote><p>优点：</p><ul><li>实现简单，广泛的应用于工业问题上。</li><li>分类时计算量非常小，速度很快，存储资源低。</li><li>便利的观测样本概率分数。</li><li>对逻辑回归而言，多重共线性并不是问题，它可以结合L2正则化来解决该问题。</li></ul><p>缺点：</p><ul><li>当特征空间很大时，逻辑回归的性能不是很好。</li><li>容易欠拟合，一般准确度不太高。</li><li>不能很好地处理大量多类特征或变量。 </li></ul></blockquote><h2 id="4-3-线性回归算法"><a href="#4-3-线性回归算法" class="headerlink" title="4.3 线性回归算法"></a>4.3 线性回归算法</h2><p>线性回归是用于回归的，而不像Logistic回归是用于分类，其基本思想是用梯度下降法对最小二乘法形式的误差函数进行优化，当然也可以使用正规方程 ( normal equation ) 直接求得参数的解。</p><blockquote><p>优点：</p><ul><li>实现简单，计算简单。</li></ul><p>缺点：</p><ul><li>不能拟合非线性数据。</li></ul></blockquote><h2 id="4-4-最近邻算法"><a href="#4-4-最近邻算法" class="headerlink" title="4.4 最近邻算法"></a>4.4 最近邻算法</h2><p>最近邻 ( K Nearest Neighbor，KNN ) 算法简单易行，具有较强的一致性结果。随着数据趋于无限，算法保证错误率不会超过贝叶斯算法错误率的两倍。对于一些好的K值，K近邻保证错误率不会超过贝叶斯理论误差率。需要根据数据的实际情况选择合适的 <code>K</code> 值。一般情况下，在分类时较大的K值能够减小噪声的影响。但会使类别之间的界限变得模糊。一个较好的K值可通过各种启发式技术来获取，比如，交叉验证。另外噪声和非相关性特征向量的存在会使K近邻算法的准确性减小。</p><blockquote><p>算法的基本流程：</p><ul><li>计算训练样本和测试样本中每个样本点的距离；</li><li>对所有计算的距离值进行排序；</li><li>选前k个最小距离的样本；</li><li>根据这k个样本的类别标签进行投票，得到最后的分类类别。</li></ul><p>优点：</p><ul><li>理论成熟，思想简单，既可以用来做分类也可以用来做回归。</li><li>可用于非线性分类。</li><li>训练时间复杂度为O(n)。</li><li>对数据没有假设，准确度高，对离群点(outlier)不敏感。</li></ul><p>缺点：</p><ul><li>计算量大。</li><li>样本不平衡问题（例如类别A的样本数量很多，而类别B的样本数量则很少）。</li><li>需要大量的内存。</li></ul></blockquote><h2 id="4-5-决策树算法"><a href="#4-5-决策树算法" class="headerlink" title="4.5 决策树算法"></a>4.5 决策树算法</h2><p>决策树易于解释，可以轻松地处理特征间的交互关系，但是不支持在线学习，当有新样本时，决策树需要全部重建。而且决策树容易过拟合，但是随机森林算法通过集成多个决策树可以解决过拟合的问题。</p><blockquote><p>优点：</p><ul><li>计算简单，易于理解，可解释性强。</li><li>比较适合处理有缺失属性的样本。</li><li>能够处理不相关的特征。</li><li>在相对短的时间内能够对大型数据源做出可行且效果良好的结果。</li></ul><p>缺点：</p><ul><li>容易发生过拟合（随机森林可以很大程度上减少过拟合）。</li><li>忽略数据之间的相关性。</li><li>样本不平衡问题 (决策树的信息增益偏向于具有更多数值的特征)。</li></ul></blockquote><h2 id="4-6-Adaboosting算法"><a href="#4-6-Adaboosting算法" class="headerlink" title="4.6 Adaboosting算法"></a>4.6 Adaboosting算法</h2><p>Adaboost是一种加和模型，每个模型都是基于上一次模型的错误率来建立的，过分关注分错的样本，而对正确分类的样本减少关注度，逐次迭代之后，可以得到一个相对较好的模型。</p><blockquote><p>优点：</p><ul><li>Adaboost是一种有很高精度的分类器。</li><li>可以使用各种方法构建子分类器，Adaboost算法提供的是框架。</li><li>当使用简单分类器时，计算出的结果是可以理解的，并且弱分类器的构造极其简单。</li><li>不用做特征筛选，不容易过拟合。</li></ul><p>缺点：</p><ul><li>对离群点(outlier)较为敏感。</li></ul></blockquote><h2 id="4-7-支持向量机算法"><a href="#4-7-支持向量机算法" class="headerlink" title="4.7 支持向量机算法"></a>4.7 支持向量机算法</h2><p>支持向量机（Support Vector Machine，SVM）是一种监督式学习的方法，可广泛地应用于统计分类以及回归分析。支持向量机将向量映射到一个更高维的空间里，在这个空间里建立有一个最大间隔超平面。在分开数据的超平面的两边建有两个互相平行的超平面，分隔超平面使两个平行超平面的距离最大化。假定平行超平面间的距离或差距越大，分类器的总误差越小。支持向量机准确率高，为避免过拟合提供了很好的理论保证，而且即使数据在原特征空间线性不可分，也可以通过核函数将原特征空间映射到更高维线性可分的空间，但是内存消耗大，难以解释，运行和调参比较麻烦。</p><blockquote><p>优点：</p><ul><li>可以解决高维特征空间问题。</li><li>能够处理非线性特征。</li><li>无需依赖整个数据。</li><li>泛化能力强。</li></ul><p>缺点：</p><ul><li>数据样本很多时，效率不高。</li><li>对非线性问题没有通用解决方案( 很难找到恰当的核函数 )。</li><li>对缺失数据敏感。</li></ul></blockquote><h2 id="4-8-神经网络"><a href="#4-8-神经网络" class="headerlink" title="4.8 神经网络"></a>4.8 神经网络</h2><p>神经网络（Neural Network）是一种运算模型，由大量的节点（或称神经元）相互联接构成。每个节点代表一种特定的输出函数，称为激励函数（activation function）。每两个节点间的连接都代表一个对于通过该连接信号的加权值，称之为权重。网络的输出则依赖于网络的连接方式，权重值和激励函数的不同而不同。而网络自身通常都是对自然界某种算法或者函数的逼近，也可能是对一种逻辑策略的表达。</p><blockquote><p>优点：</p><ul><li>分类的准确度高。</li><li>并行分布处理能力强，分布存储及学习能力强。</li><li>对噪声神经有较强的鲁棒性和容错能力，能充分逼近复杂的非线性关系。</li><li>具备联想记忆的功能。</li></ul><p>缺点：</p><ul><li>神经网络需要大量的参数，如网络拓扑结构、权值和阈值的初始值。</li><li>不能观察之间的学习过程，输出结果难以解释，会影响到结果的可信度和可接受程度。</li><li>学习时间过长。</li></ul></blockquote><h2 id="4-9-K-Means聚类算法"><a href="#4-9-K-Means聚类算法" class="headerlink" title="4.9 K-Means聚类算法"></a>4.9 K-Means聚类算法</h2><p>K均值聚类( K-Means Clustering )算法是一种基于样本间相似性度量的间接聚类方法，属于非监督学习方法。该算法以K为参数，把N个对象分为K个簇，以使簇内具有较高的相似度，而且簇间的相似度较低。相似度的计算根据一个簇中对象的平均值（簇的重心）来进行。首先随机选择K个对象，每个对象代表一个聚类的质心。对于其余的每一个对象，根据该对象与各聚类质心之间的距离，把它分配到与之最相似的聚类中。然后，计算每个聚类的新质心。重复上述过程，直到准则函数（例如误差的平方和）收敛。</p><blockquote><p>优点：</p><ul><li>算法简单，容易实现。</li><li>对处理大数据集，该算法是相对可伸缩的和高效率的，因为它的复杂度大约是O(NKT)，其中N是所有样本的数量，K是簇的类别数，T是迭代次数。</li><li>当簇是密集的、球状或团状的，且簇与簇之间区别明显时，聚类效果较好。</li></ul><p>缺点：</p><ul><li>对数据类型要求较高，适合数值型数据。</li><li>可能收敛到局部最小值，在大规模数据上收敛较慢。</li><li>难以选取合适的K值。</li><li>对初值的簇心值敏感，对于不同的初始值，可能会导致不同的聚类结果。</li><li>对于孤立点数据敏感，少量的该类数据能够对平均值产生极大影响。</li></ul></blockquote>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> meachine learning </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>你的模型运行快吗？</title>
      <link href="/2018/12/05/model-computation/"/>
      <url>/2018/12/05/model-computation/</url>
      
        <content type="html"><![CDATA[<blockquote><p>模型占用的存储空间、模型运行时消耗的内存空间、模型运行的速度</p></blockquote><h1 id="1-CNN不同层的计算量"><a href="#1-CNN不同层的计算量" class="headerlink" title="1. CNN不同层的计算量"></a>1. CNN不同层的计算量</h1><p>了解模型计算量的一种简单方法就是计算这个模型总共做了多少次浮点运算。除了计算量，内存带宽也是影响计算效率的重要因素。  </p><h2 id="1-1-乘积相加"><a href="#1-1-乘积相加" class="headerlink" title="1.1 乘积相加"></a>1.1 乘积相加</h2><p>神经网络中的绝大多数操作都是浮点数的乘法进行求和。例如：</p><pre><code class="lang-bash">y = w[0]*x[0] + w[1]*x[1] + w[2]*x[2] + ... + w[n-1]*x[n-1]</code></pre><p>上式中，输入 <code>w</code> 和 <code>x</code> 是两个向量，输出 <code>y</code> 是一个标量 ( 实数 )。在神经网络的卷积层和全连接层中， <code>w</code> 是层学习到的参数，<code>x</code> 就是层的输入，而 <code>y</code> 则是层的输出的一部分，因为典型的层结构有多个输出。其中 <code>w[0]*x[0] + ...</code> 称为一次 <a href="https://en.wikipedia.org/wiki/Multiply%E2%80%93accumulate_operation" target="_blank" rel="noopener">乘积相加操作(multiply-accumulate operations)</a>, 因此两个 <code>n</code> 维向量的点积含有 <code>n</code> 个 MACCs(乘积相加) 计算单元。</p><blockquote><p>Technically speaking there are only n - 1 additions in the above formula, one less than the number of multiplications. Think of the number of MACCs as being an approximation, just like Big-O notation is an approximation of the complexity of an algorithm.</p></blockquote><p>从每秒浮点运算 ( floating point operations per second, FLOPS ) 的角度来看，一次点积操作包含 <code>2n-1</code> 个 FLOPS，因为其中含有 <code>n</code> 次乘法运算和 <code>n-1</code> 次加法运算。</p><h2 id="1-2-全连接层"><a href="#1-2-全连接层" class="headerlink" title="1.2 全连接层"></a>1.2 全连接层</h2><p>在全连接层，所有的输入单元和输出单元相互连接。对于含有 <code>I</code> 个输入值和 <code>J</code> 个输出值的的层，权重 <code>W</code> 存储在 <code>I×J</code> 的矩阵中，因此全连接层的计算可以写作：</p><pre><code>y = matmul(x, W) + b</code></pre><p>矩阵乘法是有一系列点乘组合而成的。每一个点乘由输入 <code>x</code> 和矩阵<code>W</code> 的每一列运算得到。因此运算 <code>matmul(x, W)</code> 包含 <code>I×J</code> 个 MACCs 单元，和权重矩阵的元素个数相同。例如卷积层的最后输出为 <code>(512, 7, 7)</code>, 那么经过 <code>faltten</code> 操作后，输入 <code>I=512x7x7</code>。</p><h2 id="1-3-激活函数"><a href="#1-3-激活函数" class="headerlink" title="1.3 激活函数"></a>1.3 激活函数</h2><p>通常层的后面会跟随非线性激活函数，例如 ReLU 或者 sigmoid 函数。由于激活函数没有乘法运算，因此使用 FLOPS 来衡量计算时间。<br>不同激活函数的计算量是不同的。ReLU 激活函数的表达式为：</p><pre><code>y = max(x, 0)</code></pre><p>在 GPU 上只有一次操作。假设全连接层有 <code>J</code> 个输出单元，ReLU函数进行了 <code>J</code> 次最大值操作，因此含有 <code>J</code> 个 FLOPS 单元。<br>Sigmoid 激活函数表达式为：</p><pre><code>y = 1 / (1 + exp(-x))</code></pre><p>由于 Sigmoid 函数包含幂运算，因此计算量比较复杂。通常将加法运算、减法运算、乘法运算、除法运算、幂运算和平方根运算称为一次 FLOPS。Sigmoid 函数中包含四种不同的运算操作，因此含有 4 次 FLOPS。假设全连接层有 <code>J</code> 个输出单元，那么Sigmoid 函数含有 <code>4xJ</code> 个 FLOPS 单元。通常激活函数只占模型总运算量的很小一部分。</p><h2 id="1-3-卷积层"><a href="#1-3-卷积层" class="headerlink" title="1.3 卷积层"></a>1.3 卷积层</h2><p>卷积层的输入和输出不是向量，而是三维 ( <code>Height*Width*Channels</code> ) 的特征图。假定正方形的卷积核边长为 <code>k</code>，那么卷积层不考虑偏置和激活函数的 MACCs 为：</p><pre><code>k × k × Channels_in × Height_out × Width_out × Channels_out</code></pre><p>这里使用输出的 <code>Height</code> 和 <code>Width</code> 是因为考虑到卷积时的<code>stride, dilation factors, padding, etc</code>。<br>对于卷积核为为 <code>(3, 3, 128)</code> 且输入为 <code>(112, 112, 64)</code> 的卷积计算， 它的 MACCs 为：</p><pre><code>3 × 3 × 64 × 112 × 112 × 128 = 924844032</code></pre><blockquote><p>In this example, we used “same” padding and stride = 1, so that the output feature map has the same size as the input feature map. It’s also common to see convolutional layers use stride = 2, which would have chopped the output feature map size in half, and we would’ve used 56 × 56 instead of 112 × 112 in the above calculation.</p></blockquote><h2 id="1-4-深度可分离卷积"><a href="#1-4-深度可分离卷积" class="headerlink" title="1.4 深度可分离卷积"></a>1.4 深度可分离卷积</h2><p>深度可分离卷积 ( depthwise-separable convolution ) 首先在 <a href="https://arxiv.org/abs/1610.02357" target="_blank" rel="noopener">Xception</a> 中被使用，它将常规的卷积操作分解为 depthwise 卷积与 pointwise 卷积两个部分。该结构和常规提取特征的卷积操作类似，但是参数量和运算成本较低，在轻量级网络 ( <a href="https://arxiv.org/abs/1704.04861" target="_blank" rel="noopener">MobileNet</a> )中十分常见。  </p><p>假设输入层为 <code>(112, 112, 64)</code>，经过 <code>(3, 3, 128)</code>的卷积核，假定使用 <code>same padding</code> 并且 <code>stride=1</code>，使得输入输出特征图大小相同，那么最终得到 <code>(112, 112, 128)</code> 的特征图。常规卷积示意图如下所示。 MACCs 次数为：</p><pre><code>3 × 3 × 64 × 112 × 112 × 128 = 924844032</code></pre><div align="center"><img src="/2018/12/05/model-computation/convolution.png" alt="卷积示意图" width="400" height="300"></div><p>Depthwise Convolution 的每个卷积核只与输入的一个通道进行卷积，卷积核的数量与上一层的通道数相同。因此输入图像经过 depthwise 卷积之后生成 3 个单通道的特征图，如下图所示。</p><pre><code>MACCs = K × K × Channels_in × Height_out × Width_out      = 3 × 3 × 64 × 112 × 112       = 7225344</code></pre><div align="center"><img src="/2018/12/05/model-computation/depthwise_convolution.png" alt="卷积示意图" width="400" height="300"></div><p>Depthwise Convolution 完成后的特征图数量与输入层的通道数相同，无法扩展特征图的数量，而且无法有效利用不同通道在相同空间位置上的特征信息。因此在 depthwise 卷积之后需要 pointwise Convolution 将特征图进行组合生成新的特征图。Pointwise Convolution 的卷积核大小为 <code>1x1</code>。</p><pre><code>MACCs = 1 × 1 × Channels_in × Height_out × Width_out x Channels_out      = 1 × 1 × 64 × 112 × 112 x 128       = 102760448</code></pre><div align="center"><img src="/2018/12/05/model-computation/pointwise_convolution.png" alt="卷积示意图" width="400" height="300"></div><p>因此 depthwise-separable convolution 的 MACCs 为：</p><pre><code>MACCs = (K × K × Channels_in × Height_out × Width_out) + (Channels_in × Height_out × Width_out × Channels_out)       = Channels_in × Height_out × Width_out × (K × K + Channel_out)</code></pre><blockquote><p>The exact factor is <code>K × K × Cout / (K × K + Cout)</code> . It should be pointed out that depthwise convolutions sometimes have a <code>stride &gt; 1</code>, which reduces the dimensions of their output feature map. But a pointwise layer usually has <code>stride = 1</code>, and so its output feature map will always have the same dimensions as the depthwise layer’s.</p></blockquote><h2 id="1-5-批量归一化层"><a href="#1-5-批量归一化层" class="headerlink" title="1.5 批量归一化层"></a>1.5 批量归一化层</h2><p>批量归一化层 ( Batch normalization Layer) 每一个输出的函数表达式可以写为：</p><pre><code>z = gamma * (y - mean) / sqrt(variance + epsilon) + beta</code></pre><p>其中 <code>y</code> 是上一层输出特征图的一个元素，mean 为均值，variance 为方差，epsilon 确保分母不为0，gamma为尺度因子，beta 为偏置。每一个通道都有其对应的值，因此对于通道为 <code>c</code> 的卷积输出层，batch normalization layer 学习的参数量为 <code>4c</code>。</p><pre><code>z = gamma * ((x[0]*w[0] + x[1]*w[1] + ... + x[n-1]*w[n-1] + b) - mean) / sqrt(variance + epsilon) + beta</code></pre><p>由于在预测过程中移除了 batch normlization layer，因此考虑模型的计算量时可以不用关注正则化层的影响。</p><blockquote><p> This trick only works when the order of the layers is: convolution, batch norm, ReLU — but not when it is: convolution, ReLU, batch norm. The ReLU is a non-linear operation, which messes up the math.</p></blockquote><h2 id="1-6-池化层"><a href="#1-6-池化层" class="headerlink" title="1.6 池化层"></a>1.6 池化层</h2><p>对于 <code>112, 112, 128)</code> 的特征图，如果最大池化的 <code>pooling size = 2</code> 并且 <code>stride = 2</code>，那么 FLOPS 操作数为 <code>112 × 112 × 128 = 1605632</code>。可以看到，池化层的操作数远远少于卷积层的操作数，因此池化层也是网络计算复杂度的舍入误差。</p><h1 id="2-模型耗费的内存"><a href="#2-模型耗费的内存" class="headerlink" title="2. 模型耗费的内存"></a>2. 模型耗费的内存</h1><p>在模型的每一层计算中，硬件设备需要从主存储中读取输入向量或者特征图的值，从主存储中读取权重参数并与输入计算点积，将得到的新向量或者特征图作为结果写入主存储中。这些操作都涉及到大量的内存读写，耗费的时间可能远远大于计算的次数。</p><h2 id="2-1-权重的内存"><a href="#2-1-权重的内存" class="headerlink" title="2.1 权重的内存"></a>2.1 权重的内存</h2><p>层将权重保存在主存储中，这意味着权重参数越少，模型运行速度越快。如前文所述，输入为 <code>I</code> 个神经元和输出为 <code>J</code> 个神经元之间的权重参数为 <code>I x J</code>，加上偏置向量，总的参数为 <code>( I + 1) x J</code>。对于大小为 <code>k</code>，输入通道数为 <code>Channels_in</code>，输出通道数为 <code>Channels_out</code> 的卷积层的参数为 <code>k x k Channels_in x Channels_out</code> 加上偏置向量参数 <code>Channels_out</code>。<br>对于输入 <code>4096</code> 输出为 <code>4096</code>的全连接层，其权重参数量为 <code>(4096 + 1) x 4096 = 16781312</code>。对于输入为 <code>(64, 64, 32)</code> 卷积核为 <code>(3, 3, 48)</code> 的卷积层，其权重参数量为 <code>(3 x 3 x 32 x 48 + 48 = 13872)</code>。可以看到，相比于卷积层，全连接层的参数量相对更多。</p><blockquote><p>Fully-connected and convolutional layers are actually very similar. A convolutional layer is basically a fully-connected layer with the vast majority of the connections set to 0 — each output is only connected to K × K inputs rather than all of them, and all the outputs use the same values for these connections. This is why convolutional layers are so much more efficient about memory, since they don’t store the weights for connections that are not used.</p></blockquote>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
    </entry>
    
    <entry>
      <title>Centos7使用Docker快速搭建深度学习环境</title>
      <link href="/2018/12/04/docker-env/"/>
      <url>/2018/12/04/docker-env/</url>
      
        <content type="html"><![CDATA[<h1 id="1-什么是-Docker"><a href="#1-什么是-Docker" class="headerlink" title="1. 什么是 Docker"></a>1. 什么是 Docker</h1><blockquote><p>解决 “在我的机器上可以正常工作，为什么在你的机器上不能工作” 的问题。  </p></blockquote><p>随着深度学习技术的飞速发展，各种深度学习框架都有大量的粉丝。如何在一台电脑上安装多个深度学习框架？同一深度学习框架的不同版本依赖于不同的GPU版本，但是一台服务器只可能安装唯一版本的GPU。当多名开发人员在统一服务器上使用不同的深度学习框架进行开发时，往往会产生环境冲突。最好的解决方案就是采用虚拟技术。  </p><p><a href="https://www.docker.com/" target="_blank" rel="noopener">Docker</a> 是世界领先的软件容器平台，也是目前最流行的 Linux 容器解决方案。Docker 将应用程序与该程序的依赖，打包在一个文件里面。运行这个文件，就会生成一个虚拟容器。程序在这个虚拟容器里运行，就好像在真实的物理机上运行一样。有了 Docker，就不用担心环境问题。而且 Docker 的接口相当简单，用户可以方便地创建和使用容器，把自己的应用放入容器。容器还可以进行版本管理、复制、分享、修改，就像管理普通的代码一样。和虚拟机相比，由于没有臃肿的从操作系统，Docker可以节省大量的磁盘空间以及其他系统资源。  </p><blockquote><p>虚拟机通常用于彻底隔离整个运行环境。例如，云服务提供商通常采用虚拟机技术隔离不同的用户。而Docker通常用于隔离不同的应用，例如前端，后端以及数据库。</p></blockquote><h1 id="2-安装-Docker"><a href="#2-安装-Docker" class="headerlink" title="2. 安装 Docker"></a>2. 安装 <a href="https://docs.docker.com/install/linux/docker-ce/centos/" target="_blank" rel="noopener">Docker</a></h1><p>(a) 安装依赖包</p><pre><code class="lang-bash">$ yum install -y yum-utils device-mapper-persistent-data</code></pre><p>(b) 配置稳定仓库</p><pre><code class="lang-bash">$ yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo</code></pre><p>(c) 安装(默认为最新版)</p><pre><code class="lang-bash">$ yum install docker-ce</code></pre><p>(4) 修改docker运行时的根目录, 解决存储不足的问题</p><pre><code class="lang-bash">$ vim /lib/systemd/system/docker.service # 在 ExecStart=/usr/bin/dockerd 后添加 --graph=/home/docker$ ExecStart=/usr/bin/dockerd --graph=/home/docker</code></pre><p>(5) 重新启动docker服务</p><pre><code class="lang-bash">$ systemctl daemon-reload$ systemctl status docker$ systemctl start docker</code></pre><p>(7) 测试Docker是否正确安装</p><pre><code class="lang-bash">$  docker version$  docker run hello-world</code></pre><h1 id="3-安装-nvidia-docker"><a href="#3-安装-nvidia-docker" class="headerlink" title="3. 安装 nvidia-docker"></a>3. 安装 <a href="https://github.com/NVIDIA/nvidia-docker" target="_blank" rel="noopener">nvidia-docker</a></h1><p>深度学习框架需要使用 GPU 加入计算，如果不安装 nvidia-docker 工具，那么在容器中将会无法调用宿主机上的 GPU 硬件设备。  </p><p>(a) 移除旧版本 nvidia-GPU 和已经存在的 GPU 容器</p><pre><code class="lang-bash">$ docker volume ls -q -f driver=nvidia-docker | xargs -r -I{} -n1 docker ps -q -a -f volume={} | xargs -r docker rm -f$ sudo yum remove nvidia-docker</code></pre><p>(b) 安装依赖包</p><pre><code class="lang-bash">$ distribution=$(. /etc/os-release;echo $ID$VERSION_ID)$ curl -s -L https://nvidia.github.io/nvidia-docker/$distribution/nvidia-docker.repo | \ sudo tee /etc/yum.repos.d/nvidia-docker.repo</code></pre><p>(c) 安装nvidia-docker</p><pre><code class="lang-bash">$ sudo yum install -y nvidia-docker2$ sudo pkill -SIGHUP dockerd</code></pre><p>(d) 测试 nvidia-docker 是否安装成功</p><pre><code class="lang-bash">docker run --runtime=nvidia --rm nvidia/cuda:9.0-base nvidia-smi</code></pre><h1 id="4-CUDA9-CUDNN7-python3-6-源码安装-Caffe、Caffe2、Tensorflow、-Detectron-和-Darknet"><a href="#4-CUDA9-CUDNN7-python3-6-源码安装-Caffe、Caffe2、Tensorflow、-Detectron-和-Darknet" class="headerlink" title="4. CUDA9-CUDNN7-python3.6 源码安装 Caffe、Caffe2、Tensorflow、 Detectron 和 Darknet"></a>4. CUDA9-CUDNN7-python3.6 源码安装 Caffe、Caffe2、Tensorflow、 Detectron 和 Darknet</h1><p>(a) 启动 GPU container 并登录</p><pre><code class="lang-bash">$ nvidia-docker run -tid --name TestMyGPU --net=&#39;host&#39; nvidia/cuda:9.0-cudnn7-devel-centos7 /bin/bash # 启动容器$ docker exec -it TestMyGPU /bin/bash # 登录容器</code></pre><p>(b) 配置变量</p><pre><code class="lang-bash">$ export http_proxy=http://xx.xx.xx.xx:8080 # 设置网络$ export https_proxy=https://xx.xx.xx.xx:8080$ export PYINSTALL=/usr/local/python3 # 设置python3.6安装路径$ export PATH=$PYINSTALL/bin:$PATH</code></pre><p>(c) 安装 python3.6 和 Caffe，参照上一篇博客 <code>Centos7 安装 Caffe</code>。</p><p>(d) 在 <code>/home</code> 下安装 Caffe2、Tensorflow、 Detectron 和 Darknet</p><pre><code class="lang-bash"># 安装依赖包和 opencv$ yum install -y cmake3 &amp;&amp; pip3 install cython opencv-python==3.4.2.16# 安装 nccl，GPU分布式通信函数库$ cd /home &amp;&amp; git clone https://github.com/NVIDIA/nccl.git $ cd nccl &amp;&amp; make -j8 src.build CUDA_HOME=&#39;/usr/local/cuda-9.0/&#39; NVCC_GENCODE=&quot;-gencode=arch=compute_70,code=sm_70&quot; $ yum install -y rpm-build rpmdevtools &amp;&amp; make -j8 pkg.redhat.build &amp;&amp; make install $ echo &#39;export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/lib&#39; &gt;&gt; /root/.bashrc $ cd /home &amp;&amp; rm -rf nccl &amp;&amp; source  ~/.bashrc # 验证nccl是否安装成功# cd /home &amp;&amp; git clone https://github.com/NVIDIA/nccl-tests.git \# cd nccl-tests &amp;&amp; make -j8 &amp;&amp; ./build/all_reduce_perf -b 8 -e 256M -f 2 -g 1 \# cd /home &amp;&amp; rm -rf nccl-tests# 安装 darknet$ cd /home &amp;&amp; git clone https://github.com/pjreddie/darknet.git &amp;&amp; cd darknet \# 修改 Makefile 文件，令 GPU=1，CUDNN=1，OPENCV=1$ sed -i &#39;s/GPU=0/GPU=1/&#39; Makefile $ sed -i &#39;s/CUDNN=0/CUDNN=1/&#39; Makefile $ sed -i &#39;s/OPENCV=0/OPENCV=1/&#39; Makefile $ make -j32# 验证 darknet 是否安装成功# 执行 ./darknet 输出 usage: ./darknet &lt;function&gt;# 安装 Tensorflow 和 Keras$ cd /home &amp;&amp; pip3 install tensorflow-gpu==1.10 keras==2.2.0# 安装caffe2$ pip3 install pyyaml future hypothesis pydot $ cd /home &amp;&amp; git clone https://github.com/pytorch/pytorch.git $ cd pytorch &amp;&amp; git submodule update --init --recursive # 解决编译时 cmake 的版本问题：# 将文件 `pytorch/tools/build_pytorch_libs.sh` 复制到 `/home` 路径下# 修改./tools/build_pytorch_libs.sh 第31和32行 CMAKE_VERSION、CMAKE3_VERSION# CMAKE_VERSION=$(cmake --version | grep &#39;version&#39; | awk &#39;{print $3}&#39; | awk -F. &#39;{print $1&quot;.&quot;$2&quot;.&quot;$3}&#39;)# CMAKE3_VERSION=$(cmake3 --version | grep &#39;version&#39; | awk &#39;{print $3}&#39; | awk -F. &#39;{print $1&quot;.&quot;$2&quot;.&quot;$3}&#39;)$ rm -f /home/pytorch/tools/build_pytorch_libs.sh$ cp -f /home/build_pytorch_libs.sh /home/pytorch/tools/build_pytorch_libs.sh$ rm -f /home/build_pytorch_libs.sh $ python setup.py install $ cd /home &amp;&amp; rm -rf pytorch</code></pre><p>验证 caffe2 是否安装成功， python 命令窗口执行</p><pre><code class="lang-python">&gt;&gt;&gt; import torch&gt;&gt;&gt; import caffe2&gt;&gt;&gt; exit()</code></pre><p>验证是否能使用 GPU</p><pre><code class="lang-bash">$ cd /home &amp;&amp; python -c &#39;from caffe2.python import core&#39; 2&gt;/dev/null &amp;&amp; echo &quot;Success&quot; || echo &quot;Failure&quot;$ python -c &#39;from caffe2.python import workspace; print(workspace.NumCudaDevices())&#39;$ python /usr/local/anaconda3/lib/python3.6/site-packages/caffe2/python/operator_test/rnn_cell_test.py</code></pre><p>安装 COCO-API</p><pre><code class="lang-bash">$ cd /home &amp;&amp; git clone  https://github.com/pdollar/coco $ pip3 install setuptools==18.4 &amp;&amp; yum install -y tkinter$ cd coco/PythonAPI &amp;&amp; make -j8 &amp;&amp; make install &amp;&amp; python setup.py install $ cd /home &amp;&amp; rm -rf coco</code></pre><p>验证 coco-api 是否安装成功, python命令窗口执行</p><pre><code class="lang-Python">from pycocotools.coco import COCO</code></pre><p>安装detectron</p><pre><code class="lang-bash">$ cd /home &amp;&amp; git clone https://github.com/facebookresearch/detectron $ cd detectron &amp;&amp; make -j8 # 验证 detectron 是否正确安装$ cd /home/detectron &amp;&amp; python detectron/tests/test_spatial_narrow_as_op.py$ python tools/infer_simple.py \    --cfg configs/12_2017_baselines/e2e_mask_rcnn_R-101-FPN_2x.yaml \    --output-dir tmp/detectron-visualizations \    --image-ext jpg \    --wts https://s3-us-west-2.amazonaws.com/detectron/35861858/12_2017_baselines/e2e_mask_rcnn_R-101-FPN_2x.yaml.02_32_51.SgT4y1cO/output/train/coco_2014_train:coco_2014_valminusminival/generalized_rcnn/model_final.pkl \    demo</code></pre><h1 id="5-使用-Anaconda3-安装-Caffe、Pytorch-和-Tensorflow"><a href="#5-使用-Anaconda3-安装-Caffe、Pytorch-和-Tensorflow" class="headerlink" title="5. 使用 Anaconda3 安装 Caffe、Pytorch 和 Tensorflow"></a>5. 使用 Anaconda3 安装 Caffe、Pytorch 和 Tensorflow</h1><p><a href="https://www.anaconda.com/" target="_blank" rel="noopener">Anaconda</a> 是一个开源的Python发行版本，包含了conda、Python等180多个科学包及其依赖项，是当前最流行的 Python 数据科学开发平台。 因为包含了大量的科学包，Anaconda 的下载文件比较大（约 531 MB），如果只需要某些包，或者需要节省带宽或存储空间，也可以使用 <a href="https://conda.io/miniconda.html" target="_blank" rel="noopener">Miniconda</a> 发行版 ( 仅包含 conda 和 Python )。</p><div align="center"><img src="/2018/12/04/docker-env/anaconda.png" alt="anaconda wesite image" width="500" height="300"></div><p>Anaconda 当前集成了 caffe 和 pytorch，可以利用 Anaconda 快速安装 caffe 和 caffe2 ( 集成在 pytorch ) 中。由于 Docker 容器需要应用程序占用内存尽可能小，因此采用 Miniconda 代替 Anaconda。完成的 Dockerfile 文件如下，在宿主机中含有 <code>Dockfile</code> 文件的当前路径下运行 <code>docker build -t deep_learning_environment:v0.1 .</code> 即可生成满足深度学习环境对应要求的镜像。</p><pre><code class="lang-bash">FROM nvidia/cuda:9.0-cudnn7-runtime-centos7ENV LANG=en_US.UTF-8ARG http_proxy=http://xx.xx.xx.xx:8080ARG https_proxy=https://xx.xx.xx.xx:8080# 安装Miniconda3RUN cd /home \    # 安装依赖包    &amp;&amp; yum install -y epel-release-7-11.noarch wget git make bzip2 &amp;&amp; pip install cython \    # 安装nccl    &amp;&amp; cd /home &amp;&amp; git clone https://github.com/NVIDIA/nccl.git \    &amp;&amp; cd nccl &amp;&amp; make -j8 src.build CUDA_HOME=&#39;/usr/local/cuda-9.0/&#39; NVCC_GENCODE=&quot;-gencode=arch=compute_70,code=sm_70&quot; \    &amp;&amp; yum install -y rpm-build rpmdevtools &amp;&amp; make -j8 pkg.redhat.build &amp;&amp; make install \    &amp;&amp; echo &#39;export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/nccl/build/lib&#39; &gt;&gt; /root/.bashrc \    &amp;&amp; cd /home &amp;&amp; rm -rf nccl &amp;&amp; source  ~/.bashrc \    # 验证nccl是否安装成功    # cd /home &amp;&amp; git clone https://github.com/NVIDIA/nccl-tests.git \    # cd nccl-tests &amp;&amp; make -j8 &amp;&amp; ./build/all_reduce_perf -b 8 -e 256M -f 2 -g 1 \    # cd /home &amp;&amp; rm -rf nccl-tests    &amp;&amp; wget https://repo.anaconda.com/miniconda/Miniconda3-4.3.30-Linux-x86_64.sh \    &amp;&amp; bash Miniconda3-4.3.30-Linux-x86_64.sh -p /usr/local/miniconda3 -b \    # 将 miniconda 添加到系统路径    &amp;&amp; echo &#39;export PATH=/usr/local/miniconda3/bin:$PATH&#39; &gt;&gt; /root/.bashrc \    &amp;&amp; echo &#39;export LD_LIBRARY_PATH=/usr/local/miniconda3/lib:$LD_LIBRARY_PATH&#39; &gt;&gt; /root/.bashrc \    &amp;&amp; source  ~/.bashrc &amp;&amp; rm -rf Miniconda3-4.3.30-Linux-x86_64.sh \    # 修改yum的链接问题    &amp;&amp; ln -s -f /usr/lib64/liblzma.so.5 /usr/local/miniconda3/lib/liblzma.so.5 \    # conda 安装 caffe-gpu    &amp;&amp; conda install -y caffe-gpu protobuf \    # conda 安装 caffe2    # 直接安装下载速度非常慢，而且有可能失败    &amp;&amp; conda install -y pytorch-nightly -c pytorch \    &amp;&amp; pip install future hypothesis pydot \    # 验证 caffe 和 caffe2 是否安装成功    # $ python &amp;&amp; import torch &amp;&amp; import caffe &amp;&amp; import caffe2    # python -c &#39;from caffe2.python import core&#39; 2&gt;/dev/null &amp;&amp; echo &quot;Success&quot; || echo &quot;Failure&quot;    # python -c &#39;from caffe2.python import workspace; print(workspace.NumCudaDevices())&#39;    # python /usr/local/anaconda3/lib/python3.6/site-packages/caffe2/python/operator_test/rnn_cell_test.py    # 安装 detectron    &amp;&amp; cd /home &amp;&amp; git clone https://github.com/facebookresearch/detectron \    &amp;&amp; cd detectron &amp;&amp; pip install cython &amp;&amp; make -j8 \    # 验证 detectron 是否安装正确    # cd /home/detectron &amp;&amp; python detectron/tests/test_spatial_narrow_as_op.py    # 安装darknet, 从 github 上下载darknet源码, 修改 Makefile 文件，令 GPU=1，CUDNN=1，OPENCV=1。    &amp;&amp; cd /home &amp;&amp; git clone https://github.com/pjreddie/darknet.git &amp;&amp; cd darknet \    &amp;&amp; sed -i &#39;s/GPU=0/GPU=1/&#39; Makefile \    &amp;&amp; sed -i &#39;s/CUDNN=0/CUDNN=1/&#39; Makefile \    &amp;&amp; sed -i &#39;s/OPENCV=0/OPENCV=1/&#39; Makefile \    &amp;&amp; make -j8 \    # 验证 darknet 是否安装成功    # 执行 ./darknet 输出 usage: ./darknet &lt;function&gt;    # 安装tensorflow和keras    &amp;&amp; cd /home &amp;&amp; pip install tensorflow-gpu==1.10 keras==2.2.0 -i https://pypi.douban.com/simple/ \    # 验证 tensorflow 和 keras 是否安装成功    # $ python &amp;&amp; import tensorflow &amp;&amp; import keras    # 删除 minicond3/pkgs 里面的安装包, 降低内存占用    &amp;&amp; cd /usr/local/miniconda3/ &amp;&amp; rm -rf pkgs</code></pre>]]></content>
      
      
      <categories>
          
          <category> 安装教程 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> centoe7 深度学习环境 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Centos7 安装 Caffe</title>
      <link href="/2018/12/03/centos-caffe/"/>
      <url>/2018/12/03/centos-caffe/</url>
      
        <content type="html"><![CDATA[<h1 id="1-Caffe-是什么"><a href="#1-Caffe-是什么" class="headerlink" title="1. Caffe 是什么"></a>1. Caffe 是什么</h1><p><a href="http://caffe.berkeleyvision.org/" target="_blank" rel="noopener">Caffe</a> 全称 Convolutional Architecture for Fast Feature Embedding，是一种常用的深度学习框架，主要应用在视频、图像处理方面的应用上。得益于RCNN框架的影响力，当前主流的目标检测模型 ( 例如 <a href="https://github.com/ShaoqingRen/faster_rcnn" target="_blank" rel="noopener">Faster-RCNN</a> 和 <a href="https://github.com/weiliu89/caffe" target="_blank" rel="noopener">SSD</a> ) 的作者源码都是基于 Caffe 编写的。</p><h1 id="2-Centos7-安装-Caffe"><a href="#2-Centos7-安装-Caffe" class="headerlink" title="2. Centos7 安装 Caffe"></a>2. Centos7 安装 Caffe</h1><p>虽然网上已经有很多相关的安装教程，但是大多数都是基于 Ubantu 系统的，而且网上的教程在安装过程中往往会报出各种莫名其妙的 bug。经过笔者多次血泪实践，发现大多数错误都是因为未能正确安装 boost 和 protouf 工具包。假设 Centos7 已经正确安装 Nvidia GPU 驱动程序和 CUDA9+CUDNN7的加速包，按照如下教程即可正确编译 Caffe 的 Python3.6 接口。如果电脑没有安装 GPU 驱动，请先参照 Nvidia <a href="https://developer.nvidia.com/cuda-downloads" target="_blank" rel="noopener">官网安装教程</a> 正确安装 GPU。</p><pre><code class="lang-bash"># 安装依赖包$ yum clean all &amp;&amp; yum makecache &amp;&amp; yum install -y epel-release-7-11.noarch $ yum -y install zlib-devel openssl-devel bzip2-devel expat-devel$ yum -y install gdbm-devel readline-devel sqlite-devel$ yum -y install wget git make unzip libSM libXrender libXext# 安装 Python3.6$ cd /home &amp;&amp; wget https://www.python.org/ftp/python/3.6.6/Python-3.6.6.tgz$ tar -xvf Python-3.6.6.tgz &amp;&amp; cd Python-3.6.6$ ./configure --prefix=$PYINSTALL &amp;&amp; make -j32 &amp;&amp; make install$ ln -s $PYINSTALL/bin/python3 $PYINSTALL/bin/python$ cd /home &amp;&amp; rm -rf Python-3.6.6.tgz Python-3.6.6# 安装 scikit-image$ pip3 install numpy scikit-image -i http://pypi.douban.com/simple/ --trusted-host pypi.douban.com # 安装 caffe 依赖包$ yum -y install leveldb-devel snappy-devel opencv-devel hdf5-devel $ yum -y install gflags-devel glog-devel lmdb-devel openblas-devel python36-devel# 编译 boost 修复 libboost_python3.so 无法连接的错误$ cd /home &amp;&amp; wget https://dl.bintray.com/boostorg/release/1.67.0/source/boost_1_67_0.tar.gz $ tar -xvf boost_1_67_0.tar.gz $ cd boost_1_67_0 &amp;&amp; ./bootstrap.sh --with-toolset=gcc $ ./b2 cflags=&#39;-fPIC&#39; cxxflags=&#39;-fPIC&#39; include=/usr/include/python3.6m &amp;&amp; ./b2 install $ ln -s /usr/local/lib/libboost_python36.so /usr/lib64/libboost_python3.so $ echo /usr/local/lib &gt;&gt; /etc/ld.so.conf.d/caffe.conf &amp;&amp; ldconfig $ cd /home &amp;&amp; rm -rf boost_1_67_0.tar.gz boost_1_67_0  # 安装 protobuf$ echo &#39;export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/lib&#39; &gt;&gt; /root/.bashrc $ source  ~/.bashrc$ cd /home &amp;&amp; wget https://github.com/protocolbuffers/protobuf/releases/download/v3.5.1/protobuf-cpp-3.5.1.zip $ unzip protobuf-cpp-3.5.1.zip $ cd protobuf-3.5.1 &amp;&amp; ./configure &amp;&amp; make -j32 &amp;&amp; make install $ cd /home &amp;&amp; rm -rf protobuf-cpp-3.5.1.zip protobuf-3.5.1 $ wget https://github.com/protocolbuffers/protobuf/releases/download/v3.5.1/protobuf-python-3.5.1.zip $ unzip protobuf-python-3.5.1.zip $ cd protobuf-3.5.1/python &amp;&amp; python setup.py build &amp;&amp; python setup.py install $ cd /home &amp;&amp; rm -rf protobuf-python-3.5.1.zip protobuf-3.5.1</code></pre><p>安装 caffe， 需要修改配置文件 <code>Makefile.config</code></p><pre><code class="lang-bash"># 安装 caffe$ cd /home &amp;&amp; git clone https://github.com/bvlc/caffe.git# 将 caffe/Makefile.config.example 文件复制到 /home 路径下，命名为 Makefile.config 并进行修改# 第05行改为 USE_CUDNN := 1# 第11行改为 USE_OPENCV := 1# 第39行改为 CUDA_ARCH :=    -gencode arch=compute_30,code=sm_30 \# 第51行改为 BLAS := open# 第55行改为 BLAS_INCLUDE := /usr/include/openblas# 第56行改为 BLAS_LIB := /usr/lib64$ cp Makefile.config caffe/Makefile.config &amp;&amp; rm -f Makefile.config$ cd caffe &amp;&amp; make -j32 &amp;&amp; make pycaffe -j32$ cp -r python/caffe /usr/local/python3/lib/python3.6/site-packages$ cp .build_release/lib/* /usr/lib64</code></pre><p>在 <code>python</code> 命令窗口中执行 <code>import caffe</code> 查看 caffe 的 python 接口是否编译成功。</p><pre><code class="lang-python"># 验证 caffe 是否安装成功&gt;&gt;&gt; import caffe</code></pre>]]></content>
      
      
      <categories>
          
          <category> 安装教程 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> centos </tag>
            
            <tag> caffe </tag>
            
            <tag> python </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Hello World——利用hexo和github快速搭建个人博客</title>
      <link href="/2018/12/01/hello-world/"/>
      <url>/2018/12/01/hello-world/</url>
      
        <content type="html"><![CDATA[<h2 id="一、hexo和github简介"><a href="#一、hexo和github简介" class="headerlink" title="一、hexo和github简介"></a>一、hexo和github简介</h2><blockquote><p>Hexo 生成静态网页，Github 托管网页，Markdown 编辑博客。</p></blockquote><h3 id="1-hexo是什么？"><a href="#1-hexo是什么？" class="headerlink" title="1. hexo是什么？"></a>1. hexo是什么？</h3><p><a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> 是一款基于 Node.js 语言、快速、简洁且高效的博客框架。通过使用Markdown（或其他渲染引擎）解析文章，即使是前端小白也可利用 hexo 框架的靓丽主题快速生成相当专业的静态网页。</p><h3 id="2-github是什么？"><a href="#2-github是什么？" class="headerlink" title="2. github是什么？"></a>2. github是什么？</h3><p><a href="https://github.com/explore" target="_blank" rel="noopener">GitHub</a> 是一个面向开源及私有软件项目的托管平台，除了git 代码仓库托管及基本的 Web 管理界面以外，还提供了订阅、讨论组、文本渲染、在线文件编辑器、协作图谱（报表）、代码片段分享（Gist）等功能，是当前最活跃的“程序猿交友平台”。</p><h3 id="3-markdown是什么？"><a href="#3-markdown是什么？" class="headerlink" title="3. markdown是什么？"></a>3. markdown是什么？</h3><p><a href="http://www.markdown.cn/" target="_blank" rel="noopener">Markdown</a> 是一种可以使用普通文本编辑器编写的标记语言，通过简单的标记语法，它可以使普通文本内容具有一定的格式。Markdown的语法简洁易学，功能比纯文本更强大，世界上最流行的博客平台 WordPress 能很好的支持Markdown。</p><h2 id="二、搭建博客环境"><a href="#二、搭建博客环境" class="headerlink" title="二、搭建博客环境"></a>二、搭建博客环境</h2><h3 id="1-安装-Node-js"><a href="#1-安装-Node-js" class="headerlink" title="1. 安装 Node.js"></a>1. 安装 Node.js</h3><p>Hexo 博客框架基于 Node.js 语言，首先下载 <a href="https://nodejs.org/en/download/" target="_blank" rel="noopener">Node.js安装包</a>，选择对应的版本进行安装。默认安装过程会配置环境变量及 npm 包，安装完成后在命令窗口（例如 windows 系统的 cmd 窗口）输入 <code>node -v</code> 即可验证是否安装成功。</p><h3 id="2-安装-Git"><a href="#2-安装-Git" class="headerlink" title="2. 安装 Git"></a>2. 安装 Git</h3><p>Git 是开源的分布式版本控制系统，可以将本地编辑完成的博客同步到 Github 服务器上。首先下载 <a href="https://git-scm.com/downloads" target="_blank" rel="noopener">Git安装包</a>，安装完成后在命令窗口输入 <code>git -v</code> 即可验证是否安装成功。</p><h3 id="3-安装-Hexo"><a href="#3-安装-Hexo" class="headerlink" title="3. 安装 Hexo"></a>3. 安装 Hexo</h3><p>Hexo是个人博客网站的框架，安装步骤参考 <a href="https://hexo.io/zh-cn/docs/" target="_blank" rel="noopener">官网文档</a>。首先在本地建立名为blog的文件夹（文件夹名任意），然后在blog文件夹当前路径下开启命令窗口，通过 npm 命令即可完成安装。</p><pre><code class="lang-bash">$ npm install -g hexo-cli</code></pre><p>安装完成后，在命令窗口中初始化博客。</p><pre><code class="lang-bash">$ hexo init blog</code></pre><p>初始化完成后，分别下述命令检测博客环境是否正常。</p><pre><code class="lang-bash">$ hexo generate       # 生成博客$ hexo server         # 启动本地服务器</code></pre><p>hexo 3.0把服务器独立成个别模块，需要单独安装<code>npm i hexo-server</code>。如果没有报错，接下来就是见证奇迹的时刻了。在浏览器中输入网址 <a href="http://localhost:4000" target="_blank" rel="noopener">http://localhost:4000</a>，就可以看到诞生的第一篇博客。</p><h3 id="4-上传到-Github"><a href="#4-上传到-Github" class="headerlink" title="4. 上传到 Github"></a>4. 上传到 Github</h3><p>首先到 <a href="https://github.com/" target="_blank" rel="noopener">官网注册</a>，假定注册的用户名为 user_name，注册的邮箱为 user_email，然后创建一个仓库，设置该仓库的主页面，得到你的github主页面网址 <a href="http://user_name.github.io" target="_blank" rel="noopener">http://user_name.github.io</a>。其他用户在浏览器中输入该网址，就能看到你的主页面。最后编辑站点配置文件 <code>/blog/_config.yml</code>，在该文件的末尾加入：</p><pre><code>deploy:  type: git  repository: https://github.com/user_name/user_name.github.io  branch: master</code></pre><p>在命令窗口运行代码 <code>npm install hexo-deployer-git --save</code> 安装 git 命令部署插件后，执行如下代码：</p><pre><code class="lang-bash">$ git config --global user.name &quot;user_name&quot;     # 指定 git 上传的仓库$ git config --global user.email user_email$ hexo clean             # 清理缓存$ hexo generate          # 生成博客$ hexo deploy            # 同步到 github 主页面</code></pre><h3 id="5-绑定个人域名"><a href="#5-绑定个人域名" class="headerlink" title="5. 绑定个人域名"></a>5. 绑定个人域名</h3><p>待续</p><h3 id="6-图床加速"><a href="#6-图床加速" class="headerlink" title="6. 图床加速"></a>6. 图床加速</h3><p>待续</p><h3 id="7-Markdown-编辑工具"><a href="#7-Markdown-编辑工具" class="headerlink" title="7. Markdown 编辑工具"></a>7. Markdown 编辑工具</h3><p>当前有许多好用的 <a href="https://www.jianshu.com/p/d4e331770e60" target="_blank" rel="noopener">Markdown 编辑工具</a>，有的收费，有的免费，相对而言收费工具的体验较好。Markdown 文件的后缀名为<code>.md</code>，对于一名程序员来说，最友好的Markdown 编辑界面当然是 IDE 自带的 Markdown 编辑插件。<br>在 <a href="https://www.jetbrains.com/pycharm/" target="_blank" rel="noopener">Pycharm</a> 中添加 Markdown 插件的步骤如下，<code>File-&gt;Settings-&gt;Plugins-&gt;Install JetBrains Plugins-&gt;输入Markdown-&gt;选择插件-&gt;Install-&gt;安装完成后重启PyCharm</code>。编辑界面如下图所示，黑色背景，支持预览，所见即所得。</p><div align="center"><img src="/2018/12/01/hello-world/pycharm_markdown.png" alt="Pycharm Markdown 插件编辑效果图" width="500" height="300"><p style="font-size:90%;color:#00CD00">Pycharm Markdown 插件编辑效果图</p></div> <p>在 VSCodescode 中支持 Markdown 语法，只需要下载 Markdown 预览插件即可。在 VSCode 中添加插件的步骤如下，选择左边栏第四个图标 <code>Extensions</code>，在输入框搜索 <code>Markdown Preview Enhanced</code>，安装成功后重启 VSCode。编辑界面如下图所示，黑色背景，支持预览，所见即所得。</p><div align="center"><img src="/2018/12/01/hello-world/vscode_markdown.png" alt="VSCode Markdown 插件编辑效果图" width="500" height="300"><p style="font-size:90%;color:#00CD00">VSCode Markdown 插件编辑效果图</p></div> ]]></content>
      
      
      <categories>
          
          <category> 博客 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> hexo 博客 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>为什么写博客</title>
      <link href="/2018/11/28/why-blog/"/>
      <url>/2018/11/28/why-blog/</url>
      
        <content type="html"><![CDATA[<blockquote><p><strong>刘未鹏 —— 《暗时间》</strong><br>写一个博客有很多的好处，却没有任何明显的坏处。更明确的说：用博客的形式来记录下你有价值的思考，会带来很多好处，却没有任何明显的坏处。写一个长期的价值博客最大的几点好处:  </p><ol><li>能够交到很多志同道合的朋友。 书写是为了更好地思考。 </li><li>“教”是最好的“学”。如果一件事情你不能讲清楚，十有八九你还没有完全理解。</li><li>激励你去持续学习和思考。</li><li>学会持之以恒地做一件事情。</li><li>一个长期的价值博客是一份很好的简历。</li></ol></blockquote><p>谨以博客记录算法菜鸟的“攻城狮”之路。</p>]]></content>
      
      
      
    </entry>
    
  
  
</search>
