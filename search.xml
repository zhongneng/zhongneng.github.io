<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[GCC编译简介]]></title>
    <url>%2F2019%2F01%2F01%2Fcompile-and-link%2F</url>
    <content type="text"><![CDATA[1. 编译 C 程序GCC (GNU Compiler Collection) 使用可移植性的C语言写成，能够对自身进行编译，因此很容易被移植到新系统上。编译是指把一个纯文本的源代码“程序”转变为机器码，即用于控制计算机的中央处理单元的 1 和 0 的序列。这种机器码被存放在称为“可执行文件”的文件中，有时也称为二进制文件。程序可以编译自单个源文件或多个源文件，还可以用到系统的库文件和头文件。假定已经编写好“Hello World”的C语言程序 #include &lt;stdio.h&gt; int main(void) { printf(&quot;Hello, world!&quot;); return 0; } 那么在linux系统 hello.c 程序文件路径下输入如下命令，即可对“Hello World”的纯文本源码进行编译。 $ gcc -Wall hello.c -o hello 这样就把“hello.c”中的源代码编译成机器码并存储在可执行文件“hello”中。用“-o”选项可以指定存储机器码的输出文件，该选项通常是命令上的最后一个参数。如果省略，输出将被保存到默认文件“a.out”中。“-Wall”选项打开所有最常用的编译警告，推荐总是使用该选项。 一个程序可以被分成多个源文件以便于编辑和理解，由其是在大型程序中。我们可以将 hello.c 文件分为 “main.c” “hello_func.c” 和 “hello.h”，其中“hello.h”文件中仅仅只有一行 void hello(const char *name) ，是对hello函数的原型进行声明。“main.c”为主程序 #include &quot;hello.h&quot; int main(void) { hello(&quot;world&quot;); return 0; } hello函数自身的定义包含在“hello_func.c”文件中 #include &lt;stdio.h&gt; #include &quot;hello.h&quot; void hello(const char *name) { print(&quot;Hello, %s !&quot;, name) } 其中 # include &quot;FILE.h&quot; 和 # include &lt;FILE.h&gt; 这两种 include 声明形式的含义是有差异的，前者是先在当前目录搜索&quot;FILE.h&quot;，然后再查看包含系统头文件的目录，而后者则是直接搜索系统目录的头文件，默认情况下不会再当前目录下去查找头文件。使用如下命令即可对多个源码文件进行编译 $ gcc -Wall main.c hello_func.c -o newhello 注意，头文件“hello.h”不需要在命令行上的源文件名列表中指定。“hello.h”源码文件中的# include指示符会指导编译器在合适的时候自动地包含它。程序中的所有部分已经被组合成单个的可执行文件，其象前面由单个源文件生成的可执行文件一样输出同样的结果。 2. 独立地编译文件如果整个程序代码被存储在单个源文件中，那么对某个函数的任何改变都需要将整个源码文件重新编译以生成一个新的可执行文件，而重新编译大型源码文件可能需要花费大量的时间。当程序被存储在一个个单独的源码文件中时，只有那些被修改过的源码文件才需要重新编译。通过将源文件分开一个个编译，然后再链接在一起，对大型程序进行修改时可以节约大量时间。在第一阶段，文件被编译但不生成可执行文件，编译的结果被称为对象文件（obj文件），用GCC时有 .o 的后缀名。在第二个阶段，各个对象文件由一个被称为连接器的单独程序合成在一起。连接器把所有的对象文件组合在一起生成单个的可执行文件。对象文件包含的是机器码，其中任何对在其他文件中的函数（或变量）的内存地址的引用都留着没有被解析。这样就允许在互相之间不直接引用的情况下编译各个源代码文件。连接器在生成可执行文件时会填写这些还缺少的地址。 2.1 从源文件生成对象文件命令行选项“-c”用于把源码文件编译成对象文件。例如，下面的命令将把源文件“main.c”编译成一个对象文件 $ gcc -Wall -c main.c 该命令会生成一个包含main函数机器码的对象文件“main.o”。它包含一个队外部函数hello的引用，但在这个阶段该对象文件中的对应的内存地址留着没有被解析（它将在后面链接时被填写）。编译源文件“hello_func.c” 的相应命令为： $ gcc -Wall -c hello_func.c 在这里不需要用 -o 选项来指定输出文件的文件名。当用 -c 来编译时，编译器会自动生成与源文件同名，但用 .o 来代替原来的扩展名的对象文件。由于 “main.c” 和 “hello_func.c”中的 # include 声明，“hello.h”会自动被包括进来，所以在命令行上不需要指定该头文件。 2.2 从对象文件生成可执行文件生成可执行文件的最后步骤是用gcc把各个对象文件链接在一起并补充缺失的外部函数的地址。要把对象文件链接在一起，只需要把他们简单的列在命令行上即可： $ gcc main.o hello_func.o -o hello 这是几个很少需要用到 “-Wall” 警告选项的场合之一，因为每个源文件已经被成功的被编译成对象文件了。一旦源文件被编译，链接是一个要么成功要么失败的明确过程（只有在有引用不能解析的情况下才会链接失败）。 2.3 对象文件的链接次序在类Unix系统上，传统上编译器和链接器搜索外部函数的次序是在命令行上指定的对象文件中从左到右的查找，这意味着包含函数定义的对象文件应当出现在调用这些函数的任何文件之后。例如 main.o 调用 hello_func.o 函数，因此包含hello函数的文件“hello_func.o”应该被放在“main.o”之后。 $ gcc main.o hello_func.o -o hello 如果次序搞反了，有的编译器或链接器会报错。虽然当前绝大部分编译器和链接器会不管次序搜索所有的对象文件，但由于不是所有的编译器都这么做，最好遵守从左到右排序对象文件的惯例。如果命令行上已经包括了所有必须的对象文件，但你还是碰到意料之外的未定义引用这种问题，那就应该想想这个问题。 2.4 与外部库文件链接2.4.1 静态库库是已经编译好并能被链接入程序的对象文件的集合。库通常被存储在扩展名为 .a 的特殊归档文件中，被称为静态库。标准的系统库通常能在 /usr/lib 和 /lib 目录下找到。例如在类Unix系统中， C的数学库常被放在文件 /usr/lib/math.a 中，而该库中的相应的函数的原型声明在头文件 /usr/include/math.h 中。下面是调用数学库 libm.a 中外部函数 sqrt 的一个例子，假定文件名为“calc.c”： #include &lt;math.h&gt; #include &lt;stdio.h&gt; int main(void) { double x = sart(2.0); printf(&quot;The square root of 2.0 is %f&quot;, x); return 0; } 试图只用该源文件就生成可执行文件会导致在链接阶段编译器报错 $ gcc -Wall calc.c -o calc 由于在没有外部数学库 libm.a 的情况下，对函数 sqrt 的引用不能解决。函数 sqrt 并不定义在源程序中或默认的C库 libc.a 中，而且除非 libm.a 被显示指定，否则编译器不会链接该库文件。为了使得编译器能够把 sqrt 函数链接到主程序 “calc.c”， 需要在命令行上显示地指定该库文件： $ gcc -Wall calc.c /usr/lib/libm.a -o calc 为了避免在命令行上指定长路径名，编译器提供了短选项 “-l” 用于链接库文件。例如去路径指定的苦命可用下述命令代替： $ gcc -Wall calc.c -lm -o calc 通常，编译器选项“-lName” 试图链接标准库目录下的文件名为“libName.a” 中的对象文件。另外可以通过命令行和环境变量指定的目录链接，在大型程序中通常会用到很多 -l 选项，来链接像数学库、图像库和网络等。 2.4.2 共享库虽然上面的例子程序可以被成功编译和链接，但生成的可执行文件要能被载入并运行，还缺少最后一步。如果你试图直接启动该可执行文件，在绝大部分系统上将报错：libgdbm.so.*: cannot open shared object file: No such file or directory。因为 GDBM 软件包提供的共享库在可执行文件运行以前必须先从磁盘上被载入。外部库通常用两种形式提供：静态库和共享库。静态库就是前面看到过的 .a 文件，当程序与一个静态库链接时改程序用到的外部函数（在静态库包含的对象文件中）的机器码被从库中复制到最终生成的可执行文件中。处理共享库（动态链接库）用的是一种更高级的链接形式，它会使得可执行文件比较小。共享库使用 .so 后缀名，表示 共享对象 (shared object)。一个与共享库链接的可执行文件仅仅包含它用到的函数相关的一个表格，而不是外部函数所在的对象文件的整个机器码。在可执行文件开始运行以前，外部函数的机器码由操作系统从磁盘上的该共享库中复制到内存中，这个过程被称为动态链接（dynamic linking）。因为一份库可以在多个程序间共享，所以动态链接使得可执行文件更小，也节省了磁盘空间。绝大部分操作系统提供了虚拟内存机制，该机制允许物理内存中的一份共享库被要用到该库的所有运行的程序共用，节省了内存和磁盘空间。此外，共享库使得升级库时不需要重新编译用到它的程序（只要库提供的接口不变就行）。由于上述优点，在绝大部分系统上gcc编译程序时默认链接到共享库。 2.4.3 链接库搜索路径在命令行上的库的次序遵照像对象文件中的同样的惯例——从左到右搜索，即包含函数定义的库应该出现在任何使用到该函数的源文件和对象文件之后，否则有的编译器会报错。使用库文件，为了得到函数参数和返回值正确类型的声明，必须包括相应的头文件。如果没有函数声明，可能传递错误类型的函数参数，从而导致错误的结果。在编译用到库的程序时，常碰到的一个问题是include的头文件头错误：FILE.h: No such file or directory。如果头文件不在GCC用到的标准库目录中，就会出现这样的错误。搜索文件的目录列表被称为 include路径，而搜索库的目录列表被称为 搜索路径 或 链接路径。在这些路径中的目录是按次序搜索的，例如 /usr/local/include 中找到的头文件优先于 /usr/include 中的同名文件。类似的， /usr/local/lib 中找到的库优先于 /usr/lib 中的同名库。当有其他库被安装到另外的目录中，为了能够按序找到这些库，需要扩展搜索路径。编译器选项 -I 和 -L 用于把新目录添加到各自的include路径和库搜索路径的头上。 通过shell中的环境可以控制头问价和库的搜索路径。除了可以在每次开始shell会话的相应登录文件“.bash_profile”中自动设置，还可以使用环境变量 C_INCLUDE_PATH（针对C的头文件）和 CPP_INCLUDE_PATH（针对C++的头文件）把其他目录添加到 include 路径中。例如，当编译C程序时，下面的命令会把 /opt/gdbm/include 添加到include路径中。 $ C_INCLUDE_PATH=/opt/gdbm/include $ export C_INCLUDE_PATH 该目录将在命令行上用选项 -I 指定的任何目录之后，但在标准默认目录 /usr/local/include 和 /usr/include 之前被搜索。 Shell命令 export 是必要的，以便shell以外的程序也能获得该环境变量。类似的，使用环境变量 LIBRARY_PATH 可以把另外的目录添加到链接路径中去。例如下面的命令会把 /opt/gdbm/lib 添加到链接路径中。该目录将在命令行上用选项 -L 指定的任何目录之后，但在标准默认目录 /usr/local/lib 和 /usr/lib 之前被搜索。 $ LIBRARY_PATH=/opt/gdbm/lib $ export LIBRARY_PATH 环境变量设置好之后，默认路径就包含了环境便令 C_INCLUDE_PATH 和 LIBRARY_PATH 中指定的目录。 遵循标准Unix搜索路径的规范，搜索目录可以在环境变量中用冒号分割的列表形式一起指定：DIR1:DIR2:DIR:3:...，这些目录被依次从左到右搜索。单个点 . 可以用来指示当前目录。在命令行上可以重复使用 -I 和 -L 选项来指定多个搜索路径的目录。在日常的使用情况中，通常用 -I 和 -L 选项把目录添加到搜索路径。当环境变量和命令行选项被同时使用时，编译器按照下面的次序搜索目录: 从左到右搜索由命令行 -I 和 -L 指定的目录 由环境变量指定的目录 默认的系统目录]]></content>
      <categories>
        <category>基础知识</category>
      </categories>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习简介]]></title>
    <url>%2F2018%2F12%2F13%2Fintroduction-ml%2F</url>
    <content type="text"><![CDATA[Tom Mitchell —— Meaching LearningA computer program is said to learn to perform a task T from experience E, if its performance at task T, as measured by a performance metric P, improves with experience E over time. Machine Learning is a subfield within Artificial Intelligence that builds algorithms that allow computers to learn to perform tasks from data instead of being explicitly programmed. 1. 机器学习的应用领域机器学习是人工智能的核心，是使计算机具有智能的根本途径，其应用遍及人工智能的各个领域。机器学习的应用领域十分广泛，例如图像中的目标检测 ( Object Detection )，光学字符识别 ( Optical Character Recognition，OCR )，文本分析中的垃圾邮件过滤 ( Spam Filtering ) 和情感分析 ( Sentiment Analysis )，数据挖掘中的异常值检测 ( Anomaly Detection )和聚类 ( Clustering ) 以及视频中的自动驾驶和机器人技术。 2. 机器学习算法如何工作机器学习是对能通过经验自动改进的计算机算法的研究。一个程序被认为能从经验E中学习，解决任务T，达到性能度量值P，而且仅当有了经验E后，经过P评判，程序在处理任务T时的性能有所提升。 A computer program is said to learn to perform a task T from experience E, if its performance at task T, as measured by a performance metric P, improves with experience E over time. 例如鼎鼎大名的阿尔法围棋 ( AlphaGo ) 通过大量的学习人类棋手的比赛 ( experience E )，最终在围棋比赛 ( task T ) 中战胜 ( performance P ) 了人类的顶尖围棋高手。那么我们可以说 AlphaGo 通过机器学习获得了人工智能 ( Artificial Intelligence )。 想象如下两幅画面：(a) 当你向智能系统输入一张特朗普的照片，系统会输出特朗普的名字；(b) 当你向智能系统输入一段歌声，系统会输出这段歌声所对应的歌词; 在例子(a)中，人脸识别系统的“经验E”就是由包含特朗普的照片和其他照片组成的图像集合，“性能P”就是判断特朗普照片正确的概率。同理，语音识别系统的”经验E”就是每个文字所对应的各种语音数据，“性能P”就是一段语音中文字识别的准确率。 因此为了能够通过机器学习算法训练“智能系统”，我们必须拥有包含许多训练样本 ( training examples ) 的数据集 ( dataset )。通常会将每一个样本 ( sample )表示成一些属性 (attribute) 或者特征 (feature) 的固定集合，以生成计算机能够理解的数据。 3. 机器学习算法的分类从是否使用训练数据标签的角度，机器学习算法可以分为监督学习和非监督学习两大类。监督学习的训练集要求包括输入数据和对应的输出标签，通过已有的训练样本（即已知数据及其对应的输出）去训练得到一个最优模型（这个模型属于某个函数的集合，最优表示某个评价准则下是最佳的），再利用这个模型将所有的输入映射为相应的输出。在非监督学习中，输入数据没有对应的输出标签，也就没有确定的正确结果，需要根据样本间的相似性去发现数据本身的内在规律。在监督学习中，最常用的两种方法是分类 ( Classification ) 和 回归 ( Regression )，分类和回归最本质的区别就是其输出的标签值是否是连续的。假设明天的天气为有雨和没有雨，那么可以将有雨的情况视为0，没有雨的情况视为1，那么输出的值就是离散的( 0和1 )。那么根据今天的天气预测明天有没有雨就可以视为分类。假设不能准确判断明天是否下雨或者不下雨，但是可以根据今天的天气给出明天下雨的概率值，概率值可以在[0, 1]范围内的连续值，值越大，下雨的概率越高。因此根据今天的天气判断明天下雨的概率为0.8就可以视为回归。在非监督学习中，最常用的方法是聚类 ( Clustering )。例如有一堆苹果和橘子装在一个黑箱子里，假设我们事先不知道盒子里是橘子和苹果，那么我们可以根据水果的大小将其分为两类，但是并不清楚每一类究竟是什么水果，这个过程可以视为聚类。 4. 常见机器学习算法的优缺点4.1 朴素贝叶斯算法朴素贝叶斯属于生成式模型（关于生成模型和判别式模型，主要还是在于是否是要求联合分布），如果满足条件独立性假设，朴素贝叶斯分类器的收敛速度将快于判别模型，例如逻辑回归。即使NB条件独立假设不成立，NB分类器在实践中仍然表现的很出色。它的主要缺点是它不能学习特征间的相互作用，比如，虽然你喜欢 A 和 B 主演的电影，但是它不能学习出你不喜欢 A 和 B 在一起演的电影。 优点： 朴素贝叶斯模型发源于古典数学理论，有着坚实的数学基础，以及稳定的分类效率。 对小规模的数据表现很好，能个处理多分类任务，适合增量式训练。 对缺失数据不太敏感，算法也比较简单，常用于文本分类。 缺点： 需要计算先验概率。 分类决策存在错误率。 对输入数据的表达形式很敏感。 4.2 逻辑回归算法逻辑回归 ( Logistic Regression ) 属于判别式模型，有很多正则化模型的方法 (例如 L0、 L1 和 L2 范数等，而且不用像朴素贝叶斯方法那样假设特征之间互不相关。与决策树与支持向量机相比，得到的概率结果可以进行解释，而且可以方便地利用新数据来更新模型。 优点： 实现简单，广泛的应用于工业问题上。 分类时计算量非常小，速度很快，存储资源低。 便利的观测样本概率分数。 对逻辑回归而言，多重共线性并不是问题，它可以结合L2正则化来解决该问题。 缺点： 当特征空间很大时，逻辑回归的性能不是很好。 容易欠拟合，一般准确度不太高。 不能很好地处理大量多类特征或变量。 4.3 线性回归算法线性回归是用于回归的，而不像Logistic回归是用于分类，其基本思想是用梯度下降法对最小二乘法形式的误差函数进行优化，当然也可以使用正规方程 ( normal equation ) 直接求得参数的解。 优点： 实现简单，计算简单。 缺点： 不能拟合非线性数据。 4.4 最近邻算法最近邻 ( K Nearest Neighbor，KNN ) 算法简单易行，具有较强的一致性结果。随着数据趋于无限，算法保证错误率不会超过贝叶斯算法错误率的两倍。对于一些好的K值，K近邻保证错误率不会超过贝叶斯理论误差率。需要根据数据的实际情况选择合适的 K 值。一般情况下，在分类时较大的K值能够减小噪声的影响。但会使类别之间的界限变得模糊。一个较好的K值可通过各种启发式技术来获取，比如，交叉验证。另外噪声和非相关性特征向量的存在会使K近邻算法的准确性减小。 算法的基本流程： 计算训练样本和测试样本中每个样本点的距离； 对所有计算的距离值进行排序； 选前k个最小距离的样本； 根据这k个样本的类别标签进行投票，得到最后的分类类别。 优点： 理论成熟，思想简单，既可以用来做分类也可以用来做回归。 可用于非线性分类。 训练时间复杂度为O(n)。 对数据没有假设，准确度高，对离群点(outlier)不敏感。 缺点： 计算量大。 样本不平衡问题（例如类别A的样本数量很多，而类别B的样本数量则很少）。 需要大量的内存。 4.5 决策树算法决策树易于解释，可以轻松地处理特征间的交互关系，但是不支持在线学习，当有新样本时，决策树需要全部重建。而且决策树容易过拟合，但是随机森林算法通过集成多个决策树可以解决过拟合的问题。 优点： 计算简单，易于理解，可解释性强。 比较适合处理有缺失属性的样本。 能够处理不相关的特征。 在相对短的时间内能够对大型数据源做出可行且效果良好的结果。 缺点： 容易发生过拟合（随机森林可以很大程度上减少过拟合）。 忽略数据之间的相关性。 样本不平衡问题 (决策树的信息增益偏向于具有更多数值的特征)。 4.6 Adaboosting算法Adaboost是一种加和模型，每个模型都是基于上一次模型的错误率来建立的，过分关注分错的样本，而对正确分类的样本减少关注度，逐次迭代之后，可以得到一个相对较好的模型。 优点： Adaboost是一种有很高精度的分类器。 可以使用各种方法构建子分类器，Adaboost算法提供的是框架。 当使用简单分类器时，计算出的结果是可以理解的，并且弱分类器的构造极其简单。 不用做特征筛选，不容易过拟合。 缺点： 对离群点(outlier)较为敏感。 4.7 支持向量机算法支持向量机（Support Vector Machine，SVM）是一种监督式学习的方法，可广泛地应用于统计分类以及回归分析。支持向量机将向量映射到一个更高维的空间里，在这个空间里建立有一个最大间隔超平面。在分开数据的超平面的两边建有两个互相平行的超平面，分隔超平面使两个平行超平面的距离最大化。假定平行超平面间的距离或差距越大，分类器的总误差越小。支持向量机准确率高，为避免过拟合提供了很好的理论保证，而且即使数据在原特征空间线性不可分，也可以通过核函数将原特征空间映射到更高维线性可分的空间，但是内存消耗大，难以解释，运行和调参比较麻烦。 优点： 可以解决高维特征空间问题。 能够处理非线性特征。 无需依赖整个数据。 泛化能力强。 缺点： 数据样本很多时，效率不高。 对非线性问题没有通用解决方案( 很难找到恰当的核函数 )。 对缺失数据敏感。 4.8 神经网络神经网络（Neural Network）是一种运算模型，由大量的节点（或称神经元）相互联接构成。每个节点代表一种特定的输出函数，称为激励函数（activation function）。每两个节点间的连接都代表一个对于通过该连接信号的加权值，称之为权重。网络的输出则依赖于网络的连接方式，权重值和激励函数的不同而不同。而网络自身通常都是对自然界某种算法或者函数的逼近，也可能是对一种逻辑策略的表达。 优点： 分类的准确度高。 并行分布处理能力强，分布存储及学习能力强。 对噪声神经有较强的鲁棒性和容错能力，能充分逼近复杂的非线性关系。 具备联想记忆的功能。 缺点： 神经网络需要大量的参数，如网络拓扑结构、权值和阈值的初始值。 不能观察之间的学习过程，输出结果难以解释，会影响到结果的可信度和可接受程度。 学习时间过长。 4.9 K-Means聚类算法K均值聚类( K-Means Clustering )算法是一种基于样本间相似性度量的间接聚类方法，属于非监督学习方法。该算法以K为参数，把N个对象分为K个簇，以使簇内具有较高的相似度，而且簇间的相似度较低。相似度的计算根据一个簇中对象的平均值（簇的重心）来进行。首先随机选择K个对象，每个对象代表一个聚类的质心。对于其余的每一个对象，根据该对象与各聚类质心之间的距离，把它分配到与之最相似的聚类中。然后，计算每个聚类的新质心。重复上述过程，直到准则函数（例如误差的平方和）收敛。 优点： 算法简单，容易实现。 对处理大数据集，该算法是相对可伸缩的和高效率的，因为它的复杂度大约是O(NKT)，其中N是所有样本的数量，K是簇的类别数，T是迭代次数。 当簇是密集的、球状或团状的，且簇与簇之间区别明显时，聚类效果较好。 缺点： 对数据类型要求较高，适合数值型数据。 可能收敛到局部最小值，在大规模数据上收敛较慢。 难以选取合适的K值。 对初值的簇心值敏感，对于不同的初始值，可能会导致不同的聚类结果。 对于孤立点数据敏感，少量的该类数据能够对平均值产生极大影响。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>meachine learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[你的模型运行快吗？]]></title>
    <url>%2F2018%2F12%2F05%2Fmodel-computation%2F</url>
    <content type="text"><![CDATA[模型占用的存储空间、模型运行时消耗的内存空间、模型运行的速度 1. CNN不同层的计算量了解模型计算量的一种简单方法就是计算这个模型总共做了多少次浮点运算。除了计算量，内存带宽也是影响计算效率的重要因素。 1.1 乘积相加神经网络中的绝大多数操作都是浮点数的乘法进行求和。例如： y = w[0]*x[0] + w[1]*x[1] + w[2]*x[2] + ... + w[n-1]*x[n-1] 上式中，输入 w 和 x 是两个向量，输出 y 是一个标量 ( 实数 )。在神经网络的卷积层和全连接层中， w 是层学习到的参数，x 就是层的输入，而 y 则是层的输出的一部分，因为典型的层结构有多个输出。其中 w[0]*x[0] + ... 称为一次 乘积相加操作(multiply-accumulate operations), 因此两个 n 维向量的点积含有 n 个 MACCs(乘积相加) 计算单元。 Technically speaking there are only n - 1 additions in the above formula, one less than the number of multiplications. Think of the number of MACCs as being an approximation, just like Big-O notation is an approximation of the complexity of an algorithm. 从每秒浮点运算 ( floating point operations per second, FLOPS ) 的角度来看，一次点积操作包含 2n-1 个 FLOPS，因为其中含有 n 次乘法运算和 n-1 次加法运算。 1.2 全连接层在全连接层，所有的输入单元和输出单元相互连接。对于含有 I 个输入值和 J 个输出值的的层，权重 W 存储在 I×J 的矩阵中，因此全连接层的计算可以写作： y = matmul(x, W) + b 矩阵乘法是有一系列点乘组合而成的。每一个点乘由输入 x 和矩阵W 的每一列运算得到。因此运算 matmul(x, W) 包含 I×J 个 MACCs 单元，和权重矩阵的元素个数相同。例如卷积层的最后输出为 (512, 7, 7), 那么经过 faltten 操作后，输入 I=512x7x7。 1.3 激活函数通常层的后面会跟随非线性激活函数，例如 ReLU 或者 sigmoid 函数。由于激活函数没有乘法运算，因此使用 FLOPS 来衡量计算时间。不同激活函数的计算量是不同的。ReLU 激活函数的表达式为： y = max(x, 0) 在 GPU 上只有一次操作。假设全连接层有 J 个输出单元，ReLU函数进行了 J 次最大值操作，因此含有 J 个 FLOPS 单元。Sigmoid 激活函数表达式为： y = 1 / (1 + exp(-x)) 由于 Sigmoid 函数包含幂运算，因此计算量比较复杂。通常将加法运算、减法运算、乘法运算、除法运算、幂运算和平方根运算称为一次 FLOPS。Sigmoid 函数中包含四种不同的运算操作，因此含有 4 次 FLOPS。假设全连接层有 J 个输出单元，那么Sigmoid 函数含有 4xJ 个 FLOPS 单元。通常激活函数只占模型总运算量的很小一部分。 1.3 卷积层卷积层的输入和输出不是向量，而是三维 ( Height*Width*Channels ) 的特征图。假定正方形的卷积核边长为 k，那么卷积层不考虑偏置和激活函数的 MACCs 为： k × k × Channels_in × Height_out × Width_out × Channels_out 这里使用输出的 Height 和 Width 是因为考虑到卷积时的stride, dilation factors, padding, etc。对于卷积核为为 (3, 3, 128) 且输入为 (112, 112, 64) 的卷积计算， 它的 MACCs 为： 3 × 3 × 64 × 112 × 112 × 128 = 924844032 In this example, we used “same” padding and stride = 1, so that the output feature map has the same size as the input feature map. It’s also common to see convolutional layers use stride = 2, which would have chopped the output feature map size in half, and we would’ve used 56 × 56 instead of 112 × 112 in the above calculation. 1.4 深度可分离卷积深度可分离卷积 ( depthwise-separable convolution ) 首先在 Xception 中被使用，它将常规的卷积操作分解为 depthwise 卷积与 pointwise 卷积两个部分。该结构和常规提取特征的卷积操作类似，但是参数量和运算成本较低，在轻量级网络 ( MobileNet )中十分常见。 假设输入层为 (112, 112, 64)，经过 (3, 3, 128)的卷积核，假定使用 same padding 并且 stride=1，使得输入输出特征图大小相同，那么最终得到 (112, 112, 128) 的特征图。常规卷积示意图如下所示。 MACCs 次数为： 3 × 3 × 64 × 112 × 112 × 128 = 924844032 Depthwise Convolution 的每个卷积核只与输入的一个通道进行卷积，卷积核的数量与上一层的通道数相同。因此输入图像经过 depthwise 卷积之后生成 3 个单通道的特征图，如下图所示。 MACCs = K × K × Channels_in × Height_out × Width_out = 3 × 3 × 64 × 112 × 112 = 7225344 Depthwise Convolution 完成后的特征图数量与输入层的通道数相同，无法扩展特征图的数量，而且无法有效利用不同通道在相同空间位置上的特征信息。因此在 depthwise 卷积之后需要 pointwise Convolution 将特征图进行组合生成新的特征图。Pointwise Convolution 的卷积核大小为 1x1。 MACCs = 1 × 1 × Channels_in × Height_out × Width_out x Channels_out = 1 × 1 × 64 × 112 × 112 x 128 = 102760448 因此 depthwise-separable convolution 的 MACCs 为： MACCs = (K × K × Channels_in × Height_out × Width_out) + (Channels_in × Height_out × Width_out × Channels_out) = Channels_in × Height_out × Width_out × (K × K + Channel_out) The exact factor is K × K × Cout / (K × K + Cout) . It should be pointed out that depthwise convolutions sometimes have a stride &gt; 1, which reduces the dimensions of their output feature map. But a pointwise layer usually has stride = 1, and so its output feature map will always have the same dimensions as the depthwise layer’s. 1.5 批量归一化层批量归一化层 ( Batch normalization Layer) 每一个输出的函数表达式可以写为： z = gamma * (y - mean) / sqrt(variance + epsilon) + beta 其中 y 是上一层输出特征图的一个元素，mean 为均值，variance 为方差，epsilon 确保分母不为0，gamma为尺度因子，beta 为偏置。每一个通道都有其对应的值，因此对于通道为 c 的卷积输出层，batch normalization layer 学习的参数量为 4c。 z = gamma * ((x[0]*w[0] + x[1]*w[1] + ... + x[n-1]*w[n-1] + b) - mean) / sqrt(variance + epsilon) + beta 由于在预测过程中移除了 batch normlization layer，因此考虑模型的计算量时可以不用关注正则化层的影响。 This trick only works when the order of the layers is: convolution, batch norm, ReLU — but not when it is: convolution, ReLU, batch norm. The ReLU is a non-linear operation, which messes up the math. 1.6 池化层对于 112, 112, 128) 的特征图，如果最大池化的 pooling size = 2 并且 stride = 2，那么 FLOPS 操作数为 112 × 112 × 128 = 1605632。可以看到，池化层的操作数远远少于卷积层的操作数，因此池化层也是网络计算复杂度的舍入误差。 2. 模型耗费的内存在模型的每一层计算中，硬件设备需要从主存储中读取输入向量或者特征图的值，从主存储中读取权重参数并与输入计算点积，将得到的新向量或者特征图作为结果写入主存储中。这些操作都涉及到大量的内存读写，耗费的时间可能远远大于计算的次数。 2.1 权重的内存层将权重保存在主存储中，这意味着权重参数越少，模型运行速度越快。如前文所述，输入为 I 个神经元和输出为 J 个神经元之间的权重参数为 I x J，加上偏置向量，总的参数为 ( I + 1) x J。对于大小为 k，输入通道数为 Channels_in，输出通道数为 Channels_out 的卷积层的参数为 k x k Channels_in x Channels_out 加上偏置向量参数 Channels_out。对于输入 4096 输出为 4096的全连接层，其权重参数量为 (4096 + 1) x 4096 = 16781312。对于输入为 (64, 64, 32) 卷积核为 (3, 3, 48) 的卷积层，其权重参数量为 (3 x 3 x 32 x 48 + 48 = 13872)。可以看到，相比于卷积层，全连接层的参数量相对更多。 Fully-connected and convolutional layers are actually very similar. A convolutional layer is basically a fully-connected layer with the vast majority of the connections set to 0 — each output is only connected to K × K inputs rather than all of them, and all the outputs use the same values for these connections. This is why convolutional layers are so much more efficient about memory, since they don’t store the weights for connections that are not used.]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Centos7使用Docker快速搭建深度学习环境]]></title>
    <url>%2F2018%2F12%2F04%2Fdocker-env%2F</url>
    <content type="text"><![CDATA[1. 什么是 Docker 解决 “在我的机器上可以正常工作，为什么在你的机器上不能工作” 的问题。 随着深度学习技术的飞速发展，各种深度学习框架都有大量的粉丝。如何在一台电脑上安装多个深度学习框架？同一深度学习框架的不同版本依赖于不同的GPU版本，但是一台服务器只可能安装唯一版本的GPU。当多名开发人员在统一服务器上使用不同的深度学习框架进行开发时，往往会产生环境冲突。最好的解决方案就是采用虚拟技术。 Docker 是世界领先的软件容器平台，也是目前最流行的 Linux 容器解决方案。Docker 将应用程序与该程序的依赖，打包在一个文件里面。运行这个文件，就会生成一个虚拟容器。程序在这个虚拟容器里运行，就好像在真实的物理机上运行一样。有了 Docker，就不用担心环境问题。而且 Docker 的接口相当简单，用户可以方便地创建和使用容器，把自己的应用放入容器。容器还可以进行版本管理、复制、分享、修改，就像管理普通的代码一样。和虚拟机相比，由于没有臃肿的从操作系统，Docker可以节省大量的磁盘空间以及其他系统资源。 虚拟机通常用于彻底隔离整个运行环境。例如，云服务提供商通常采用虚拟机技术隔离不同的用户。而Docker通常用于隔离不同的应用，例如前端，后端以及数据库。 2. 安装 Docker(a) 安装依赖包 $ yum install -y yum-utils device-mapper-persistent-data (b) 配置稳定仓库 $ yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo (c) 安装(默认为最新版) $ yum install docker-ce (4) 修改docker运行时的根目录, 解决存储不足的问题 $ vim /lib/systemd/system/docker.service # 在 ExecStart=/usr/bin/dockerd 后添加 --graph=/home/docker $ ExecStart=/usr/bin/dockerd --graph=/home/docker (5) 重新启动docker服务 $ systemctl daemon-reload $ systemctl status docker $ systemctl start docker (7) 测试Docker是否正确安装 $ docker version $ docker run hello-world 3. 安装 nvidia-docker深度学习框架需要使用 GPU 加入计算，如果不安装 nvidia-docker 工具，那么在容器中将会无法调用宿主机上的 GPU 硬件设备。 (a) 移除旧版本 nvidia-GPU 和已经存在的 GPU 容器 $ docker volume ls -q -f driver=nvidia-docker | xargs -r -I{} -n1 docker ps -q -a -f volume={} | xargs -r docker rm -f $ sudo yum remove nvidia-docker (b) 安装依赖包 $ distribution=$(. /etc/os-release;echo $ID$VERSION_ID) $ curl -s -L https://nvidia.github.io/nvidia-docker/$distribution/nvidia-docker.repo | \ sudo tee /etc/yum.repos.d/nvidia-docker.repo (c) 安装nvidia-docker $ sudo yum install -y nvidia-docker2 $ sudo pkill -SIGHUP dockerd (d) 测试 nvidia-docker 是否安装成功 docker run --runtime=nvidia --rm nvidia/cuda:9.0-base nvidia-smi 4. CUDA9-CUDNN7-python3.6 源码安装 Caffe、Caffe2、Tensorflow、 Detectron 和 Darknet(a) 启动 GPU container 并登录 $ nvidia-docker run -tid --name TestMyGPU --net=&#39;host&#39; nvidia/cuda:9.0-cudnn7-devel-centos7 /bin/bash # 启动容器 $ docker exec -it TestMyGPU /bin/bash # 登录容器 (b) 配置变量 $ export http_proxy=http://xx.xx.xx.xx:8080 # 设置网络 $ export https_proxy=https://xx.xx.xx.xx:8080 $ export PYINSTALL=/usr/local/python3 # 设置python3.6安装路径 $ export PATH=$PYINSTALL/bin:$PATH (c) 安装 python3.6 和 Caffe，参照上一篇博客 Centos7 安装 Caffe。 (d) 在 /home 下安装 Caffe2、Tensorflow、 Detectron 和 Darknet # 安装依赖包和 opencv $ yum install -y cmake3 &amp;&amp; pip3 install cython opencv-python==3.4.2.16 # 安装 nccl，GPU分布式通信函数库 $ cd /home &amp;&amp; git clone https://github.com/NVIDIA/nccl.git $ cd nccl &amp;&amp; make -j8 src.build CUDA_HOME=&#39;/usr/local/cuda-9.0/&#39; NVCC_GENCODE=&quot;-gencode=arch=compute_70,code=sm_70&quot; $ yum install -y rpm-build rpmdevtools &amp;&amp; make -j8 pkg.redhat.build &amp;&amp; make install $ echo &#39;export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/lib&#39; &gt;&gt; /root/.bashrc $ cd /home &amp;&amp; rm -rf nccl &amp;&amp; source ~/.bashrc # 验证nccl是否安装成功 # cd /home &amp;&amp; git clone https://github.com/NVIDIA/nccl-tests.git \ # cd nccl-tests &amp;&amp; make -j8 &amp;&amp; ./build/all_reduce_perf -b 8 -e 256M -f 2 -g 1 \ # cd /home &amp;&amp; rm -rf nccl-tests # 安装 darknet $ cd /home &amp;&amp; git clone https://github.com/pjreddie/darknet.git &amp;&amp; cd darknet \ # 修改 Makefile 文件，令 GPU=1，CUDNN=1，OPENCV=1 $ sed -i &#39;s/GPU=0/GPU=1/&#39; Makefile $ sed -i &#39;s/CUDNN=0/CUDNN=1/&#39; Makefile $ sed -i &#39;s/OPENCV=0/OPENCV=1/&#39; Makefile $ make -j32 # 验证 darknet 是否安装成功 # 执行 ./darknet 输出 usage: ./darknet &lt;function&gt; # 安装 Tensorflow 和 Keras $ cd /home &amp;&amp; pip3 install tensorflow-gpu==1.10 keras==2.2.0 # 安装caffe2 $ pip3 install pyyaml future hypothesis pydot $ cd /home &amp;&amp; git clone https://github.com/pytorch/pytorch.git $ cd pytorch &amp;&amp; git submodule update --init --recursive # 解决编译时 cmake 的版本问题： # 将文件 `pytorch/tools/build_pytorch_libs.sh` 复制到 `/home` 路径下 # 修改./tools/build_pytorch_libs.sh 第31和32行 CMAKE_VERSION、CMAKE3_VERSION # CMAKE_VERSION=$(cmake --version | grep &#39;version&#39; | awk &#39;{print $3}&#39; | awk -F. &#39;{print $1&quot;.&quot;$2&quot;.&quot;$3}&#39;) # CMAKE3_VERSION=$(cmake3 --version | grep &#39;version&#39; | awk &#39;{print $3}&#39; | awk -F. &#39;{print $1&quot;.&quot;$2&quot;.&quot;$3}&#39;) $ rm -f /home/pytorch/tools/build_pytorch_libs.sh $ cp -f /home/build_pytorch_libs.sh /home/pytorch/tools/build_pytorch_libs.sh $ rm -f /home/build_pytorch_libs.sh $ python setup.py install $ cd /home &amp;&amp; rm -rf pytorch 验证 caffe2 是否安装成功， python 命令窗口执行 &gt;&gt;&gt; import torch &gt;&gt;&gt; import caffe2 &gt;&gt;&gt; exit() 验证是否能使用 GPU $ cd /home &amp;&amp; python -c &#39;from caffe2.python import core&#39; 2&gt;/dev/null &amp;&amp; echo &quot;Success&quot; || echo &quot;Failure&quot; $ python -c &#39;from caffe2.python import workspace; print(workspace.NumCudaDevices())&#39; $ python /usr/local/anaconda3/lib/python3.6/site-packages/caffe2/python/operator_test/rnn_cell_test.py 安装 COCO-API $ cd /home &amp;&amp; git clone https://github.com/pdollar/coco $ pip3 install setuptools==18.4 &amp;&amp; yum install -y tkinter $ cd coco/PythonAPI &amp;&amp; make -j8 &amp;&amp; make install &amp;&amp; python setup.py install $ cd /home &amp;&amp; rm -rf coco 验证 coco-api 是否安装成功, python命令窗口执行 from pycocotools.coco import COCO 安装detectron $ cd /home &amp;&amp; git clone https://github.com/facebookresearch/detectron $ cd detectron &amp;&amp; make -j8 # 验证 detectron 是否正确安装 $ cd /home/detectron &amp;&amp; python detectron/tests/test_spatial_narrow_as_op.py $ python tools/infer_simple.py \ --cfg configs/12_2017_baselines/e2e_mask_rcnn_R-101-FPN_2x.yaml \ --output-dir tmp/detectron-visualizations \ --image-ext jpg \ --wts https://s3-us-west-2.amazonaws.com/detectron/35861858/12_2017_baselines/e2e_mask_rcnn_R-101-FPN_2x.yaml.02_32_51.SgT4y1cO/output/train/coco_2014_train:coco_2014_valminusminival/generalized_rcnn/model_final.pkl \ demo 5. 使用 Anaconda3 安装 Caffe、Pytorch 和 TensorflowAnaconda 是一个开源的Python发行版本，包含了conda、Python等180多个科学包及其依赖项，是当前最流行的 Python 数据科学开发平台。 因为包含了大量的科学包，Anaconda 的下载文件比较大（约 531 MB），如果只需要某些包，或者需要节省带宽或存储空间，也可以使用 Miniconda 发行版 ( 仅包含 conda 和 Python )。 Anaconda 当前集成了 caffe 和 pytorch，可以利用 Anaconda 快速安装 caffe 和 caffe2 ( 集成在 pytorch ) 中。由于 Docker 容器需要应用程序占用内存尽可能小，因此采用 Miniconda 代替 Anaconda。完成的 Dockerfile 文件如下，在宿主机中含有 Dockfile 文件的当前路径下运行 docker build -t deep_learning_environment:v0.1 . 即可生成满足深度学习环境对应要求的镜像。 FROM nvidia/cuda:9.0-cudnn7-runtime-centos7 ENV LANG=en_US.UTF-8 ARG http_proxy=http://xx.xx.xx.xx:8080 ARG https_proxy=https://xx.xx.xx.xx:8080 # 安装Miniconda3 RUN cd /home \ # 安装依赖包 &amp;&amp; yum install -y epel-release-7-11.noarch wget git make bzip2 &amp;&amp; pip install cython \ # 安装nccl &amp;&amp; cd /home &amp;&amp; git clone https://github.com/NVIDIA/nccl.git \ &amp;&amp; cd nccl &amp;&amp; make -j8 src.build CUDA_HOME=&#39;/usr/local/cuda-9.0/&#39; NVCC_GENCODE=&quot;-gencode=arch=compute_70,code=sm_70&quot; \ &amp;&amp; yum install -y rpm-build rpmdevtools &amp;&amp; make -j8 pkg.redhat.build &amp;&amp; make install \ &amp;&amp; echo &#39;export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/nccl/build/lib&#39; &gt;&gt; /root/.bashrc \ &amp;&amp; cd /home &amp;&amp; rm -rf nccl &amp;&amp; source ~/.bashrc \ # 验证nccl是否安装成功 # cd /home &amp;&amp; git clone https://github.com/NVIDIA/nccl-tests.git \ # cd nccl-tests &amp;&amp; make -j8 &amp;&amp; ./build/all_reduce_perf -b 8 -e 256M -f 2 -g 1 \ # cd /home &amp;&amp; rm -rf nccl-tests &amp;&amp; wget https://repo.anaconda.com/miniconda/Miniconda3-4.3.30-Linux-x86_64.sh \ &amp;&amp; bash Miniconda3-4.3.30-Linux-x86_64.sh -p /usr/local/miniconda3 -b \ # 将 miniconda 添加到系统路径 &amp;&amp; echo &#39;export PATH=/usr/local/miniconda3/bin:$PATH&#39; &gt;&gt; /root/.bashrc \ &amp;&amp; echo &#39;export LD_LIBRARY_PATH=/usr/local/miniconda3/lib:$LD_LIBRARY_PATH&#39; &gt;&gt; /root/.bashrc \ &amp;&amp; source ~/.bashrc &amp;&amp; rm -rf Miniconda3-4.3.30-Linux-x86_64.sh \ # 修改yum的链接问题 &amp;&amp; ln -s -f /usr/lib64/liblzma.so.5 /usr/local/miniconda3/lib/liblzma.so.5 \ # conda 安装 caffe-gpu &amp;&amp; conda install -y caffe-gpu protobuf \ # conda 安装 caffe2 # 直接安装下载速度非常慢，而且有可能失败 &amp;&amp; conda install -y pytorch-nightly -c pytorch \ &amp;&amp; pip install future hypothesis pydot \ # 验证 caffe 和 caffe2 是否安装成功 # $ python &amp;&amp; import torch &amp;&amp; import caffe &amp;&amp; import caffe2 # python -c &#39;from caffe2.python import core&#39; 2&gt;/dev/null &amp;&amp; echo &quot;Success&quot; || echo &quot;Failure&quot; # python -c &#39;from caffe2.python import workspace; print(workspace.NumCudaDevices())&#39; # python /usr/local/anaconda3/lib/python3.6/site-packages/caffe2/python/operator_test/rnn_cell_test.py # 安装 detectron &amp;&amp; cd /home &amp;&amp; git clone https://github.com/facebookresearch/detectron \ &amp;&amp; cd detectron &amp;&amp; pip install cython &amp;&amp; make -j8 \ # 验证 detectron 是否安装正确 # cd /home/detectron &amp;&amp; python detectron/tests/test_spatial_narrow_as_op.py # 安装darknet, 从 github 上下载darknet源码, 修改 Makefile 文件，令 GPU=1，CUDNN=1，OPENCV=1。 &amp;&amp; cd /home &amp;&amp; git clone https://github.com/pjreddie/darknet.git &amp;&amp; cd darknet \ &amp;&amp; sed -i &#39;s/GPU=0/GPU=1/&#39; Makefile \ &amp;&amp; sed -i &#39;s/CUDNN=0/CUDNN=1/&#39; Makefile \ &amp;&amp; sed -i &#39;s/OPENCV=0/OPENCV=1/&#39; Makefile \ &amp;&amp; make -j8 \ # 验证 darknet 是否安装成功 # 执行 ./darknet 输出 usage: ./darknet &lt;function&gt; # 安装tensorflow和keras &amp;&amp; cd /home &amp;&amp; pip install tensorflow-gpu==1.10 keras==2.2.0 -i https://pypi.douban.com/simple/ \ # 验证 tensorflow 和 keras 是否安装成功 # $ python &amp;&amp; import tensorflow &amp;&amp; import keras # 删除 minicond3/pkgs 里面的安装包, 降低内存占用 &amp;&amp; cd /usr/local/miniconda3/ &amp;&amp; rm -rf pkgs]]></content>
      <categories>
        <category>安装教程</category>
      </categories>
      <tags>
        <tag>centoe7 深度学习环境</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Centos7 安装 Caffe]]></title>
    <url>%2F2018%2F12%2F03%2Fcentos-caffe%2F</url>
    <content type="text"><![CDATA[1. Caffe 是什么Caffe 全称 Convolutional Architecture for Fast Feature Embedding，是一种常用的深度学习框架，主要应用在视频、图像处理方面的应用上。得益于RCNN框架的影响力，当前主流的目标检测模型 ( 例如 Faster-RCNN 和 SSD ) 的作者源码都是基于 Caffe 编写的。 2. Centos7 安装 Caffe虽然网上已经有很多相关的安装教程，但是大多数都是基于 Ubantu 系统的，而且网上的教程在安装过程中往往会报出各种莫名其妙的 bug。经过笔者多次血泪实践，发现大多数错误都是因为未能正确安装 boost 和 protouf 工具包。假设 Centos7 已经正确安装 Nvidia GPU 驱动程序和 CUDA9+CUDNN7的加速包，按照如下教程即可正确编译 Caffe 的 Python3.6 接口。如果电脑没有安装 GPU 驱动，请先参照 Nvidia 官网安装教程 正确安装 GPU。 # 安装依赖包 $ yum clean all &amp;&amp; yum makecache &amp;&amp; yum install -y epel-release-7-11.noarch $ yum -y install zlib-devel openssl-devel bzip2-devel expat-devel $ yum -y install gdbm-devel readline-devel sqlite-devel $ yum -y install wget git make unzip libSM libXrender libXext # 安装 Python3.6 $ cd /home &amp;&amp; wget https://www.python.org/ftp/python/3.6.6/Python-3.6.6.tgz $ tar -xvf Python-3.6.6.tgz &amp;&amp; cd Python-3.6.6 $ ./configure --prefix=$PYINSTALL &amp;&amp; make -j32 &amp;&amp; make install $ ln -s $PYINSTALL/bin/python3 $PYINSTALL/bin/python $ cd /home &amp;&amp; rm -rf Python-3.6.6.tgz Python-3.6.6 # 安装 scikit-image $ pip3 install numpy scikit-image -i http://pypi.douban.com/simple/ --trusted-host pypi.douban.com # 安装 caffe 依赖包 $ yum -y install leveldb-devel snappy-devel opencv-devel hdf5-devel $ yum -y install gflags-devel glog-devel lmdb-devel openblas-devel python36-devel # 编译 boost 修复 libboost_python3.so 无法连接的错误 $ cd /home &amp;&amp; wget https://dl.bintray.com/boostorg/release/1.67.0/source/boost_1_67_0.tar.gz $ tar -xvf boost_1_67_0.tar.gz $ cd boost_1_67_0 &amp;&amp; ./bootstrap.sh --with-toolset=gcc $ ./b2 cflags=&#39;-fPIC&#39; cxxflags=&#39;-fPIC&#39; include=/usr/include/python3.6m &amp;&amp; ./b2 install $ ln -s /usr/local/lib/libboost_python36.so /usr/lib64/libboost_python3.so $ echo /usr/local/lib &gt;&gt; /etc/ld.so.conf.d/caffe.conf &amp;&amp; ldconfig $ cd /home &amp;&amp; rm -rf boost_1_67_0.tar.gz boost_1_67_0 # 安装 protobuf $ echo &#39;export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/lib&#39; &gt;&gt; /root/.bashrc $ source ~/.bashrc $ cd /home &amp;&amp; wget https://github.com/protocolbuffers/protobuf/releases/download/v3.5.1/protobuf-cpp-3.5.1.zip $ unzip protobuf-cpp-3.5.1.zip $ cd protobuf-3.5.1 &amp;&amp; ./configure &amp;&amp; make -j32 &amp;&amp; make install $ cd /home &amp;&amp; rm -rf protobuf-cpp-3.5.1.zip protobuf-3.5.1 $ wget https://github.com/protocolbuffers/protobuf/releases/download/v3.5.1/protobuf-python-3.5.1.zip $ unzip protobuf-python-3.5.1.zip $ cd protobuf-3.5.1/python &amp;&amp; python setup.py build &amp;&amp; python setup.py install $ cd /home &amp;&amp; rm -rf protobuf-python-3.5.1.zip protobuf-3.5.1 安装 caffe， 需要修改配置文件 Makefile.config # 安装 caffe $ cd /home &amp;&amp; git clone https://github.com/bvlc/caffe.git # 将 caffe/Makefile.config.example 文件复制到 /home 路径下，命名为 Makefile.config 并进行修改 # 第05行改为 USE_CUDNN := 1 # 第11行改为 USE_OPENCV := 1 # 第39行改为 CUDA_ARCH := -gencode arch=compute_30,code=sm_30 \ # 第51行改为 BLAS := open # 第55行改为 BLAS_INCLUDE := /usr/include/openblas # 第56行改为 BLAS_LIB := /usr/lib64 $ cp Makefile.config caffe/Makefile.config &amp;&amp; rm -f Makefile.config $ cd caffe &amp;&amp; make -j32 &amp;&amp; make pycaffe -j32 $ cp -r python/caffe /usr/local/python3/lib/python3.6/site-packages $ cp .build_release/lib/* /usr/lib64 在 python 命令窗口中执行 import caffe 查看 caffe 的 python 接口是否编译成功。 # 验证 caffe 是否安装成功 &gt;&gt;&gt; import caffe]]></content>
      <categories>
        <category>安装教程</category>
      </categories>
      <tags>
        <tag>centos</tag>
        <tag>caffe</tag>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hello World——利用hexo和github快速搭建个人博客]]></title>
    <url>%2F2018%2F12%2F01%2Fhello-world%2F</url>
    <content type="text"><![CDATA[一、hexo和github简介 Hexo 生成静态网页，Github 托管网页，Markdown 编辑博客。 1. hexo是什么？Hexo 是一款基于 Node.js 语言、快速、简洁且高效的博客框架。通过使用Markdown（或其他渲染引擎）解析文章，即使是前端小白也可利用 hexo 框架的靓丽主题快速生成相当专业的静态网页。 2. github是什么？GitHub 是一个面向开源及私有软件项目的托管平台，除了git 代码仓库托管及基本的 Web 管理界面以外，还提供了订阅、讨论组、文本渲染、在线文件编辑器、协作图谱（报表）、代码片段分享（Gist）等功能，是当前最活跃的“程序猿交友平台”。 3. markdown是什么？Markdown 是一种可以使用普通文本编辑器编写的标记语言，通过简单的标记语法，它可以使普通文本内容具有一定的格式。Markdown的语法简洁易学，功能比纯文本更强大，世界上最流行的博客平台 WordPress 能很好的支持Markdown。 二、搭建博客环境1. 安装 Node.jsHexo 博客框架基于 Node.js 语言，首先下载 Node.js安装包，选择对应的版本进行安装。默认安装过程会配置环境变量及 npm 包，安装完成后在命令窗口（例如 windows 系统的 cmd 窗口）输入 node -v 即可验证是否安装成功。 2. 安装 GitGit 是开源的分布式版本控制系统，可以将本地编辑完成的博客同步到 Github 服务器上。首先下载 Git安装包，安装完成后在命令窗口输入 git -v 即可验证是否安装成功。 3. 安装 HexoHexo是个人博客网站的框架，安装步骤参考 官网文档。首先在本地建立名为blog的文件夹（文件夹名任意），然后在blog文件夹当前路径下开启命令窗口，通过 npm 命令即可完成安装。 $ npm install -g hexo-cli 安装完成后，在命令窗口中初始化博客。 $ hexo init blog 初始化完成后，分别下述命令检测博客环境是否正常。 $ hexo generate # 生成博客 $ hexo server # 启动本地服务器 hexo 3.0把服务器独立成个别模块，需要单独安装npm i hexo-server。如果没有报错，接下来就是见证奇迹的时刻了。在浏览器中输入网址 http://localhost:4000，就可以看到诞生的第一篇博客。 4. 上传到 Github首先到 官网注册，假定注册的用户名为 user_name，注册的邮箱为 user_email，然后创建一个仓库，设置该仓库的主页面，得到你的github主页面网址 http://user_name.github.io。其他用户在浏览器中输入该网址，就能看到你的主页面。最后编辑站点配置文件 /blog/_config.yml，在该文件的末尾加入： deploy: type: git repository: https://github.com/user_name/user_name.github.io branch: master 在命令窗口运行代码 npm install hexo-deployer-git --save 安装 git 命令部署插件后，执行如下代码： $ git config --global user.name &quot;user_name&quot; # 指定 git 上传的仓库 $ git config --global user.email user_email $ hexo clean # 清理缓存 $ hexo generate # 生成博客 $ hexo deploy # 同步到 github 主页面 5. 绑定个人域名待续 6. 图床加速待续 7. Markdown 编辑工具当前有许多好用的 Markdown 编辑工具，有的收费，有的免费，相对而言收费工具的体验较好。Markdown 文件的后缀名为.md，对于一名程序员来说，最友好的Markdown 编辑界面当然是 IDE 自带的 Markdown 编辑插件。在 Pycharm 中添加 Markdown 插件的步骤如下，File-&gt;Settings-&gt;Plugins-&gt;Install JetBrains Plugins-&gt;输入Markdown-&gt;选择插件-&gt;Install-&gt;安装完成后重启PyCharm。编辑界面如下图所示，黑色背景，支持预览，所见即所得。 Pycharm Markdown 插件编辑效果图 在 VSCodescode 中支持 Markdown 语法，只需要下载 Markdown 预览插件即可。在 VSCode 中添加插件的步骤如下，选择左边栏第四个图标 Extensions，在输入框搜索 Markdown Preview Enhanced，安装成功后重启 VSCode。编辑界面如下图所示，黑色背景，支持预览，所见即所得。 VSCode Markdown 插件编辑效果图]]></content>
      <categories>
        <category>博客</category>
      </categories>
      <tags>
        <tag>hexo 博客</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[为什么写博客]]></title>
    <url>%2F2018%2F11%2F28%2Fwhy-blog%2F</url>
    <content type="text"><![CDATA[刘未鹏 —— 《暗时间》写一个博客有很多的好处，却没有任何明显的坏处。更明确的说：用博客的形式来记录下你有价值的思考，会带来很多好处，却没有任何明显的坏处。写一个长期的价值博客最大的几点好处: 能够交到很多志同道合的朋友。 书写是为了更好地思考。 “教”是最好的“学”。如果一件事情你不能讲清楚，十有八九你还没有完全理解。 激励你去持续学习和思考。 学会持之以恒地做一件事情。 一个长期的价值博客是一份很好的简历。 谨以博客记录算法菜鸟的“攻城狮”之路。]]></content>
  </entry>
</search>
